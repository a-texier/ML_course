{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_cost` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    e = y - np.dot(tx,w)\n",
    "    N = y.shape[0]\n",
    "   \n",
    "    return 1/(2*N) * np.dot(e.T,e)\n",
    "    # TODO: compute loss by MSE\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        ,  11.11111111,  22.22222222,  33.33333333,\n",
       "        44.44444444,  55.55555556,  66.66666667,  77.77777778,\n",
       "        88.88888889, 100.        ])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([1,2]).T\n",
    "e = y - np.dot(tx,w)\n",
    "loss = compute_loss(y, tx, w)\n",
    "np.linspace(0,100,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "        \n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    for i in range(len(grid_w0)):\n",
    "        for j in range(len(grid_w1)):\n",
    "            w = np.array([grid_w0[i],grid_w1[j]]).T\n",
    "            losses[i,j] = compute_loss(y,tx,w)\n",
    "    \n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    # ***************************************************\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=71.1270196108493, w0*=71.42857142857142, w1*=9.183673469387742, execution time=0.052 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=50)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    N = y.shape[0]\n",
    "    e = y - np.dot(tx,w)\n",
    "    return -1/N*np.dot(tx.T,e)\n",
    "    # TODO: compute gradient vector\n",
    "    # ***************************************************\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.         6.66666667]\n",
      "6.666666666666667\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "yb = np.array([1,2,3]).T\n",
    "txb = np.c_[np.array([1,1,1]),np.array([1,2,3])]\n",
    "wb = np.array([1,2]).T\n",
    "print(compute_gradient(yb, txb, wb))\n",
    "print(20/3)\n",
    "#OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26.706078    6.52028757]\n",
      "[-23.293922    -3.47971243]\n"
     ]
    }
   ],
   "source": [
    "#intuition\n",
    "w1 = np.array([100,20]).T\n",
    "w2 = np.array([50,10]).T\n",
    "print(compute_gradient(y, tx, w1))\n",
    "print(compute_gradient(y, tx, w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        # ***************************************************\n",
    "  \n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        w = w - gamma*grad\n",
    "        # ***************************************************\n",
    "   \n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}, grad={grad}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1], grad=grad))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=1062606.4462798769, w0=-892.6706077997898, w1=901.3479712434981, grad=[-1073.293922     986.52028757]\n",
      "GD iter. 1/49: loss=860714.1448053946, w0=-796.0741548196002, w1=812.561145362647, grad=[-965.9645298   887.86825881]\n",
      "GD iter. 2/49: loss=697181.3806110646, w0=-709.1373471374294, w1=732.6530020698813, grad=[-869.36807682  799.08143293]\n",
      "GD iter. 3/49: loss=564719.8416136572, w0=-630.8942202234758, w1=660.7356731063917, grad=[-782.43126914  719.17328963]\n",
      "GD iter. 4/49: loss=457425.9950257571, w0=-560.4754060009178, w1=596.0100770392514, grad=[-704.18814223  647.25596067]\n",
      "GD iter. 5/49: loss=370517.9792895583, w0=-497.09847320061544, w1=537.757040578825, grad=[-633.769328   582.5303646]\n",
      "GD iter. 6/49: loss=300122.48654323735, w0=-440.05923368034337, w1=485.3293077644414, grad=[-570.3923952   524.27732814]\n",
      "GD iter. 7/49: loss=243102.13741871726, w0=-388.7239181120987, w1=438.1443482314959, grad=[-513.35315568  471.84959533]\n",
      "GD iter. 8/49: loss=196915.65462785604, w0=-342.52213410067833, w1=395.6778846518452, grad=[-462.01784011  424.6646358 ]\n",
      "GD iter. 9/49: loss=159504.60356725843, w0=-300.94052849039974, w1=357.4580674301595, grad=[-415.8160561   382.19817222]\n",
      "GD iter. 10/49: loss=129201.65220817429, w0=-263.51708344114905, w1=323.06023193064226, grad=[-374.23445049  343.978355  ]\n",
      "GD iter. 11/49: loss=104656.26160731616, w0=-229.83598289682348, w1=292.102179981077, grad=[-336.81100544  309.5805195 ]\n",
      "GD iter. 12/49: loss=84774.49522062112, w0=-199.5229924069306, w1=264.2399332264682, grad=[-303.1299049   278.62246755]\n",
      "GD iter. 13/49: loss=68670.26444739819, w0=-172.2413009660271, w1=239.16391114732028, grad=[-272.81691441  250.76022079]\n",
      "GD iter. 14/49: loss=55625.837521087626, w0=-147.68777866921388, w1=216.5954912760871, grad=[-245.53522297  225.68419871]\n",
      "GD iter. 15/49: loss=45059.85171077605, w0=-125.58960860208197, w1=196.28391339197728, grad=[-220.98170067  203.11577884]\n",
      "GD iter. 16/49: loss=36501.403204423674, w0=-105.70125554166322, w1=178.00349329627844, grad=[-198.8835306   182.80420096]\n",
      "GD iter. 17/49: loss=29569.05991427824, w0=-87.80173778728633, w1=161.55111521014945, grad=[-178.99517754  164.52378086]\n",
      "GD iter. 18/49: loss=23953.861849260433, w0=-71.69217180834715, w1=146.74397493263334, grad=[-161.09565979  148.07140278]\n",
      "GD iter. 19/49: loss=19405.551416596016, w0=-57.193562427301906, w1=133.4175486828689, grad=[-144.98609381  133.2642625 ]\n",
      "GD iter. 20/49: loss=15721.419966137852, w0=-44.144813984361214, w1=121.42376505808089, grad=[-130.48748443  119.93783625]\n",
      "GD iter. 21/49: loss=12737.273491266735, w0=-32.40094038571449, w1=110.62935979577168, grad=[-117.43873599  107.94405262]\n",
      "GD iter. 22/49: loss=10320.114846621123, w0=-21.831454146932515, w1=100.9143950596934, grad=[-105.69486239   97.14964736]\n",
      "GD iter. 23/49: loss=8362.216344458184, w0=-12.318916532028728, w1=92.17092679722296, grad=[-95.12537615  87.43468262]\n",
      "GD iter. 24/49: loss=6776.318557706204, w0=-3.757632678615357, w1=84.30180536099955, grad=[-85.61283853  78.69121436]\n",
      "GD iter. 25/49: loss=5491.741350437105, w0=3.947522789456702, w1=77.21959606839849, grad=[-77.05155468  70.82209293]\n",
      "GD iter. 26/49: loss=4451.233812549132, w0=10.882162710721548, w1=70.84560770505753, grad=[-69.34639921  63.73988363]\n",
      "GD iter. 27/49: loss=3608.4227068598734, w0=17.123338639859913, w1=65.10901817805068, grad=[-62.41175929  57.36589527]\n",
      "GD iter. 28/49: loss=2925.745711251575, w0=22.740396976084455, w1=59.94608760374451, grad=[-56.17058336  51.62930574]\n",
      "GD iter. 29/49: loss=2372.777344808852, w0=27.795749478686528, w1=55.299450086868966, grad=[-50.55352503  46.46637517]\n",
      "GD iter. 30/49: loss=1924.8729679902483, w0=32.3455667310284, w1=51.11747632168097, grad=[-45.49817252  41.81973765]\n",
      "GD iter. 31/49: loss=1562.070422767178, w0=36.4404022581361, w1=47.353699933011775, grad=[-40.94835527  37.63776389]\n",
      "GD iter. 32/49: loss=1268.2003611364908, w0=40.12575423253302, w1=43.966301183209495, grad=[-36.85351974  33.8739875 ]\n",
      "GD iter. 33/49: loss=1030.1656112156347, w0=43.442571009490244, w1=40.91764230838744, grad=[-33.16816777  30.48658875]\n",
      "GD iter. 34/49: loss=837.3574637797411, w0=46.42770610875173, w1=38.1738493210476, grad=[-29.85135099  27.43792987]\n",
      "GD iter. 35/49: loss=681.182864356668, w0=49.11432769808709, w1=35.704435632441744, grad=[-26.86621589  24.69413689]\n",
      "GD iter. 36/49: loss=554.6814388239784, w0=51.5322871284889, w1=33.48196331269647, grad=[-24.1795943  22.2247232]\n",
      "GD iter. 37/49: loss=452.2152841424999, w0=53.70845061585052, w1=31.481738224925728, grad=[-21.76163487  20.00225088]\n",
      "GD iter. 38/49: loss=369.2176988505026, w0=55.66699775447599, w1=29.681535645932055, grad=[-19.58547139  18.00202579]\n",
      "GD iter. 39/49: loss=301.98965476398456, w0=57.42969017923892, w1=28.061353324837754, grad=[-17.62692425  16.20182321]\n",
      "GD iter. 40/49: loss=247.53493905390502, w0=59.01611336152555, w1=26.603189235852884, grad=[-15.86423182  14.58164089]\n",
      "GD iter. 41/49: loss=203.42661932874066, w0=60.44389422558352, w1=25.290841555766498, grad=[-14.27780864  13.1234768 ]\n",
      "GD iter. 42/49: loss=167.69888035135733, w0=61.72889700323569, w1=24.109728643688754, grad=[-12.85002778  11.81112912]\n",
      "GD iter. 43/49: loss=138.75941177967707, w0=62.88539950312264, w1=23.046727022818782, grad=[-11.565025    10.63001621]\n",
      "GD iter. 44/49: loss=115.31844223661595, w0=63.92625175302089, w1=22.09002556403581, grad=[-10.4085225    9.56701459]\n",
      "GD iter. 45/49: loss=96.33125690673654, w0=64.86301877792933, w1=21.228994251131134, grad=[-9.36767025  8.61031313]\n",
      "GD iter. 46/49: loss=80.95163678953415, w0=65.70610910034691, w1=20.454066069516923, grad=[-8.43090322  7.74928182]\n",
      "GD iter. 47/49: loss=68.49414449460028, w0=66.46489039052274, w1=19.756630706064133, grad=[-7.5878129   6.97435363]\n",
      "GD iter. 48/49: loss=58.403575735703754, w0=67.14779355168099, w1=19.128938878956625, grad=[-6.82903161  6.27691827]\n",
      "GD iter. 49/49: loss=50.230215040997614, w0=67.7624063967234, w1=18.564016234559865, grad=[-6.14612845  5.64922644]\n",
      "GD: execution time=0.056 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([-1000, 1000])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c9a8074fc04bf89d6bd084f6588811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widgeâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    N = y.shape[0]\n",
    "    e = y - np.dot(tx,w)\n",
    "    return -1/N*np.dot(tx.T,e)\n",
    "    # ***************************************************\n",
    "   \n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        # ***************************************************\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            grad = compute_stoch_gradient(minibatch_y.T, minibatch_tx, w)\n",
    "            loss = compute_loss(minibatch_y.T, minibatch_tx, w)\n",
    "            w = w - gamma*grad\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1},grad={grad}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1],grad=grad))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=3078.7134277985438, w0=7.846927332145423, w1=0.6813100972686199,grad=[-78.46927332  -6.81310097]\n",
      "SGD iter. 1/49: loss=1932.749848650057, w0=14.064241621531245, w1=1.0490758179434294,grad=[-62.17314289  -3.67765721]\n",
      "SGD iter. 2/49: loss=1464.8450619615571, w0=19.47690281905979, w1=-0.5179845785944324,grad=[-54.12661198  15.67060397]\n",
      "SGD iter. 3/49: loss=2560.7364572173155, w0=26.63334950329816, w1=3.4655956267794004,grad=[-71.56446684 -39.83580205]\n",
      "SGD iter. 4/49: loss=1462.9705573944575, w0=32.0425464074503, w1=6.689776837642793,grad=[-54.09196904 -32.24181211]\n",
      "SGD iter. 5/49: loss=1660.0253027669978, w0=37.80453443723134, w1=18.76529503390934,grad=[ -57.6198803  -120.75518196]\n",
      "SGD iter. 6/49: loss=836.5262220889795, w0=41.89483081301874, w1=19.703249393024056,grad=[-40.90296376  -9.37954359]\n",
      "SGD iter. 7/49: loss=475.1917594191233, w0=44.97765990143452, w1=18.966919872803857,grad=[-30.82829088   7.3632952 ]\n",
      "SGD iter. 8/49: loss=266.57805490665334, w0=47.28667724613617, w1=20.402508998042958,grad=[-23.09017345 -14.35589125]\n",
      "SGD iter. 9/49: loss=388.5994147274025, w0=50.074505844624255, w1=20.658248819389048,grad=[-27.87828598  -2.55739821]\n",
      "SGD iter. 10/49: loss=175.98717222497822, w0=51.95060377503715, w1=21.700756500627083,grad=[-18.7609793  -10.42507681]\n",
      "SGD iter. 11/49: loss=31.20199297505809, w0=52.74056571051326, w1=21.684886427614714,grad=[-7.89961935  0.15870073]\n",
      "SGD iter. 12/49: loss=44.72391372580686, w0=53.686334326251874, w1=21.714757677820284,grad=[-9.45768616 -0.2987125 ]\n",
      "SGD iter. 13/49: loss=480.53595021169195, w0=56.786450292507624, w1=18.116456124314706,grad=[-31.00115966  35.98301554]\n",
      "SGD iter. 14/49: loss=25.79179217141558, w0=57.50466742083882, w1=19.46374123493044,grad=[ -7.18217128 -13.47285111]\n",
      "SGD iter. 15/49: loss=167.366726365733, w0=59.33423964606379, w1=20.208357364994505,grad=[-18.29572225  -7.4461613 ]\n",
      "SGD iter. 16/49: loss=0.20904300696444159, w0=59.26958007492936, w1=20.12864123188168,grad=[0.64659571 0.79716133]\n",
      "SGD iter. 17/49: loss=62.20792409392259, w0=60.384998598120745, w1=21.188730288214135,grad=[-11.15418523 -10.60089056]\n",
      "SGD iter. 18/49: loss=178.2441224352366, w0=62.273088227535816, w1=22.28860397774239,grad=[-18.88089629 -10.9987369 ]\n",
      "SGD iter. 19/49: loss=19.767591231865545, w0=62.90185831638969, w1=22.277517886324382,grad=[-6.28770089  0.11086091]\n",
      "SGD iter. 20/49: loss=50.55105518356596, w0=63.90735376822989, w1=22.71840218005207,grad=[-10.05495452  -4.40884294]\n",
      "SGD iter. 21/49: loss=254.23113423296945, w0=66.16226456129009, w1=19.877620046485315,grad=[-22.54910793  28.40782134]\n",
      "SGD iter. 22/49: loss=10.267113003585195, w0=66.61541162359073, w1=20.401114406723433,grad=[-4.53147062 -5.2349436 ]\n",
      "SGD iter. 23/49: loss=59.382568393086075, w0=67.70520580960097, w1=19.456900226362077,grad=[-10.89794186   9.4421418 ]\n",
      "SGD iter. 24/49: loss=103.60247850110147, w0=69.14466736519113, w1=17.293622816326987,grad=[-14.39461556  21.6327741 ]\n",
      "SGD iter. 25/49: loss=0.6347092892499469, w0=69.03199888785608, w1=17.322448890845447,grad=[ 1.12668477 -0.28826075]\n",
      "SGD iter. 26/49: loss=1.096389980498901, w0=68.88391850559705, w1=17.32448815809086,grad=[ 1.48080382 -0.02039267]\n",
      "SGD iter. 27/49: loss=40.26943290879245, w0=69.78135299244043, w1=18.14422339571823,grad=[-8.97434487 -8.19735238]\n",
      "SGD iter. 28/49: loss=40.679627112949426, w0=70.6833466411204, w1=16.71174089384733,grad=[-9.01993649 14.32482502]\n",
      "SGD iter. 29/49: loss=0.20439087518097074, w0=70.7472826834097, w1=16.781119372625575,grad=[-0.63936042 -0.69378479]\n",
      "SGD iter. 30/49: loss=23.192050082060565, w0=70.06622394758641, w1=15.825560356115444,grad=[6.81058736 9.55559017]\n",
      "SGD iter. 31/49: loss=11.484990324288578, w0=70.545494024355, w1=15.788634723865442,grad=[-4.79270077  0.36925632]\n",
      "SGD iter. 32/49: loss=0.8377005254598195, w0=70.67493130843187, w1=15.738424281936659,grad=[-1.29437284  0.50210442]\n",
      "SGD iter. 33/49: loss=3.903114227417188, w0=70.95432759428468, w1=15.553616543714236,grad=[-2.79396286  1.84807738]\n",
      "SGD iter. 34/49: loss=9.87870196311912, w0=71.39882060802323, w1=15.97682587240621,grad=[-4.44493014 -4.23209329]\n",
      "SGD iter. 35/49: loss=5.266099996969395, w0=71.72335411929654, w1=15.711108516090567,grad=[-3.24533511  2.65717356]\n",
      "SGD iter. 36/49: loss=13.668457318566936, w0=72.24620127322524, w1=15.82371779814311,grad=[-5.22847154 -1.12609282]\n",
      "SGD iter. 37/49: loss=4.80672050744658, w0=71.93614577795232, w1=16.321390079931362,grad=[ 3.10055495 -4.97672282]\n",
      "SGD iter. 38/49: loss=2.4794577942405773, w0=71.71345955052725, w1=16.333639805593293,grad=[ 2.22686227 -0.12249726]\n",
      "SGD iter. 39/49: loss=24.372273386708127, w0=71.01528660079762, w1=15.93294587527109,grad=[6.9817295 4.0069393]\n",
      "SGD iter. 40/49: loss=0.15245585341246157, w0=71.07050541161648, w1=15.9404407276885,grad=[-0.55218811 -0.07494852]\n",
      "SGD iter. 41/49: loss=40.379122163409896, w0=70.1718495023452, w1=15.438173192554515,grad=[8.98655909 5.02267535]\n",
      "SGD iter. 42/49: loss=5.736153102138368, w0=69.83314157205308, w1=15.561465962467787,grad=[ 3.3870793 -1.2329277]\n",
      "SGD iter. 43/49: loss=6.97610292250949, w0=69.45961505565808, w1=14.626925283008141,grad=[3.73526516 9.34540679]\n",
      "SGD iter. 44/49: loss=12.75959882095374, w0=69.964780351793, w1=14.274603633899378,grad=[-5.05165296  3.52321649]\n",
      "SGD iter. 45/49: loss=8.076566764873174, w0=70.3666899626478, w1=14.141213219657091,grad=[-4.01909611  1.33390414]\n",
      "SGD iter. 46/49: loss=63.90850926660557, w0=71.49725185185693, w1=15.291247979200884,grad=[-11.30561889 -11.5003476 ]\n",
      "SGD iter. 47/49: loss=2.443407759001866, w0=71.27619042333527, w1=15.191922089096801,grad=[2.21061429 0.9932589 ]\n",
      "SGD iter. 48/49: loss=18.806084812306274, w0=71.88947803632458, w1=14.278029452246052,grad=[-6.13287613  9.13892637]\n",
      "SGD iter. 49/49: loss=22.38979885400959, w0=72.55865363142664, w1=14.823496092225717,grad=[-6.69175595 -5.4546664 ]\n",
      "SGD: execution time=0.169 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15d09d5f5d84cc4802700b923066a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widgeâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "# ***************************************************\n",
    "\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.8351145358524, w0=51.84746409844842, w1=7.724426406192425, grad=[-74.06780585 -11.03489487]\n",
      "GD iter. 1/49: loss=318.28212470159644, w0=67.40170332798297, w1=10.041754328050116, grad=[-22.22034176  -3.31046846]\n",
      "GD iter. 2/49: loss=88.6423556165128, w0=72.06797509684336, w1=10.736952704607413, grad=[-6.66610253 -0.99314054]\n",
      "GD iter. 3/49: loss=67.97477639885521, w0=73.46785662750146, w1=10.9455122175746, grad=[-1.99983076 -0.29794216]\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475, grad=[-0.59994923 -0.08938265]\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631798, grad=[-0.17998477 -0.02681479]\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.0516072257859, w1=11.032481534481912, grad=[-0.05399543 -0.00804444]\n",
      "GD iter. 7/49: loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536945, grad=[-0.01619863 -0.00241333]\n",
      "GD iter. 8/49: loss=65.93074217249234, w0=74.06634797830372, w1=11.034677666153454, grad=[-0.00485959 -0.000724  ]\n",
      "GD iter. 9/49: loss=65.93073118889338, w0=74.06736849193958, w1=11.034829706038408, grad=[-0.00145788 -0.0002172 ]\n",
      "GD iter. 10/49: loss=65.93073020036947, w0=74.06767464603033, w1=11.034875318003895, grad=[-4.37362987e-04 -6.51599507e-05]\n",
      "GD iter. 11/49: loss=65.93073011140231, w0=74.06776649225755, w1=11.034889001593541, grad=[-1.31208896e-04 -1.95479852e-05]\n",
      "GD iter. 12/49: loss=65.93073010339529, w0=74.06779404612573, w1=11.034893106670431, grad=[-3.93626688e-05 -5.86439556e-06]\n",
      "GD iter. 13/49: loss=65.93073010267466, w0=74.06780231228618, w1=11.034894338193501, grad=[-1.18088006e-05 -1.75931867e-06]\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421, grad=[-3.54264019e-06 -5.27795600e-07]\n",
      "GD iter. 15/49: loss=65.93073010260393, w0=74.06780553608874, w1=11.034894818487496, grad=[-1.06279205e-06 -1.58338678e-07]\n",
      "GD iter. 16/49: loss=65.93073010260342, w0=74.06780575927507, w1=11.03489485173862, grad=[-3.18837625e-07 -4.75016062e-08]\n",
      "GD iter. 17/49: loss=65.93073010260339, w0=74.06780582623098, w1=11.034894861713957, grad=[-9.56512909e-08 -1.42504794e-08]\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706557, grad=[-2.86953844e-08 -4.27514288e-09]\n",
      "GD iter. 19/49: loss=65.93073010260338, w0=74.06780585234378, w1=11.034894865604338, grad=[-8.60861196e-09 -1.28254399e-09]\n",
      "GD iter. 20/49: loss=65.93073010260338, w0=74.06780585415159, w1=11.034894865873675, grad=[-2.58258323e-09 -3.84765504e-10]\n",
      "GD iter. 21/49: loss=65.93073010260338, w0=74.06780585469393, w1=11.034894865954474, grad=[-7.74778754e-10 -1.15427034e-10]\n",
      "GD iter. 22/49: loss=65.93073010260338, w0=74.06780585485663, w1=11.034894865978712, grad=[-2.32434710e-10 -3.46272098e-11]\n",
      "GD iter. 23/49: loss=65.93073010260336, w0=74.06780585490544, w1=11.034894865985985, grad=[-6.97352671e-11 -1.03894011e-11]\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.0678058549201, w1=11.034894865988166, grad=[-2.09215439e-11 -3.11541332e-12]\n",
      "GD iter. 25/49: loss=65.93073010260338, w0=74.06780585492449, w1=11.034894865988823, grad=[-6.26937886e-12 -9.38197814e-13]\n",
      "GD iter. 26/49: loss=65.93073010260338, w0=74.06780585492581, w1=11.034894865989017, grad=[-1.87892826e-12 -2.76056405e-13]\n",
      "GD iter. 27/49: loss=65.93073010260338, w0=74.06780585492619, w1=11.034894865989076, grad=[-5.57600171e-13 -8.35767099e-14]\n",
      "GD iter. 28/49: loss=65.93073010260338, w0=74.06780585492632, w1=11.034894865989099, grad=[-1.72781481e-13 -3.17985462e-14]\n",
      "GD iter. 29/49: loss=65.93073010260339, w0=74.06780585492635, w1=11.0348948659891, grad=[-4.57279978e-14 -1.96982145e-15]\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-1.70952361e-14 -1.12561225e-15]\n",
      "GD iter. 31/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-3.16578447e-15 -0.00000000e+00]\n",
      "GD iter. 32/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-3.16578447e-15 -0.00000000e+00]\n",
      "GD iter. 33/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-3.16578447e-15 -0.00000000e+00]\n",
      "GD iter. 34/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-3.16578447e-15 -0.00000000e+00]\n",
      "GD iter. 35/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-3.16578447e-15 -0.00000000e+00]\n",
      "GD iter. 36/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-3.16578447e-15 -0.00000000e+00]\n",
      "GD iter. 37/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-3.16578447e-15 -0.00000000e+00]\n",
      "GD iter. 38/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-3.16578447e-15 -0.00000000e+00]\n",
      "GD iter. 39/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-3.16578447e-15 -0.00000000e+00]\n",
      "GD iter. 40/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-3.16578447e-15 -0.00000000e+00]\n",
      "GD iter. 41/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-3.16578447e-15 -0.00000000e+00]\n",
      "GD iter. 42/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-3.16578447e-15 -0.00000000e+00]\n",
      "GD iter. 43/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-3.16578447e-15 -0.00000000e+00]\n",
      "GD iter. 44/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-3.16578447e-15 -0.00000000e+00]\n",
      "GD iter. 45/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-3.16578447e-15 -0.00000000e+00]\n",
      "GD iter. 46/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-3.16578447e-15 -0.00000000e+00]\n",
      "GD iter. 47/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-3.16578447e-15 -0.00000000e+00]\n",
      "GD iter. 48/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-3.16578447e-15 -0.00000000e+00]\n",
      "GD iter. 49/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891, grad=[-3.16578447e-15 -0.00000000e+00]\n",
      "GD: execution time=0.037 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points \n",
    "#       and the model fit\n",
    "gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "# ***************************************************\n",
    "\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb76ee7b5f6457aa32bb7d8cd7c3681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widgeâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses, gd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute subgradient gradient vector for MAE\n",
    "    N = y.shape[0]\n",
    "    e = y - np.dot(tx,w)\n",
    "    return -1/N*np.dot(tx.T,np.sign(e))\n",
    "    # ***************************************************\n",
    "    \n",
    "def calculate_mae(e):\n",
    "    \"\"\"Calculate the mae for vector e.\"\"\"\n",
    "    return np.mean(np.abs(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute subgradient and loss\n",
    "        grad = compute_subgradient_mae(y, tx, w)\n",
    "        e = y - np.dot(tx,w)\n",
    "        loss = calculate_mae(e)\n",
    "        # ***************************************************\n",
    "    \n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "        # ***************************************************\n",
    "        \n",
    "        w = w - gamma * grad\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=73.29392200210518, w0=0.7, w1=-1.5529755259535704e-15\n",
      "SubGD iter. 1/499: loss=72.59392200210517, w0=1.4, w1=-3.1059510519071408e-15\n",
      "SubGD iter. 2/499: loss=71.89392200210517, w0=2.0999999999999996, w1=-4.658926577860711e-15\n",
      "SubGD iter. 3/499: loss=71.19392200210518, w0=2.8, w1=-6.2119021038142816e-15\n",
      "SubGD iter. 4/499: loss=70.49392200210517, w0=3.5, w1=-7.764877629767851e-15\n",
      "SubGD iter. 5/499: loss=69.79392200210518, w0=4.2, w1=-9.317853155721422e-15\n",
      "SubGD iter. 6/499: loss=69.09392200210519, w0=4.9, w1=-1.0870828681674993e-14\n",
      "SubGD iter. 7/499: loss=68.39392200210517, w0=5.6000000000000005, w1=-1.2423804207628563e-14\n",
      "SubGD iter. 8/499: loss=67.69392200210518, w0=6.300000000000001, w1=-1.3976779733582134e-14\n",
      "SubGD iter. 9/499: loss=66.99392200210517, w0=7.000000000000001, w1=-1.5529755259535703e-14\n",
      "SubGD iter. 10/499: loss=66.29392200210518, w0=7.700000000000001, w1=-1.7082730785489272e-14\n",
      "SubGD iter. 11/499: loss=65.59392200210519, w0=8.4, w1=-1.863570631144284e-14\n",
      "SubGD iter. 12/499: loss=64.89392200210517, w0=9.1, w1=-2.018868183739641e-14\n",
      "SubGD iter. 13/499: loss=64.19392200210517, w0=9.799999999999999, w1=-2.174165736334998e-14\n",
      "SubGD iter. 14/499: loss=63.49392200210517, w0=10.499999999999998, w1=-2.3294632889303548e-14\n",
      "SubGD iter. 15/499: loss=62.79392200210517, w0=11.199999999999998, w1=-2.4847608415257117e-14\n",
      "SubGD iter. 16/499: loss=62.09392200210518, w0=11.899999999999997, w1=-2.6400583941210686e-14\n",
      "SubGD iter. 17/499: loss=61.393922002105185, w0=12.599999999999996, w1=-2.7953559467164255e-14\n",
      "SubGD iter. 18/499: loss=60.69392200210517, w0=13.299999999999995, w1=-2.9506534993117824e-14\n",
      "SubGD iter. 19/499: loss=59.99392200210518, w0=13.999999999999995, w1=-3.105951051907139e-14\n",
      "SubGD iter. 20/499: loss=59.293922002105184, w0=14.699999999999994, w1=-3.261248604502496e-14\n",
      "SubGD iter. 21/499: loss=58.59392200210518, w0=15.399999999999993, w1=-3.416546157097853e-14\n",
      "SubGD iter. 22/499: loss=57.893922002105185, w0=16.099999999999994, w1=-3.57184370969321e-14\n",
      "SubGD iter. 23/499: loss=57.19392200210518, w0=16.799999999999994, w1=-3.727141262288567e-14\n",
      "SubGD iter. 24/499: loss=56.49392200210518, w0=17.499999999999993, w1=-3.882438814883924e-14\n",
      "SubGD iter. 25/499: loss=55.793922002105184, w0=18.199999999999992, w1=-4.0377363674792807e-14\n",
      "SubGD iter. 26/499: loss=55.09392200210518, w0=18.89999999999999, w1=-4.1930339200746376e-14\n",
      "SubGD iter. 27/499: loss=54.393922002105185, w0=19.59999999999999, w1=-4.3483314726699945e-14\n",
      "SubGD iter. 28/499: loss=53.69392200210518, w0=20.29999999999999, w1=-4.5036290252653514e-14\n",
      "SubGD iter. 29/499: loss=52.99392200210518, w0=20.99999999999999, w1=-4.658926577860708e-14\n",
      "SubGD iter. 30/499: loss=52.293922002105184, w0=21.69999999999999, w1=-4.814224130456065e-14\n",
      "SubGD iter. 31/499: loss=51.59392200210518, w0=22.399999999999988, w1=-4.969521683051422e-14\n",
      "SubGD iter. 32/499: loss=50.893922002105185, w0=23.099999999999987, w1=-5.124819235646779e-14\n",
      "SubGD iter. 33/499: loss=50.19392200210518, w0=23.799999999999986, w1=-5.280116788242136e-14\n",
      "SubGD iter. 34/499: loss=49.49392200210518, w0=24.499999999999986, w1=-5.435414340837493e-14\n",
      "SubGD iter. 35/499: loss=48.79392200210519, w0=25.199999999999985, w1=-5.59071189343285e-14\n",
      "SubGD iter. 36/499: loss=48.09392200210519, w0=25.899999999999984, w1=-5.746009446028207e-14\n",
      "SubGD iter. 37/499: loss=47.393922002105185, w0=26.599999999999984, w1=-5.901306998623565e-14\n",
      "SubGD iter. 38/499: loss=46.6939220021052, w0=27.299999999999983, w1=-6.056604551218922e-14\n",
      "SubGD iter. 39/499: loss=45.99392200210519, w0=27.999999999999982, w1=-6.21190210381428e-14\n",
      "SubGD iter. 40/499: loss=45.29392200210519, w0=28.69999999999998, w1=-6.367199656409637e-14\n",
      "SubGD iter. 41/499: loss=44.593922002105195, w0=29.39999999999998, w1=-6.522497209004995e-14\n",
      "SubGD iter. 42/499: loss=43.89392723059967, w0=30.099859999999982, w1=0.0004404657702421764\n",
      "SubGD iter. 43/499: loss=43.194206925442394, w0=30.799719999999983, w1=0.0008809315405495777\n",
      "SubGD iter. 44/499: loss=42.494486620285116, w0=31.499579999999984, w1=0.001321397310856979\n",
      "SubGD iter. 45/499: loss=41.794801882440076, w0=32.19929999999999, w1=0.002151200058803724\n",
      "SubGD iter. 46/499: loss=41.09536078676493, w0=32.899019999999986, w1=0.002981002806750469\n",
      "SubGD iter. 47/499: loss=40.396015121762325, w0=33.59859999999998, w1=0.004238399708372432\n",
      "SubGD iter. 48/499: loss=39.696964631836686, w0=34.298039999999986, w1=0.005823822408791713\n",
      "SubGD iter. 49/499: loss=38.99808059302934, w0=34.99747999999999, w1=0.0074092451092109945\n",
      "SubGD iter. 50/499: loss=38.299196554222, w0=35.69691999999999, w1=0.008994667809630276\n",
      "SubGD iter. 51/499: loss=37.600470352601796, w0=36.39607999999999, w1=0.011248049853488248\n",
      "SubGD iter. 52/499: loss=36.90236166793368, w0=37.09495999999999, w1=0.014269124444697119\n",
      "SubGD iter. 53/499: loss=36.20468598163629, w0=37.793699999999994, w1=0.01766349946233946\n",
      "SubGD iter. 54/499: loss=35.507288901302104, w0=38.49215999999999, w1=0.021587972991156293\n",
      "SubGD iter. 55/499: loss=34.81047272559701, w0=39.19019999999999, w1=0.026549981650513393\n",
      "SubGD iter. 56/499: loss=34.11463938511051, w0=39.88739999999999, w1=0.0334590206309035\n",
      "SubGD iter. 57/499: loss=33.42034933030703, w0=40.584039999999995, w1=0.04155352991358485\n",
      "SubGD iter. 58/499: loss=32.72708260590542, w0=41.28025999999999, w1=0.05080738787870366\n",
      "SubGD iter. 59/499: loss=32.035274191106154, w0=41.97507999999999, w1=0.06316953459264504\n",
      "SubGD iter. 60/499: loss=31.346253501120472, w0=42.667939999999994, w1=0.0797066345318716\n",
      "SubGD iter. 61/499: loss=30.66081118573862, w0=43.35925999999999, w1=0.09929003796957597\n",
      "SubGD iter. 62/499: loss=29.97824373392408, w0=44.04889999999999, w1=0.12249682042556717\n",
      "SubGD iter. 63/499: loss=29.29919217002557, w0=44.735739999999986, w1=0.15117673515145405\n",
      "SubGD iter. 64/499: loss=28.625370150150545, w0=45.41991999999998, w1=0.1848471294408023\n",
      "SubGD iter. 65/499: loss=27.95640812788488, w0=46.10073999999998, w1=0.22490170464881898\n",
      "SubGD iter. 66/499: loss=27.293211335702274, w0=46.77847999999998, w1=0.27030807598900264\n",
      "SubGD iter. 67/499: loss=26.635946775492176, w0=47.45243999999998, w1=0.32205641313644706\n",
      "SubGD iter. 68/499: loss=25.986354455661576, w0=48.119959999999985, w1=0.3843602843516495\n",
      "SubGD iter. 69/499: loss=25.34635990054962, w0=48.78257999999998, w1=0.45460143187065083\n",
      "SubGD iter. 70/499: loss=24.713407766391825, w0=49.44141999999998, w1=0.5311991534378644\n",
      "SubGD iter. 71/499: loss=24.086931144489547, w0=50.094659999999976, w1=0.6167864550871748\n",
      "SubGD iter. 72/499: loss=23.469061835298344, w0=50.74159999999998, w1=0.7118294204257005\n",
      "SubGD iter. 73/499: loss=22.860312219449888, w0=51.382239999999975, w1=0.8163783816188553\n",
      "SubGD iter. 74/499: loss=22.260394194603613, w0=52.015879999999974, w1=0.9309819479180104\n",
      "SubGD iter. 75/499: loss=21.670214357989362, w0=52.64223999999997, w1=1.056041039445561\n",
      "SubGD iter. 76/499: loss=21.089713092061302, w0=53.261179999999975, w1=1.1909782249707368\n",
      "SubGD iter. 77/499: loss=20.51839520029387, w0=53.87353999999998, w1=1.3354326069268716\n",
      "SubGD iter. 78/499: loss=19.954842019021974, w0=54.479039999999976, w1=1.488859155423101\n",
      "SubGD iter. 79/499: loss=19.39981727796524, w0=55.07641999999998, w1=1.6526057998941344\n",
      "SubGD iter. 80/499: loss=18.854292127308746, w0=55.664699999999975, w1=1.827199911738537\n",
      "SubGD iter. 81/499: loss=18.31878394789317, w0=56.24345999999998, w1=2.013212915976214\n",
      "SubGD iter. 82/499: loss=17.792491479525168, w0=56.81479999999998, w1=2.2075986497860063\n",
      "SubGD iter. 83/499: loss=17.27474434540427, w0=57.37633999999998, w1=2.411376270974464\n",
      "SubGD iter. 84/499: loss=16.766906752678832, w0=57.92975999999998, w1=2.6230481013704767\n",
      "SubGD iter. 85/499: loss=16.267994479749298, w0=58.47407999999998, w1=2.8433267790651087\n",
      "SubGD iter. 86/499: loss=15.777334186667382, w0=59.00957999999998, w1=3.072330965520354\n",
      "SubGD iter. 87/499: loss=15.294906834780795, w0=59.53569999999998, w1=3.3102924945409007\n",
      "SubGD iter. 88/499: loss=14.820404573025089, w0=60.053559999999976, w1=3.555351748353824\n",
      "SubGD iter. 89/499: loss=14.354283714893946, w0=60.559939999999976, w1=3.809170691886062\n",
      "SubGD iter. 90/499: loss=13.898001392894923, w0=61.057499999999976, w1=4.069720751433983\n",
      "SubGD iter. 91/499: loss=13.44882382810378, w0=61.547919999999976, w1=4.335508393726905\n",
      "SubGD iter. 92/499: loss=13.006044196344101, w0=62.02965999999998, w1=4.6069897470010375\n",
      "SubGD iter. 93/499: loss=12.57110174936068, w0=62.502719999999975, w1=4.88502014251682\n",
      "SubGD iter. 94/499: loss=12.143380485959392, w0=62.96583999999998, w1=5.16875462665388\n",
      "SubGD iter. 95/499: loss=11.723894837245577, w0=63.41971999999998, w1=5.456484559032046\n",
      "SubGD iter. 96/499: loss=11.313435023601548, w0=63.86463999999998, w1=5.747997496770113\n",
      "SubGD iter. 97/499: loss=10.911156920015095, w0=64.30129999999998, w1=6.043221223968383\n",
      "SubGD iter. 98/499: loss=10.51694812260807, w0=64.72773999999998, w1=6.340354080836923\n",
      "SubGD iter. 99/499: loss=10.133595131038662, w0=65.14381999999998, w1=6.63994524590634\n",
      "SubGD iter. 100/499: loss=9.76067742992348, w0=65.54995999999997, w1=6.939754306256994\n",
      "SubGD iter. 101/499: loss=9.39867462318993, w0=65.94755999999997, w1=7.240382293155359\n",
      "SubGD iter. 102/499: loss=9.046552477133647, w0=66.33451999999997, w1=7.542759507922872\n",
      "SubGD iter. 103/499: loss=8.704760514732406, w0=66.71139999999997, w1=7.846017516367578\n",
      "SubGD iter. 104/499: loss=8.372871671628259, w0=67.07791999999996, w1=8.149168076773346\n",
      "SubGD iter. 105/499: loss=8.052713872262588, w0=67.43603999999996, w1=8.450471378640396\n",
      "SubGD iter. 106/499: loss=7.741845358498811, w0=67.78645999999996, w1=8.751045452661094\n",
      "SubGD iter. 107/499: loss=7.440636497847941, w0=68.12511999999997, w1=9.04921572225288\n",
      "SubGD iter. 108/499: loss=7.152144892432372, w0=68.45537999999996, w1=9.343976087699637\n",
      "SubGD iter. 109/499: loss=6.874953545674696, w0=68.77695999999996, w1=9.635092372673002\n",
      "SubGD iter. 110/499: loss=6.609759691657246, w0=69.08803999999996, w1=9.919493282073365\n",
      "SubGD iter. 111/499: loss=6.359664465347398, w0=69.38749999999996, w1=10.197623582279002\n",
      "SubGD iter. 112/499: loss=6.125559830337079, w0=69.67281999999996, w1=10.46806359361339\n",
      "SubGD iter. 113/499: loss=5.908756851126097, w0=69.94651999999996, w1=10.728826346302005\n",
      "SubGD iter. 114/499: loss=5.708346277966664, w0=70.20887999999997, w1=10.979299692022279\n",
      "SubGD iter. 115/499: loss=5.524903595719341, w0=70.45653999999996, w1=11.220392679253374\n",
      "SubGD iter. 116/499: loss=5.358572110476772, w0=70.69215999999996, w1=11.449066352016724\n",
      "SubGD iter. 117/499: loss=5.208366578733321, w0=70.91545999999995, w1=11.66451264164341\n",
      "SubGD iter. 118/499: loss=5.07545205091803, w0=71.12447999999995, w1=11.864387168308943\n",
      "SubGD iter. 119/499: loss=4.960839914882397, w0=71.31865999999995, w1=12.045105403559484\n",
      "SubGD iter. 120/499: loss=4.864240181709678, w0=71.49939999999995, w1=12.20983607316598\n",
      "SubGD iter. 121/499: loss=4.7827247689452586, w0=71.66543999999995, w1=12.357476521898722\n",
      "SubGD iter. 122/499: loss=4.715208074668132, w0=71.81593999999994, w1=12.48980888194029\n",
      "SubGD iter. 123/499: loss=4.660146063774251, w0=71.95369999999994, w1=12.612418157319299\n",
      "SubGD iter. 124/499: loss=4.613848453064219, w0=72.08067999999994, w1=12.721366944472148\n",
      "SubGD iter. 125/499: loss=4.576038370591761, w0=72.19435999999995, w1=12.815458765817613\n",
      "SubGD iter. 126/499: loss=4.546649851049038, w0=72.29697999999995, w1=12.90059276939534\n",
      "SubGD iter. 127/499: loss=4.522563544988971, w0=72.38965999999995, w1=12.97576965427992\n",
      "SubGD iter. 128/499: loss=4.503077398110058, w0=72.47435999999995, w1=13.04396334930227\n",
      "SubGD iter. 129/499: loss=4.487012427842283, w0=72.55093999999995, w1=13.103277584601688\n",
      "SubGD iter. 130/499: loss=4.474309393425117, w0=72.62051999999996, w1=13.156032035188144\n",
      "SubGD iter. 131/499: loss=4.464166669156604, w0=72.68239999999996, w1=13.201870566969161\n",
      "SubGD iter. 132/499: loss=4.456202124936146, w0=72.73769999999996, w1=13.241365939188695\n",
      "SubGD iter. 133/499: loss=4.449913837301207, w0=72.78823999999996, w1=13.276636749385748\n",
      "SubGD iter. 134/499: loss=4.444739448232767, w0=72.83401999999995, w1=13.308112586749687\n",
      "SubGD iter. 135/499: loss=4.440577063150133, w0=72.87447999999995, w1=13.335151520405006\n",
      "SubGD iter. 136/499: loss=4.4373967610583085, w0=72.91115999999995, w1=13.358430829065458\n",
      "SubGD iter. 137/499: loss=4.434846528707499, w0=72.94447999999996, w1=13.379256780851263\n",
      "SubGD iter. 138/499: loss=4.432811551436952, w0=72.97401999999995, w1=13.396406376905544\n",
      "SubGD iter. 139/499: loss=4.43123366185832, w0=73.00103999999995, w1=13.41218912045937\n",
      "SubGD iter. 140/499: loss=4.429913776135237, w0=73.02483999999994, w1=13.425274895313084\n",
      "SubGD iter. 141/499: loss=4.428916084676415, w0=73.04625999999995, w1=13.436222021322303\n",
      "SubGD iter. 142/499: loss=4.428112214589719, w0=73.06613999999995, w1=13.447440046601253\n",
      "SubGD iter. 143/499: loss=4.427409042227933, w0=73.08405999999995, w1=13.457856669597893\n",
      "SubGD iter. 144/499: loss=4.426825784996948, w0=73.10029999999995, w1=13.467335162734873\n",
      "SubGD iter. 145/499: loss=4.42636960336517, w0=73.11401999999995, w1=13.475393628220223\n",
      "SubGD iter. 146/499: loss=4.426030178383452, w0=73.12619999999995, w1=13.482212864437892\n",
      "SubGD iter. 147/499: loss=4.425766799370374, w0=73.13697999999995, w1=13.48799725691146\n",
      "SubGD iter. 148/499: loss=4.4255559290079125, w0=73.14747999999994, w1=13.493479439140252\n",
      "SubGD iter. 149/499: loss=4.425362490640805, w0=73.15699999999994, w1=13.498468271254364\n",
      "SubGD iter. 150/499: loss=4.425203617363584, w0=73.16595999999994, w1=13.50294680429713\n",
      "SubGD iter. 151/499: loss=4.4250685279472615, w0=73.17365999999994, w1=13.506576029236252\n",
      "SubGD iter. 152/499: loss=4.424972782434856, w0=73.18009999999994, w1=13.509768665455107\n",
      "SubGD iter. 153/499: loss=4.424903121175712, w0=73.18597999999994, w1=13.512571468366584\n",
      "SubGD iter. 154/499: loss=4.424846327975847, w0=73.19143999999994, w1=13.515117410468855\n",
      "SubGD iter. 155/499: loss=4.42479923253376, w0=73.19605999999995, w1=13.517185961787685\n",
      "SubGD iter. 156/499: loss=4.4247664094040005, w0=73.19969999999995, w1=13.518069930874283\n",
      "SubGD iter. 157/499: loss=4.424747663519319, w0=73.20291999999995, w1=13.518854137767013\n",
      "SubGD iter. 158/499: loss=4.424732182668888, w0=73.20599999999995, w1=13.51952983926887\n",
      "SubGD iter. 159/499: loss=4.424717978422431, w0=73.20907999999994, w1=13.520205540770727\n",
      "SubGD iter. 160/499: loss=4.424704053977274, w0=73.21201999999994, w1=13.521000216270515\n",
      "SubGD iter. 161/499: loss=4.4246908038213455, w0=73.21495999999993, w1=13.521794891770304\n",
      "SubGD iter. 162/499: loss=4.424678207991666, w0=73.21747999999994, w1=13.522407209546943\n",
      "SubGD iter. 163/499: loss=4.42466891160565, w0=73.21985999999994, w1=13.522732908460886\n",
      "SubGD iter. 164/499: loss=4.424660947850395, w0=73.22181999999994, w1=13.522991164028737\n",
      "SubGD iter. 165/499: loss=4.424655537307837, w0=73.22363999999993, w1=13.523086524955808\n",
      "SubGD iter. 166/499: loss=4.424651183353665, w0=73.22503999999994, w1=13.522757010157395\n",
      "SubGD iter. 167/499: loss=4.424648228239375, w0=73.22643999999994, w1=13.522427495358983\n",
      "SubGD iter. 168/499: loss=4.424645273125087, w0=73.22783999999994, w1=13.52209798056057\n",
      "SubGD iter. 169/499: loss=4.424642498283796, w0=73.22895999999994, w1=13.522225963141242\n",
      "SubGD iter. 170/499: loss=4.424640682884451, w0=73.23007999999994, w1=13.522353945721914\n",
      "SubGD iter. 171/499: loss=4.424638867485108, w0=73.23119999999994, w1=13.522481928302586\n",
      "SubGD iter. 172/499: loss=4.424637300852184, w0=73.23189999999994, w1=13.523013495747021\n",
      "SubGD iter. 173/499: loss=4.424636258655271, w0=73.23273999999994, w1=13.52326627956579\n",
      "SubGD iter. 174/499: loss=4.424635339412462, w0=73.23329999999993, w1=13.523741235074766\n",
      "SubGD iter. 175/499: loss=4.424634640448436, w0=73.23399999999992, w1=13.523937406958076\n",
      "SubGD iter. 176/499: loss=4.424633962890471, w0=73.23455999999992, w1=13.523888604707691\n",
      "SubGD iter. 177/499: loss=4.424633633499304, w0=73.23497999999992, w1=13.524118586082972\n",
      "SubGD iter. 178/499: loss=4.424633305940115, w0=73.23539999999993, w1=13.524348567458253\n",
      "SubGD iter. 179/499: loss=4.424632978380925, w0=73.23581999999993, w1=13.524578548833533\n",
      "SubGD iter. 180/499: loss=4.424632650821734, w0=73.23623999999994, w1=13.524808530208814\n",
      "SubGD iter. 181/499: loss=4.424632389924093, w0=73.23651999999994, w1=13.524964267899264\n",
      "SubGD iter. 182/499: loss=4.424632243275195, w0=73.23679999999995, w1=13.525120005589715\n",
      "SubGD iter. 183/499: loss=4.424632096626298, w0=73.23707999999995, w1=13.525275743280165\n",
      "SubGD iter. 184/499: loss=4.4246319933287, w0=73.23721999999995, w1=13.525174424196196\n",
      "SubGD iter. 185/499: loss=4.42463195066362, w0=73.23735999999995, w1=13.525073105112227\n",
      "SubGD iter. 186/499: loss=4.424631907998538, w0=73.23749999999995, w1=13.524971786028258\n",
      "SubGD iter. 187/499: loss=4.424631865333457, w0=73.23763999999996, w1=13.52487046694429\n",
      "SubGD iter. 188/499: loss=4.424631822668376, w0=73.23777999999996, w1=13.52476914786032\n",
      "SubGD iter. 189/499: loss=4.424631782685974, w0=73.23805999999996, w1=13.52492488555077\n",
      "SubGD iter. 190/499: loss=4.42463174654501, w0=73.23819999999996, w1=13.524823566466802\n",
      "SubGD iter. 191/499: loss=4.424631703879928, w0=73.23833999999997, w1=13.524722247382833\n",
      "SubGD iter. 192/499: loss=4.424631661214847, w0=73.23847999999997, w1=13.524620928298864\n",
      "SubGD iter. 193/499: loss=4.424631622214843, w0=73.23847999999997, w1=13.524586781150308\n",
      "SubGD iter. 194/499: loss=4.42463162054909, w0=73.23847999999997, w1=13.524552634001752\n",
      "SubGD iter. 195/499: loss=4.424631618883335, w0=73.23847999999997, w1=13.524518486853196\n",
      "SubGD iter. 196/499: loss=4.424631617217582, w0=73.23847999999997, w1=13.52448433970464\n",
      "SubGD iter. 197/499: loss=4.424631615551827, w0=73.23847999999997, w1=13.524450192556085\n",
      "SubGD iter. 198/499: loss=4.424631613886073, w0=73.23847999999997, w1=13.524416045407529\n",
      "SubGD iter. 199/499: loss=4.4246316122203195, w0=73.23847999999997, w1=13.524381898258973\n",
      "SubGD iter. 200/499: loss=4.424631615444578, w0=73.23861999999997, w1=13.524604807884836\n",
      "SubGD iter. 201/499: loss=4.424631621428463, w0=73.23861999999997, w1=13.52457066073628\n",
      "SubGD iter. 202/499: loss=4.4246316197627085, w0=73.23861999999997, w1=13.524536513587725\n",
      "SubGD iter. 203/499: loss=4.424631618096955, w0=73.23861999999997, w1=13.524502366439169\n",
      "SubGD iter. 204/499: loss=4.424631616431201, w0=73.23861999999997, w1=13.524468219290613\n",
      "SubGD iter. 205/499: loss=4.424631614765446, w0=73.23861999999997, w1=13.524434072142057\n",
      "SubGD iter. 206/499: loss=4.4246316130996926, w0=73.23861999999997, w1=13.524399924993501\n",
      "SubGD iter. 207/499: loss=4.42463161143394, w0=73.23861999999997, w1=13.524365777844945\n",
      "SubGD iter. 208/499: loss=4.424631609768185, w0=73.23861999999997, w1=13.52433163069639\n",
      "SubGD iter. 209/499: loss=4.424631608102431, w0=73.23861999999997, w1=13.524297483547834\n",
      "SubGD iter. 210/499: loss=4.4246316143257935, w0=73.23875999999997, w1=13.524520393173697\n",
      "SubGD iter. 211/499: loss=4.424631617310575, w0=73.23875999999997, w1=13.524486246025141\n",
      "SubGD iter. 212/499: loss=4.42463161564482, w0=73.23875999999997, w1=13.524452098876585\n",
      "SubGD iter. 213/499: loss=4.424631613979067, w0=73.23875999999997, w1=13.52441795172803\n",
      "SubGD iter. 214/499: loss=4.424631612313313, w0=73.23875999999997, w1=13.524383804579474\n",
      "SubGD iter. 215/499: loss=4.42463161064756, w0=73.23875999999997, w1=13.524349657430918\n",
      "SubGD iter. 216/499: loss=4.424631608981805, w0=73.23875999999997, w1=13.524315510282362\n",
      "SubGD iter. 217/499: loss=4.4246316073160505, w0=73.23875999999997, w1=13.524281363133806\n",
      "SubGD iter. 218/499: loss=4.424631605650298, w0=73.23875999999997, w1=13.52424721598525\n",
      "SubGD iter. 219/499: loss=4.424631603984544, w0=73.23875999999997, w1=13.524213068836694\n",
      "SubGD iter. 220/499: loss=4.42463161320701, w0=73.23889999999997, w1=13.524435978462558\n",
      "SubGD iter. 221/499: loss=4.424631613192687, w0=73.23889999999997, w1=13.524401831314002\n",
      "SubGD iter. 222/499: loss=4.424631611526934, w0=73.23889999999997, w1=13.524367684165446\n",
      "SubGD iter. 223/499: loss=4.424631609861179, w0=73.23889999999997, w1=13.52433353701689\n",
      "SubGD iter. 224/499: loss=4.4246316081954244, w0=73.23889999999997, w1=13.524299389868334\n",
      "SubGD iter. 225/499: loss=4.424631606529672, w0=73.23889999999997, w1=13.524265242719778\n",
      "SubGD iter. 226/499: loss=4.424631604863916, w0=73.23889999999997, w1=13.524231095571222\n",
      "SubGD iter. 227/499: loss=4.424631603198162, w0=73.23889999999997, w1=13.524196948422667\n",
      "SubGD iter. 228/499: loss=4.4246316015324085, w0=73.23889999999997, w1=13.52416280127411\n",
      "SubGD iter. 229/499: loss=4.42463160121433, w0=73.23903999999997, w1=13.524385710899974\n",
      "SubGD iter. 230/499: loss=4.424631610740552, w0=73.23903999999997, w1=13.524351563751418\n",
      "SubGD iter. 231/499: loss=4.424631609074798, w0=73.23903999999997, w1=13.524317416602862\n",
      "SubGD iter. 232/499: loss=4.424631607409045, w0=73.23903999999997, w1=13.524283269454306\n",
      "SubGD iter. 233/499: loss=4.42463160574329, w0=73.23903999999997, w1=13.52424912230575\n",
      "SubGD iter. 234/499: loss=4.424631604077537, w0=73.23903999999997, w1=13.524214975157195\n",
      "SubGD iter. 235/499: loss=4.424631602411782, w0=73.23903999999997, w1=13.524180828008639\n",
      "SubGD iter. 236/499: loss=4.42463160074603, w0=73.23903999999997, w1=13.524146680860083\n",
      "SubGD iter. 237/499: loss=4.424631599080274, w0=73.23903999999997, w1=13.524112533711527\n",
      "SubGD iter. 238/499: loss=4.424631597414521, w0=73.23903999999997, w1=13.524078386562971\n",
      "SubGD iter. 239/499: loss=4.424631600095546, w0=73.23917999999998, w1=13.524301296188835\n",
      "SubGD iter. 240/499: loss=4.424631606622665, w0=73.23917999999998, w1=13.524267149040279\n",
      "SubGD iter. 241/499: loss=4.42463160495691, w0=73.23917999999998, w1=13.524233001891723\n",
      "SubGD iter. 242/499: loss=4.424631603291156, w0=73.23917999999998, w1=13.524198854743167\n",
      "SubGD iter. 243/499: loss=4.424631601625403, w0=73.23917999999998, w1=13.524164707594611\n",
      "SubGD iter. 244/499: loss=4.424631599959649, w0=73.23917999999998, w1=13.524130560446055\n",
      "SubGD iter. 245/499: loss=4.424631598293894, w0=73.23917999999998, w1=13.5240964132975\n",
      "SubGD iter. 246/499: loss=4.4246315966281395, w0=73.23917999999998, w1=13.524062266148944\n",
      "SubGD iter. 247/499: loss=4.424631594962387, w0=73.23917999999998, w1=13.524028119000388\n",
      "SubGD iter. 248/499: loss=4.424631593296633, w0=73.23917999999998, w1=13.523993971851832\n",
      "SubGD iter. 249/499: loss=4.424631598976763, w0=73.23931999999998, w1=13.524216881477695\n",
      "SubGD iter. 250/499: loss=4.424631602504776, w0=73.23931999999998, w1=13.52418273432914\n",
      "SubGD iter. 251/499: loss=4.424631600839023, w0=73.23931999999998, w1=13.524148587180584\n",
      "SubGD iter. 252/499: loss=4.424631599173268, w0=73.23931999999998, w1=13.524114440032028\n",
      "SubGD iter. 253/499: loss=4.424631597507515, w0=73.23931999999998, w1=13.524080292883472\n",
      "SubGD iter. 254/499: loss=4.424631595841761, w0=73.23931999999998, w1=13.524046145734916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 255/499: loss=4.424631594176007, w0=73.23931999999998, w1=13.52401199858636\n",
      "SubGD iter. 256/499: loss=4.424631592510252, w0=73.23931999999998, w1=13.523977851437804\n",
      "SubGD iter. 257/499: loss=4.424631590844498, w0=73.23931999999998, w1=13.523943704289248\n",
      "SubGD iter. 258/499: loss=4.424631589178745, w0=73.23931999999998, w1=13.523909557140692\n",
      "SubGD iter. 259/499: loss=4.424631597857979, w0=73.23945999999998, w1=13.524132466766556\n",
      "SubGD iter. 260/499: loss=4.424631598386889, w0=73.23945999999998, w1=13.524098319618\n",
      "SubGD iter. 261/499: loss=4.424631596721134, w0=73.23945999999998, w1=13.524064172469444\n",
      "SubGD iter. 262/499: loss=4.42463159505538, w0=73.23945999999998, w1=13.524030025320888\n",
      "SubGD iter. 263/499: loss=4.424631593389626, w0=73.23945999999998, w1=13.523995878172332\n",
      "SubGD iter. 264/499: loss=4.424631591723871, w0=73.23945999999998, w1=13.523961731023777\n",
      "SubGD iter. 265/499: loss=4.424631590058119, w0=73.23945999999998, w1=13.52392758387522\n",
      "SubGD iter. 266/499: loss=4.424631588392365, w0=73.23945999999998, w1=13.523893436726665\n",
      "SubGD iter. 267/499: loss=4.424631586726611, w0=73.23945999999998, w1=13.523859289578109\n",
      "SubGD iter. 268/499: loss=4.4246315858653, w0=73.23959999999998, w1=13.524082199203972\n",
      "SubGD iter. 269/499: loss=4.424631595934755, w0=73.23959999999998, w1=13.524048052055416\n",
      "SubGD iter. 270/499: loss=4.424631594269, w0=73.23959999999998, w1=13.52401390490686\n",
      "SubGD iter. 271/499: loss=4.424631592603246, w0=73.23959999999998, w1=13.523979757758305\n",
      "SubGD iter. 272/499: loss=4.4246315909374925, w0=73.23959999999998, w1=13.523945610609749\n",
      "SubGD iter. 273/499: loss=4.424631589271738, w0=73.23959999999998, w1=13.523911463461193\n",
      "SubGD iter. 274/499: loss=4.424631587605985, w0=73.23959999999998, w1=13.523877316312637\n",
      "SubGD iter. 275/499: loss=4.42463158594023, w0=73.23959999999998, w1=13.523843169164081\n",
      "SubGD iter. 276/499: loss=4.424631584274477, w0=73.23959999999998, w1=13.523809022015525\n",
      "SubGD iter. 277/499: loss=4.424631582608722, w0=73.23959999999998, w1=13.52377487486697\n",
      "SubGD iter. 278/499: loss=4.424631584746516, w0=73.23973999999998, w1=13.523997784492833\n",
      "SubGD iter. 279/499: loss=4.4246315918168655, w0=73.23973999999998, w1=13.523963637344277\n",
      "SubGD iter. 280/499: loss=4.424631590151112, w0=73.23973999999998, w1=13.523929490195721\n",
      "SubGD iter. 281/499: loss=4.424631588485358, w0=73.23973999999998, w1=13.523895343047165\n",
      "SubGD iter. 282/499: loss=4.424631586819604, w0=73.23973999999998, w1=13.52386119589861\n",
      "SubGD iter. 283/499: loss=4.42463158515385, w0=73.23973999999998, w1=13.523827048750054\n",
      "SubGD iter. 284/499: loss=4.424631583488097, w0=73.23973999999998, w1=13.523792901601498\n",
      "SubGD iter. 285/499: loss=4.424631581822342, w0=73.23973999999998, w1=13.523758754452942\n",
      "SubGD iter. 286/499: loss=4.424631580156589, w0=73.23973999999998, w1=13.523724607304386\n",
      "SubGD iter. 287/499: loss=4.4246315784908345, w0=73.23973999999998, w1=13.52369046015583\n",
      "SubGD iter. 288/499: loss=4.4246315836277335, w0=73.23987999999999, w1=13.523913369781694\n",
      "SubGD iter. 289/499: loss=4.424631587698977, w0=73.23987999999999, w1=13.523879222633138\n",
      "SubGD iter. 290/499: loss=4.424631586033223, w0=73.23987999999999, w1=13.523845075484582\n",
      "SubGD iter. 291/499: loss=4.424631584367471, w0=73.23987999999999, w1=13.523810928336026\n",
      "SubGD iter. 292/499: loss=4.424631582701715, w0=73.23987999999999, w1=13.52377678118747\n",
      "SubGD iter. 293/499: loss=4.424631581035962, w0=73.23987999999999, w1=13.523742634038914\n",
      "SubGD iter. 294/499: loss=4.4246315793702085, w0=73.23987999999999, w1=13.523708486890358\n",
      "SubGD iter. 295/499: loss=4.424631577704454, w0=73.23987999999999, w1=13.523674339741802\n",
      "SubGD iter. 296/499: loss=4.4246315760387, w0=73.23987999999999, w1=13.523640192593247\n",
      "SubGD iter. 297/499: loss=4.424631574372945, w0=73.23987999999999, w1=13.52360604544469\n",
      "SubGD iter. 298/499: loss=4.424631582508949, w0=73.24001999999999, w1=13.523828955070554\n",
      "SubGD iter. 299/499: loss=4.42463158358109, w0=73.24001999999999, w1=13.523794807921998\n",
      "SubGD iter. 300/499: loss=4.424631581915336, w0=73.24001999999999, w1=13.523760660773442\n",
      "SubGD iter. 301/499: loss=4.4246315802495815, w0=73.24001999999999, w1=13.523726513624887\n",
      "SubGD iter. 302/499: loss=4.424631578583828, w0=73.24001999999999, w1=13.52369236647633\n",
      "SubGD iter. 303/499: loss=4.424631576918074, w0=73.24001999999999, w1=13.523658219327775\n",
      "SubGD iter. 304/499: loss=4.424631575252319, w0=73.24001999999999, w1=13.523624072179219\n",
      "SubGD iter. 305/499: loss=4.4246315735865664, w0=73.24001999999999, w1=13.523589925030663\n",
      "SubGD iter. 306/499: loss=4.424631571920812, w0=73.24001999999999, w1=13.523555777882107\n",
      "SubGD iter. 307/499: loss=4.4246315705162695, w0=73.24015999999999, w1=13.52377868750797\n",
      "SubGD iter. 308/499: loss=4.4246315811289545, w0=73.24015999999999, w1=13.523744540359415\n",
      "SubGD iter. 309/499: loss=4.424631579463201, w0=73.24015999999999, w1=13.523710393210859\n",
      "SubGD iter. 310/499: loss=4.424631577797447, w0=73.24015999999999, w1=13.523676246062303\n",
      "SubGD iter. 311/499: loss=4.424631576131693, w0=73.24015999999999, w1=13.523642098913747\n",
      "SubGD iter. 312/499: loss=4.4246315744659395, w0=73.24015999999999, w1=13.523607951765191\n",
      "SubGD iter. 313/499: loss=4.424631572800186, w0=73.24015999999999, w1=13.523573804616635\n",
      "SubGD iter. 314/499: loss=4.424631571134431, w0=73.24015999999999, w1=13.52353965746808\n",
      "SubGD iter. 315/499: loss=4.424631569468678, w0=73.24015999999999, w1=13.523505510319524\n",
      "SubGD iter. 316/499: loss=4.4246315678029235, w0=73.24015999999999, w1=13.523471363170968\n",
      "SubGD iter. 317/499: loss=4.424631569397485, w0=73.24029999999999, w1=13.523694272796831\n",
      "SubGD iter. 318/499: loss=4.424631577011067, w0=73.24029999999999, w1=13.523660125648275\n",
      "SubGD iter. 319/499: loss=4.424631575345313, w0=73.24029999999999, w1=13.52362597849972\n",
      "SubGD iter. 320/499: loss=4.42463157367956, w0=73.24029999999999, w1=13.523591831351164\n",
      "SubGD iter. 321/499: loss=4.424631572013805, w0=73.24029999999999, w1=13.523557684202608\n",
      "SubGD iter. 322/499: loss=4.424631570348052, w0=73.24029999999999, w1=13.523523537054052\n",
      "SubGD iter. 323/499: loss=4.4246315686822975, w0=73.24029999999999, w1=13.523489389905496\n",
      "SubGD iter. 324/499: loss=4.424631567016544, w0=73.24029999999999, w1=13.52345524275694\n",
      "SubGD iter. 325/499: loss=4.42463156535079, w0=73.24029999999999, w1=13.523421095608384\n",
      "SubGD iter. 326/499: loss=4.424631563685036, w0=73.24029999999999, w1=13.523386948459828\n",
      "SubGD iter. 327/499: loss=4.424631568278702, w0=73.24043999999999, w1=13.523609858085692\n",
      "SubGD iter. 328/499: loss=4.42463157289318, w0=73.24043999999999, w1=13.523575710937136\n",
      "SubGD iter. 329/499: loss=4.424631571227425, w0=73.24043999999999, w1=13.52354156378858\n",
      "SubGD iter. 330/499: loss=4.424631569561671, w0=73.24043999999999, w1=13.523507416640024\n",
      "SubGD iter. 331/499: loss=4.424631567895918, w0=73.24043999999999, w1=13.523473269491468\n",
      "SubGD iter. 332/499: loss=4.424631566230163, w0=73.24043999999999, w1=13.523439122342912\n",
      "SubGD iter. 333/499: loss=4.424631564564409, w0=73.24043999999999, w1=13.523404975194357\n",
      "SubGD iter. 334/499: loss=4.4246315628986554, w0=73.24043999999999, w1=13.5233708280458\n",
      "SubGD iter. 335/499: loss=4.424631561232901, w0=73.24043999999999, w1=13.523336680897245\n",
      "SubGD iter. 336/499: loss=4.424631559567148, w0=73.24043999999999, w1=13.523302533748689\n",
      "SubGD iter. 337/499: loss=4.424631567159919, w0=73.24058, w1=13.523525443374552\n",
      "SubGD iter. 338/499: loss=4.4246315687752915, w0=73.24058, w1=13.523491296225997\n",
      "SubGD iter. 339/499: loss=4.424631567109536, w0=73.24058, w1=13.52345714907744\n",
      "SubGD iter. 340/499: loss=4.424631565443783, w0=73.24058, w1=13.523423001928885\n",
      "SubGD iter. 341/499: loss=4.424631563778029, w0=73.24058, w1=13.523388854780329\n",
      "SubGD iter. 342/499: loss=4.424631562112276, w0=73.24058, w1=13.523354707631773\n",
      "SubGD iter. 343/499: loss=4.424631560446521, w0=73.24058, w1=13.523320560483217\n",
      "SubGD iter. 344/499: loss=4.424631558780767, w0=73.24058, w1=13.523286413334661\n",
      "SubGD iter. 345/499: loss=4.424631557115013, w0=73.24058, w1=13.523252266186105\n",
      "SubGD iter. 346/499: loss=4.42463155544926, w0=73.24058, w1=13.52321811903755\n",
      "SubGD iter. 347/499: loss=4.424631566041135, w0=73.24072, w1=13.523441028663413\n",
      "SubGD iter. 348/499: loss=4.424631564657402, w0=73.24072, w1=13.523406881514857\n",
      "SubGD iter. 349/499: loss=4.4246315629916495, w0=73.24072, w1=13.523372734366301\n",
      "SubGD iter. 350/499: loss=4.424631561325894, w0=73.24072, w1=13.523338587217745\n",
      "SubGD iter. 351/499: loss=4.424631559660141, w0=73.24072, w1=13.52330444006919\n",
      "SubGD iter. 352/499: loss=4.424631557994387, w0=73.24072, w1=13.523270292920634\n",
      "SubGD iter. 353/499: loss=4.424631556328634, w0=73.24072, w1=13.523236145772078\n",
      "SubGD iter. 354/499: loss=4.424631554662879, w0=73.24072, w1=13.523201998623522\n",
      "SubGD iter. 355/499: loss=4.424631552997125, w0=73.24072, w1=13.523167851474966\n",
      "SubGD iter. 356/499: loss=4.424631554048455, w0=73.24086, w1=13.52339076110083\n",
      "SubGD iter. 357/499: loss=4.424631562205269, w0=73.24086, w1=13.523356613952274\n",
      "SubGD iter. 358/499: loss=4.424631560539514, w0=73.24086, w1=13.523322466803718\n",
      "SubGD iter. 359/499: loss=4.424631558873761, w0=73.24086, w1=13.523288319655162\n",
      "SubGD iter. 360/499: loss=4.4246315572080075, w0=73.24086, w1=13.523254172506606\n",
      "SubGD iter. 361/499: loss=4.424631555542253, w0=73.24086, w1=13.52322002535805\n",
      "SubGD iter. 362/499: loss=4.424631553876499, w0=73.24086, w1=13.523185878209494\n",
      "SubGD iter. 363/499: loss=4.4246315522107444, w0=73.24086, w1=13.523151731060938\n",
      "SubGD iter. 364/499: loss=4.424631550544991, w0=73.24086, w1=13.523117583912382\n",
      "SubGD iter. 365/499: loss=4.424631548879238, w0=73.24086, w1=13.523083436763827\n",
      "SubGD iter. 366/499: loss=4.424631552929672, w0=73.241, w1=13.52330634638969\n",
      "SubGD iter. 367/499: loss=4.4246315580873805, w0=73.241, w1=13.523272199241134\n",
      "SubGD iter. 368/499: loss=4.424631556421627, w0=73.241, w1=13.523238052092578\n",
      "SubGD iter. 369/499: loss=4.424631554755872, w0=73.241, w1=13.523203904944022\n",
      "SubGD iter. 370/499: loss=4.424631553090118, w0=73.241, w1=13.523169757795467\n",
      "SubGD iter. 371/499: loss=4.424631551424365, w0=73.241, w1=13.52313561064691\n",
      "SubGD iter. 372/499: loss=4.424631549758611, w0=73.241, w1=13.523101463498355\n",
      "SubGD iter. 373/499: loss=4.424631548092857, w0=73.241, w1=13.523067316349799\n",
      "SubGD iter. 374/499: loss=4.424631546427102, w0=73.241, w1=13.523033169201243\n",
      "SubGD iter. 375/499: loss=4.424631544761349, w0=73.241, w1=13.522999022052687\n",
      "SubGD iter. 376/499: loss=4.424631551810888, w0=73.24114, w1=13.52322193167855\n",
      "SubGD iter. 377/499: loss=4.424631553969493, w0=73.24114, w1=13.523187784529995\n",
      "SubGD iter. 378/499: loss=4.4246315523037385, w0=73.24114, w1=13.523153637381439\n",
      "SubGD iter. 379/499: loss=4.424631550637984, w0=73.24114, w1=13.523119490232883\n",
      "SubGD iter. 380/499: loss=4.424631548972231, w0=73.24114, w1=13.523085343084327\n",
      "SubGD iter. 381/499: loss=4.424631547306476, w0=73.24114, w1=13.523051195935771\n",
      "SubGD iter. 382/499: loss=4.4246315456407235, w0=73.24114, w1=13.523017048787215\n",
      "SubGD iter. 383/499: loss=4.42463154397497, w0=73.24114, w1=13.52298290163866\n",
      "SubGD iter. 384/499: loss=4.424631542309215, w0=73.24114, w1=13.522948754490104\n",
      "SubGD iter. 385/499: loss=4.42463154064346, w0=73.24114, w1=13.522914607341548\n",
      "SubGD iter. 386/499: loss=4.424631550692106, w0=73.24128, w1=13.523137516967411\n",
      "SubGD iter. 387/499: loss=4.424631549851605, w0=73.24128, w1=13.523103369818855\n",
      "SubGD iter. 388/499: loss=4.42463154818585, w0=73.24128, w1=13.5230692226703\n",
      "SubGD iter. 389/499: loss=4.4246315465200965, w0=73.24128, w1=13.523035075521744\n",
      "SubGD iter. 390/499: loss=4.424631544854343, w0=73.24128, w1=13.523000928373188\n",
      "SubGD iter. 391/499: loss=4.424631543188589, w0=73.24128, w1=13.522966781224632\n",
      "SubGD iter. 392/499: loss=4.4246315415228334, w0=73.24128, w1=13.522932634076076\n",
      "SubGD iter. 393/499: loss=4.424631539857081, w0=73.24128, w1=13.52289848692752\n",
      "SubGD iter. 394/499: loss=4.424631538191327, w0=73.24128, w1=13.522864339778964\n",
      "SubGD iter. 395/499: loss=4.424631538699424, w0=73.24142, w1=13.523087249404828\n",
      "SubGD iter. 396/499: loss=4.4246315473994695, w0=73.24142, w1=13.523053102256272\n",
      "SubGD iter. 397/499: loss=4.424631545733716, w0=73.24142, w1=13.523018955107716\n",
      "SubGD iter. 398/499: loss=4.424631544067962, w0=73.24142, w1=13.52298480795916\n",
      "SubGD iter. 399/499: loss=4.424631542402208, w0=73.24142, w1=13.522950660810604\n",
      "SubGD iter. 400/499: loss=4.4246315407364545, w0=73.24142, w1=13.522916513662048\n",
      "SubGD iter. 401/499: loss=4.424631539070701, w0=73.24142, w1=13.522882366513493\n",
      "SubGD iter. 402/499: loss=4.424631537404946, w0=73.24142, w1=13.522848219364937\n",
      "SubGD iter. 403/499: loss=4.424631535739192, w0=73.24142, w1=13.52281407221638\n",
      "SubGD iter. 404/499: loss=4.424631534073439, w0=73.24142, w1=13.522779925067825\n",
      "SubGD iter. 405/499: loss=4.424631537580641, w0=73.24156, w1=13.523002834693688\n",
      "SubGD iter. 406/499: loss=4.424631543281583, w0=73.24156, w1=13.522968687545132\n",
      "SubGD iter. 407/499: loss=4.4246315416158275, w0=73.24156, w1=13.522934540396577\n",
      "SubGD iter. 408/499: loss=4.424631539950075, w0=73.24156, w1=13.52290039324802\n",
      "SubGD iter. 409/499: loss=4.42463153828432, w0=73.24156, w1=13.522866246099465\n",
      "SubGD iter. 410/499: loss=4.424631536618566, w0=73.24156, w1=13.522832098950909\n",
      "SubGD iter. 411/499: loss=4.4246315349528125, w0=73.24156, w1=13.522797951802353\n",
      "SubGD iter. 412/499: loss=4.424631533287058, w0=73.24156, w1=13.522763804653797\n",
      "SubGD iter. 413/499: loss=4.424631531621304, w0=73.24156, w1=13.522729657505241\n",
      "SubGD iter. 414/499: loss=4.424631529955551, w0=73.24156, w1=13.522695510356685\n",
      "SubGD iter. 415/499: loss=4.424631536461858, w0=73.24170000000001, w1=13.522918419982549\n",
      "SubGD iter. 416/499: loss=4.424631540335803, w0=73.24156, w1=13.522905630681082\n",
      "SubGD iter. 417/499: loss=4.424631538539811, w0=73.24156, w1=13.522871483532526\n",
      "SubGD iter. 418/499: loss=4.424631536874057, w0=73.24156, w1=13.52283733638397\n",
      "SubGD iter. 419/499: loss=4.424631535208303, w0=73.24156, w1=13.522803189235415\n",
      "SubGD iter. 420/499: loss=4.424631533542549, w0=73.24156, w1=13.522769042086859\n",
      "SubGD iter. 421/499: loss=4.424631531876795, w0=73.24156, w1=13.522734894938303\n",
      "SubGD iter. 422/499: loss=4.424631530211041, w0=73.24156, w1=13.522700747789747\n",
      "SubGD iter. 423/499: loss=4.424631534794038, w0=73.24170000000001, w1=13.52292365741561\n",
      "SubGD iter. 424/499: loss=4.424631540431493, w0=73.24156, w1=13.522910868114144\n",
      "SubGD iter. 425/499: loss=4.424631538795301, w0=73.24156, w1=13.522876720965588\n",
      "SubGD iter. 426/499: loss=4.424631537129548, w0=73.24156, w1=13.522842573817032\n",
      "SubGD iter. 427/499: loss=4.424631535463793, w0=73.24156, w1=13.522808426668476\n",
      "SubGD iter. 428/499: loss=4.424631533798038, w0=73.24156, w1=13.52277427951992\n",
      "SubGD iter. 429/499: loss=4.4246315321322855, w0=73.24156, w1=13.522740132371364\n",
      "SubGD iter. 430/499: loss=4.424631530466532, w0=73.24156, w1=13.522705985222808\n",
      "SubGD iter. 431/499: loss=4.424631533126218, w0=73.24170000000001, w1=13.522928894848672\n",
      "SubGD iter. 432/499: loss=4.4246315405271845, w0=73.24156, w1=13.522916105547205\n",
      "SubGD iter. 433/499: loss=4.424631539050791, w0=73.24156, w1=13.52288195839865\n",
      "SubGD iter. 434/499: loss=4.424631537385038, w0=73.24156, w1=13.522847811250093\n",
      "SubGD iter. 435/499: loss=4.424631535719284, w0=73.24156, w1=13.522813664101538\n",
      "SubGD iter. 436/499: loss=4.424631534053531, w0=73.24156, w1=13.522779516952982\n",
      "SubGD iter. 437/499: loss=4.424631532387776, w0=73.24156, w1=13.522745369804426\n",
      "SubGD iter. 438/499: loss=4.424631530722022, w0=73.24156, w1=13.52271122265587\n",
      "SubGD iter. 439/499: loss=4.424631531458397, w0=73.24170000000001, w1=13.522934132281733\n",
      "SubGD iter. 440/499: loss=4.424631540622873, w0=73.24156, w1=13.522921342980267\n",
      "SubGD iter. 441/499: loss=4.424631539306283, w0=73.24156, w1=13.52288719583171\n",
      "SubGD iter. 442/499: loss=4.424631537640528, w0=73.24156, w1=13.522853048683155\n",
      "SubGD iter. 443/499: loss=4.424631535974775, w0=73.24156, w1=13.522818901534599\n",
      "SubGD iter. 444/499: loss=4.42463153430902, w0=73.24156, w1=13.522784754386043\n",
      "SubGD iter. 445/499: loss=4.424631532643267, w0=73.24156, w1=13.522750607237487\n",
      "SubGD iter. 446/499: loss=4.424631530977512, w0=73.24156, w1=13.522716460088931\n",
      "SubGD iter. 447/499: loss=4.424631529790577, w0=73.24170000000001, w1=13.522939369714795\n",
      "SubGD iter. 448/499: loss=4.424631540718564, w0=73.24156, w1=13.522926580413328\n",
      "SubGD iter. 449/499: loss=4.424631539561774, w0=73.24156, w1=13.522892433264772\n",
      "SubGD iter. 450/499: loss=4.424631537896019, w0=73.24156, w1=13.522858286116216\n",
      "SubGD iter. 451/499: loss=4.424631536230265, w0=73.24156, w1=13.52282413896766\n",
      "SubGD iter. 452/499: loss=4.4246315345645115, w0=73.24156, w1=13.522789991819105\n",
      "SubGD iter. 453/499: loss=4.424631532898758, w0=73.24156, w1=13.522755844670549\n",
      "SubGD iter. 454/499: loss=4.424631531233003, w0=73.24156, w1=13.522721697521993\n",
      "SubGD iter. 455/499: loss=4.42463152956725, w0=73.24156, w1=13.522687550373437\n",
      "SubGD iter. 456/499: loss=4.424631538996652, w0=73.24170000000001, w1=13.5229104599993\n",
      "SubGD iter. 457/499: loss=4.424631540190371, w0=73.24156, w1=13.522897670697834\n",
      "SubGD iter. 458/499: loss=4.42463153815151, w0=73.24156, w1=13.522863523549278\n",
      "SubGD iter. 459/499: loss=4.424631536485755, w0=73.24156, w1=13.522829376400722\n",
      "SubGD iter. 460/499: loss=4.424631534820002, w0=73.24156, w1=13.522795229252166\n",
      "SubGD iter. 461/499: loss=4.424631533154248, w0=73.24156, w1=13.52276108210361\n",
      "SubGD iter. 462/499: loss=4.424631531488494, w0=73.24156, w1=13.522726934955054\n",
      "SubGD iter. 463/499: loss=4.42463152982274, w0=73.24156, w1=13.522692787806498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 464/499: loss=4.424631537328832, w0=73.24170000000001, w1=13.522915697432362\n",
      "SubGD iter. 465/499: loss=4.424631540286062, w0=73.24156, w1=13.522902908130895\n",
      "SubGD iter. 466/499: loss=4.4246315384070005, w0=73.24156, w1=13.52286876098234\n",
      "SubGD iter. 467/499: loss=4.424631536741247, w0=73.24156, w1=13.522834613833783\n",
      "SubGD iter. 468/499: loss=4.424631535075492, w0=73.24156, w1=13.522800466685228\n",
      "SubGD iter. 469/499: loss=4.424631533409738, w0=73.24156, w1=13.522766319536672\n",
      "SubGD iter. 470/499: loss=4.424631531743984, w0=73.24156, w1=13.522732172388116\n",
      "SubGD iter. 471/499: loss=4.424631530078231, w0=73.24156, w1=13.52269802523956\n",
      "SubGD iter. 472/499: loss=4.424631535661011, w0=73.24170000000001, w1=13.522920934865423\n",
      "SubGD iter. 473/499: loss=4.4246315403817515, w0=73.24156, w1=13.522908145563957\n",
      "SubGD iter. 474/499: loss=4.42463153866249, w0=73.24156, w1=13.5228739984154\n",
      "SubGD iter. 475/499: loss=4.4246315369967375, w0=73.24156, w1=13.522839851266845\n",
      "SubGD iter. 476/499: loss=4.424631535330983, w0=73.24156, w1=13.522805704118289\n",
      "SubGD iter. 477/499: loss=4.42463153366523, w0=73.24156, w1=13.522771556969733\n",
      "SubGD iter. 478/499: loss=4.4246315319994745, w0=73.24156, w1=13.522737409821177\n",
      "SubGD iter. 479/499: loss=4.424631530333721, w0=73.24156, w1=13.522703262672621\n",
      "SubGD iter. 480/499: loss=4.4246315339931925, w0=73.24170000000001, w1=13.522926172298485\n",
      "SubGD iter. 481/499: loss=4.424631540477442, w0=73.24156, w1=13.522913382997018\n",
      "SubGD iter. 482/499: loss=4.424631538917981, w0=73.24156, w1=13.522879235848462\n",
      "SubGD iter. 483/499: loss=4.424631537252227, w0=73.24156, w1=13.522845088699906\n",
      "SubGD iter. 484/499: loss=4.424631535586474, w0=73.24156, w1=13.52281094155135\n",
      "SubGD iter. 485/499: loss=4.42463153392072, w0=73.24156, w1=13.522776794402795\n",
      "SubGD iter. 486/499: loss=4.424631532254966, w0=73.24156, w1=13.522742647254239\n",
      "SubGD iter. 487/499: loss=4.424631530589211, w0=73.24156, w1=13.522708500105683\n",
      "SubGD iter. 488/499: loss=4.424631532325372, w0=73.24170000000001, w1=13.522931409731546\n",
      "SubGD iter. 489/499: loss=4.424631540573132, w0=73.24156, w1=13.52291862043008\n",
      "SubGD iter. 490/499: loss=4.424631539173472, w0=73.24156, w1=13.522884473281524\n",
      "SubGD iter. 491/499: loss=4.424631537507717, w0=73.24156, w1=13.522850326132968\n",
      "SubGD iter. 492/499: loss=4.4246315358419634, w0=73.24156, w1=13.522816178984412\n",
      "SubGD iter. 493/499: loss=4.424631534176211, w0=73.24156, w1=13.522782031835856\n",
      "SubGD iter. 494/499: loss=4.424631532510456, w0=73.24156, w1=13.5227478846873\n",
      "SubGD iter. 495/499: loss=4.424631530844702, w0=73.24156, w1=13.522713737538744\n",
      "SubGD iter. 496/499: loss=4.424631530657551, w0=73.24170000000001, w1=13.522936647164608\n",
      "SubGD iter. 497/499: loss=4.424631540668822, w0=73.24156, w1=13.522923857863141\n",
      "SubGD iter. 498/499: loss=4.424631539428962, w0=73.24156, w1=13.522889710714585\n",
      "SubGD iter. 499/499: loss=4.424631537763209, w0=73.24156, w1=13.52285556356603\n",
      "SubGD: execution time=0.406 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae00575e14a40a78f219da0bb3eb338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widgâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses, subgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "            \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        # ***************************************************\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad = compute_subgradient_mae(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            e = y - np.dot(tx_batch,w)\n",
    "            loss = calculate_mae(e)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        \n",
    "        print(\"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=72.42805121939259, w0=0.7, w1=0.3407485112202282\n",
      "SubSGD iter. 1/499: loss=71.88417147414519, w0=1.4, w1=0.3597224970772388\n",
      "SubSGD iter. 2/499: loss=70.90051720294385, w0=2.0999999999999996, w1=0.6674403991785984\n",
      "SubSGD iter. 3/499: loss=70.64623644887264, w0=2.8, w1=0.40263361538542247\n",
      "SubSGD iter. 4/499: loss=69.34693744817888, w0=3.5, w1=0.7958062060910234\n",
      "SubSGD iter. 5/499: loss=64.6092399838571, w0=4.2, w1=2.2138337989111063\n",
      "SubSGD iter. 6/499: loss=68.28375270736848, w0=4.9, w1=2.248137092326317\n",
      "SubSGD iter. 7/499: loss=68.21451774479219, w0=5.6000000000000005, w1=2.0722842793127665\n",
      "SubSGD iter. 8/499: loss=66.17188158323857, w0=6.300000000000001, w1=2.320283419570055\n",
      "SubSGD iter. 9/499: loss=64.56751704778688, w0=7.000000000000001, w1=2.758394585523325\n",
      "SubSGD iter. 10/499: loss=64.96975450820655, w0=7.700000000000001, w1=2.9086097348746915\n",
      "SubSGD iter. 11/499: loss=67.38637131991541, w0=8.4, w1=2.062817911656442\n",
      "SubSGD iter. 12/499: loss=65.57980131201916, w0=9.1, w1=1.3374954096658254\n",
      "SubSGD iter. 13/499: loss=62.63798969723524, w0=9.799999999999999, w1=1.6916729739157765\n",
      "SubSGD iter. 14/499: loss=63.570352953671545, w0=10.499999999999998, w1=1.2604901937697304\n",
      "SubSGD iter. 15/499: loss=62.03047102938659, w0=11.199999999999998, w1=1.2947934871849411\n",
      "SubSGD iter. 16/499: loss=59.64079765078687, w0=11.899999999999997, w1=1.9304827070169568\n",
      "SubSGD iter. 17/499: loss=50.79413084327985, w0=12.599999999999996, w1=3.769085978608201\n",
      "SubSGD iter. 18/499: loss=63.7975008184511, w0=13.299999999999995, w1=2.827409420509014\n",
      "SubSGD iter. 19/499: loss=61.45927420544885, w0=13.999999999999995, w1=2.108553230045127\n",
      "SubSGD iter. 20/499: loss=54.372765839088196, w0=14.699999999999994, w1=3.070785602132544\n",
      "SubSGD iter. 21/499: loss=58.90746907282125, w0=15.399999999999993, w1=2.8191171110584468\n",
      "SubSGD iter. 22/499: loss=59.00217711178367, w0=16.099999999999994, w1=2.258720933811897\n",
      "SubSGD iter. 23/499: loss=51.865488877931085, w0=16.799999999999994, w1=3.2542977015394525\n",
      "SubSGD iter. 24/499: loss=55.46968380004177, w0=17.499999999999993, w1=3.3226075273542164\n",
      "SubSGD iter. 25/499: loss=54.64926539961521, w0=18.199999999999992, w1=3.4137848097802332\n",
      "SubSGD iter. 26/499: loss=53.64348729023164, w0=18.89999999999999, w1=3.5612887973000675\n",
      "SubSGD iter. 27/499: loss=57.88586498834609, w0=19.59999999999999, w1=2.266786748250543\n",
      "SubSGD iter. 28/499: loss=53.591522255353034, w0=20.29999999999999, w1=2.0641244648478105\n",
      "SubSGD iter. 29/499: loss=53.62044740539464, w0=20.99999999999999, w1=1.4016358641851456\n",
      "SubSGD iter. 30/499: loss=51.29287344589897, w0=21.69999999999999, w1=1.5386005788971358\n",
      "SubSGD iter. 31/499: loss=51.40081704272819, w0=22.399999999999988, w1=1.2561230569004853\n",
      "SubSGD iter. 32/499: loss=50.62320517755537, w0=23.099999999999987, w1=0.3215275813838131\n",
      "SubSGD iter. 33/499: loss=49.530430454309375, w0=23.799999999999986, w1=0.1437614421865586\n",
      "SubSGD iter. 34/499: loss=47.77111332997283, w0=24.499999999999986, w1=-0.7773147267027738\n",
      "SubSGD iter. 35/499: loss=47.51839031700505, w0=25.199999999999985, w1=-1.132919901477112\n",
      "SubSGD iter. 36/499: loss=47.58525401429882, w0=25.899999999999984, w1=-0.998830702819665\n",
      "SubSGD iter. 37/499: loss=46.97707832150071, w0=26.599999999999984, w1=-0.2731273277608881\n",
      "SubSGD iter. 38/499: loss=46.02030563434874, w0=27.299999999999983, w1=-0.12310663144505676\n",
      "SubSGD iter. 39/499: loss=45.114435084894474, w0=27.999999999999982, w1=-0.4213166444873351\n",
      "SubSGD iter. 40/499: loss=44.651544109669295, w0=28.69999999999998, w1=-0.27423074893668986\n",
      "SubSGD iter. 41/499: loss=43.899179740126385, w0=29.39999999999998, w1=-0.014139223331387896\n",
      "SubSGD iter. 42/499: loss=42.45881818594879, w0=30.09999999999998, w1=0.710445033566071\n",
      "SubSGD iter. 43/499: loss=41.816718679321504, w0=30.79999999999998, w1=-0.41972408550113216\n",
      "SubSGD iter. 44/499: loss=41.7401116140285, w0=31.49999999999998, w1=-0.4963017069319724\n",
      "SubSGD iter. 45/499: loss=38.31376564662161, w0=32.19999999999998, w1=-1.665761544862815\n",
      "SubSGD iter. 46/499: loss=41.23820770715275, w0=32.899999999999984, w1=-1.1540805692207696\n",
      "SubSGD iter. 47/499: loss=38.05683631774576, w0=33.59999999999999, w1=-1.7940108859262582\n",
      "SubSGD iter. 48/499: loss=35.17878533649983, w0=34.29999999999999, w1=-2.76267897918458\n",
      "SubSGD iter. 49/499: loss=37.41205150470503, w0=34.99999999999999, w1=-2.971321412733334\n",
      "SubSGD iter. 50/499: loss=36.60530241284441, w0=35.699999999999996, w1=-3.189412762199827\n",
      "SubSGD iter. 51/499: loss=34.603604654538834, w0=36.4, w1=-3.63258411965482\n",
      "SubSGD iter. 52/499: loss=39.912458558174464, w0=37.1, w1=-2.651143573555123\n",
      "SubSGD iter. 53/499: loss=31.972371016390667, w0=37.800000000000004, w1=-3.3843059862884743\n",
      "SubSGD iter. 54/499: loss=35.12290274287534, w0=38.50000000000001, w1=-3.3165640482329914\n",
      "SubSGD iter. 55/499: loss=30.731237929963612, w0=39.20000000000001, w1=-3.9230418157092855\n",
      "SubSGD iter. 56/499: loss=29.55919995087582, w0=39.90000000000001, w1=-4.524699027768643\n",
      "SubSGD iter. 57/499: loss=27.13616428374227, w0=40.600000000000016, w1=-5.2779879632861695\n",
      "SubSGD iter. 58/499: loss=30.12071975279043, w0=41.30000000000002, w1=-5.52116101700379\n",
      "SubSGD iter. 59/499: loss=31.392656145537064, w0=42.00000000000002, w1=-5.512238092599972\n",
      "SubSGD iter. 60/499: loss=24.50202261831715, w0=42.700000000000024, w1=-6.228006932154507\n",
      "SubSGD iter. 61/499: loss=33.13981604948893, w0=43.40000000000003, w1=-5.840869317674565\n",
      "SubSGD iter. 62/499: loss=21.396600528031293, w0=44.10000000000003, w1=-6.721238583942812\n",
      "SubSGD iter. 63/499: loss=30.501460045152932, w0=44.80000000000003, w1=-6.509582930437596\n",
      "SubSGD iter. 64/499: loss=20.539956935082824, w0=44.10000000000003, w1=-5.244922941414438\n",
      "SubSGD iter. 65/499: loss=30.93761512452268, w0=44.80000000000003, w1=-4.900677861558601\n",
      "SubSGD iter. 66/499: loss=23.346293343045055, w0=45.500000000000036, w1=-5.514025245226913\n",
      "SubSGD iter. 67/499: loss=28.385232906469565, w0=46.20000000000004, w1=-5.355875845292525\n",
      "SubSGD iter. 68/499: loss=21.73529525937447, w0=46.90000000000004, w1=-5.972081249424794\n",
      "SubSGD iter. 69/499: loss=28.89585169825604, w0=47.600000000000044, w1=-5.578908658719193\n",
      "SubSGD iter. 70/499: loss=27.667351585237174, w0=48.30000000000005, w1=-5.2352008363712255\n",
      "SubSGD iter. 71/499: loss=22.168360457842713, w0=49.00000000000005, w1=-5.569265056873993\n",
      "SubSGD iter. 72/499: loss=26.031683647015115, w0=49.70000000000005, w1=-5.268579066459172\n",
      "SubSGD iter. 73/499: loss=22.003445434685247, w0=50.400000000000055, w1=-5.454306109768544\n",
      "SubSGD iter. 74/499: loss=21.17057517992666, w0=51.10000000000006, w1=-5.666723448812257\n",
      "SubSGD iter. 75/499: loss=27.583608258499023, w0=51.80000000000006, w1=-4.792737964185256\n",
      "SubSGD iter. 76/499: loss=19.65075773544203, w0=52.500000000000064, w1=-5.0920651605809555\n",
      "SubSGD iter. 77/499: loss=24.92344285602069, w0=53.20000000000007, w1=-4.355116900576366\n",
      "SubSGD iter. 78/499: loss=22.738590141408753, w0=53.90000000000007, w1=-3.8265233139493127\n",
      "SubSGD iter. 79/499: loss=19.465420400645566, w0=54.60000000000007, w1=-3.882927458684423\n",
      "SubSGD iter. 80/499: loss=20.81045269605381, w0=55.300000000000075, w1=-3.469042538101632\n",
      "SubSGD iter. 81/499: loss=20.173295581240176, w0=56.00000000000008, w1=-3.0095404681148223\n",
      "SubSGD iter. 82/499: loss=18.809485072761497, w0=56.70000000000008, w1=-2.7743035376791045\n",
      "SubSGD iter. 83/499: loss=18.088856116931613, w0=57.400000000000084, w1=-2.59904204921488\n",
      "SubSGD iter. 84/499: loss=17.480003631437512, w0=56.70000000000008, w1=-2.3206912860754905\n",
      "SubSGD iter. 85/499: loss=18.508010087631714, w0=57.400000000000084, w1=-1.858354563073362\n",
      "SubSGD iter. 86/499: loss=17.732370765271426, w0=58.10000000000009, w1=-1.451111373529375\n",
      "SubSGD iter. 87/499: loss=16.714940993911366, w0=58.80000000000009, w1=-0.004756247880013165\n",
      "SubSGD iter. 88/499: loss=18.467021287210706, w0=58.10000000000009, w1=1.1422949191940488\n",
      "SubSGD iter. 89/499: loss=16.865862553873345, w0=58.80000000000009, w1=0.17970387797687337\n",
      "SubSGD iter. 90/499: loss=15.447047315070163, w0=59.50000000000009, w1=1.1072807952232164\n",
      "SubSGD iter. 91/499: loss=15.935100174617626, w0=60.200000000000095, w1=1.0281066776628378\n",
      "SubSGD iter. 92/499: loss=15.13544260002916, w0=60.9000000000001, w1=1.3700364601927402\n",
      "SubSGD iter. 93/499: loss=14.402373749498604, w0=61.6000000000001, w1=1.930028884083758\n",
      "SubSGD iter. 94/499: loss=13.752796796498215, w0=62.300000000000104, w1=2.6105529066315123\n",
      "SubSGD iter. 95/499: loss=15.112034810075128, w0=63.00000000000011, w1=2.162444260499843\n",
      "SubSGD iter. 96/499: loss=12.683108239048812, w0=63.70000000000011, w1=3.3192896899419004\n",
      "SubSGD iter. 97/499: loss=13.33032143263591, w0=64.4000000000001, w1=3.6630707308982795\n",
      "SubSGD iter. 98/499: loss=13.954362765026229, w0=65.10000000000011, w1=3.497799291316356\n",
      "SubSGD iter. 99/499: loss=12.66316334377085, w0=65.80000000000011, w1=4.09824467511657\n",
      "SubSGD iter. 100/499: loss=12.473413966512917, w0=66.50000000000011, w1=4.750497330229145\n",
      "SubSGD iter. 101/499: loss=13.745601209955494, w0=65.80000000000011, w1=4.885342215715995\n",
      "SubSGD iter. 102/499: loss=13.160102807253937, w0=66.50000000000011, w1=4.933945171308072\n",
      "SubSGD iter. 103/499: loss=14.050291969172001, w0=67.20000000000012, w1=4.431499232105338\n",
      "SubSGD iter. 104/499: loss=16.377891569088842, w0=66.50000000000011, w1=5.366221574005593\n",
      "SubSGD iter. 105/499: loss=14.357476326432632, w0=65.80000000000011, w1=5.680384945102887\n",
      "SubSGD iter. 106/499: loss=13.454228007185849, w0=66.50000000000011, w1=5.588991599262796\n",
      "SubSGD iter. 107/499: loss=13.925230545522144, w0=67.20000000000012, w1=7.051625877030942\n",
      "SubSGD iter. 108/499: loss=13.223474464532844, w0=66.50000000000011, w1=7.042218700441827\n",
      "SubSGD iter. 109/499: loss=16.917810088405663, w0=67.20000000000012, w1=6.028837514635606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 110/499: loss=15.168204747558844, w0=67.90000000000012, w1=5.161516830178809\n",
      "SubSGD iter. 111/499: loss=12.761328350571182, w0=68.60000000000012, w1=5.176445058060693\n",
      "SubSGD iter. 112/499: loss=12.377052751917011, w0=69.30000000000013, w1=5.637703064471386\n",
      "SubSGD iter. 113/499: loss=12.382516788435375, w0=70.00000000000013, w1=5.947934720499034\n",
      "SubSGD iter. 114/499: loss=13.60345620750499, w0=69.30000000000013, w1=6.387537689954164\n",
      "SubSGD iter. 115/499: loss=14.209799662751927, w0=68.60000000000012, w1=6.8940480973043385\n",
      "SubSGD iter. 116/499: loss=12.440463638168469, w0=69.30000000000013, w1=7.096277483203932\n",
      "SubSGD iter. 117/499: loss=15.036430165944862, w0=70.00000000000013, w1=8.322307604620933\n",
      "SubSGD iter. 118/499: loss=15.719718251663267, w0=70.70000000000013, w1=9.449134007711656\n",
      "SubSGD iter. 119/499: loss=17.961749560661996, w0=70.00000000000013, w1=10.334538044512717\n",
      "SubSGD iter. 120/499: loss=15.428001396924996, w0=70.70000000000013, w1=11.246990766749292\n",
      "SubSGD iter. 121/499: loss=18.369917052504125, w0=71.40000000000013, w1=12.310995644127821\n",
      "SubSGD iter. 122/499: loss=17.42876051633189, w0=72.10000000000014, w1=13.19418932157913\n",
      "SubSGD iter. 123/499: loss=21.43686690607501, w0=71.40000000000013, w1=14.130194253951945\n",
      "SubSGD iter. 124/499: loss=14.929123224871171, w0=70.70000000000013, w1=14.549437565404395\n",
      "SubSGD iter. 125/499: loss=12.38666425574605, w0=71.40000000000013, w1=14.670324955662641\n",
      "SubSGD iter. 126/499: loss=21.414321521595323, w0=72.10000000000014, w1=13.668187876058747\n",
      "SubSGD iter. 127/499: loss=17.974800760969202, w0=71.40000000000013, w1=14.373854284893657\n",
      "SubSGD iter. 128/499: loss=13.01058688213172, w0=72.10000000000014, w1=14.694336213687547\n",
      "SubSGD iter. 129/499: loss=12.440167483424677, w0=71.40000000000013, w1=14.69650357982544\n",
      "SubSGD iter. 130/499: loss=17.993339099202714, w0=70.70000000000013, w1=15.327491451951127\n",
      "SubSGD iter. 131/499: loss=17.160154335344455, w0=70.00000000000013, w1=14.43850259810683\n",
      "SubSGD iter. 132/499: loss=13.15059812423762, w0=70.70000000000013, w1=14.849573625700332\n",
      "SubSGD iter. 133/499: loss=16.93757761047793, w0=71.40000000000013, w1=14.211279923151222\n",
      "SubSGD iter. 134/499: loss=12.405817507619902, w0=72.10000000000014, w1=14.20382694861681\n",
      "SubSGD iter. 135/499: loss=14.320435326358743, w0=72.80000000000014, w1=13.721200522362917\n",
      "SubSGD iter. 136/499: loss=12.382580425647722, w0=72.10000000000014, w1=13.693948027991302\n",
      "SubSGD iter. 137/499: loss=12.439443717452802, w0=72.80000000000014, w1=13.62020937838408\n",
      "SubSGD iter. 138/499: loss=14.60738473027526, w0=72.10000000000014, w1=12.998693418270847\n",
      "SubSGD iter. 139/499: loss=12.389960803503671, w0=71.40000000000013, w1=12.947010696574912\n",
      "SubSGD iter. 140/499: loss=16.449856393807796, w0=72.10000000000014, w1=12.208295634260587\n",
      "SubSGD iter. 141/499: loss=12.41868357912392, w0=71.40000000000013, w1=12.018801513791717\n",
      "SubSGD iter. 142/499: loss=13.010770300534997, w0=70.70000000000013, w1=12.204139040201442\n",
      "SubSGD iter. 143/499: loss=17.706385805134815, w0=70.00000000000013, w1=12.893976302273284\n",
      "SubSGD iter. 144/499: loss=17.478082475569195, w0=70.70000000000013, w1=13.812358274745234\n",
      "SubSGD iter. 145/499: loss=13.13701791035076, w0=71.40000000000013, w1=13.582267250190757\n",
      "SubSGD iter. 146/499: loss=15.775555449010422, w0=70.70000000000013, w1=14.095506441651901\n",
      "SubSGD iter. 147/499: loss=16.806134968577172, w0=70.00000000000013, w1=14.63843746510327\n",
      "SubSGD iter. 148/499: loss=19.325547672721306, w0=70.70000000000013, w1=15.569241682026528\n",
      "SubSGD iter. 149/499: loss=17.260534377204937, w0=71.40000000000013, w1=16.303089358622376\n",
      "SubSGD iter. 150/499: loss=29.279807219060093, w0=70.70000000000013, w1=14.799951278694033\n",
      "SubSGD iter. 151/499: loss=14.884501847832045, w0=71.40000000000013, w1=15.384820301898941\n",
      "SubSGD iter. 152/499: loss=16.190711580224285, w0=70.70000000000013, w1=15.872343107219345\n",
      "SubSGD iter. 153/499: loss=13.608632256706246, w0=70.00000000000013, w1=16.07816515853499\n",
      "SubSGD iter. 154/499: loss=23.666278711326864, w0=70.70000000000013, w1=17.134377776243934\n",
      "SubSGD iter. 155/499: loss=24.2824768090572, w0=71.40000000000013, w1=16.17843229572835\n",
      "SubSGD iter. 156/499: loss=20.228490903460422, w0=70.70000000000013, w1=15.169976925917057\n",
      "SubSGD iter. 157/499: loss=22.3441574466605, w0=71.40000000000013, w1=16.193245704763854\n",
      "SubSGD iter. 158/499: loss=12.89610519471343, w0=72.10000000000014, w1=16.456526467964515\n",
      "SubSGD iter. 159/499: loss=12.494712198648575, w0=72.80000000000014, w1=16.366563585656372\n",
      "SubSGD iter. 160/499: loss=12.723753469152317, w0=73.50000000000014, w1=16.52959369385794\n",
      "SubSGD iter. 161/499: loss=18.566023356840066, w0=74.20000000000014, w1=15.723538347787859\n",
      "SubSGD iter. 162/499: loss=21.677896742625627, w0=73.50000000000014, w1=14.735371081669648\n",
      "SubSGD iter. 163/499: loss=18.250134013820883, w0=74.20000000000014, w1=15.448777971726832\n",
      "SubSGD iter. 164/499: loss=12.772024454733925, w0=73.50000000000014, w1=15.259283851257962\n",
      "SubSGD iter. 165/499: loss=14.263759030729243, w0=72.80000000000014, w1=15.675565227254939\n",
      "SubSGD iter. 166/499: loss=14.043811037402053, w0=73.50000000000014, w1=16.057286075714867\n",
      "SubSGD iter. 167/499: loss=15.36786200438337, w0=74.20000000000014, w1=15.465117947681627\n",
      "SubSGD iter. 168/499: loss=13.45035775462178, w0=73.50000000000014, w1=15.80684849576047\n",
      "SubSGD iter. 169/499: loss=20.69666782289855, w0=72.80000000000014, w1=14.848377774701403\n",
      "SubSGD iter. 170/499: loss=20.722652246994, w0=73.50000000000014, w1=13.828623903190682\n",
      "SubSGD iter. 171/499: loss=12.529451769517662, w0=74.20000000000014, w1=13.631247480659038\n",
      "SubSGD iter. 172/499: loss=17.67054299613048, w0=73.50000000000014, w1=14.415708133414778\n",
      "SubSGD iter. 173/499: loss=14.902138280647778, w0=72.80000000000014, w1=14.920439284071515\n",
      "SubSGD iter. 174/499: loss=14.786090933769103, w0=73.50000000000014, w1=14.374310990559287\n",
      "SubSGD iter. 175/499: loss=15.034770171785494, w0=74.20000000000014, w1=14.86284333035227\n",
      "SubSGD iter. 176/499: loss=12.376974259765449, w0=73.50000000000014, w1=14.885139068358354\n",
      "SubSGD iter. 177/499: loss=26.52604947793092, w0=72.80000000000014, w1=16.016381731639882\n",
      "SubSGD iter. 178/499: loss=26.831587997185427, w0=73.50000000000014, w1=14.738642083260235\n",
      "SubSGD iter. 179/499: loss=18.510454839542472, w0=74.20000000000014, w1=13.82574343093349\n",
      "SubSGD iter. 180/499: loss=17.01661039831056, w0=74.90000000000015, w1=14.465137921285075\n",
      "SubSGD iter. 181/499: loss=17.87324541963251, w0=75.60000000000015, w1=13.503757959026679\n",
      "SubSGD iter. 182/499: loss=14.997476242576639, w0=76.30000000000015, w1=13.915970076836379\n",
      "SubSGD iter. 183/499: loss=22.702096093937193, w0=77.00000000000016, w1=14.790503053123334\n",
      "SubSGD iter. 184/499: loss=14.360573960805873, w0=76.30000000000015, w1=14.463317170045924\n",
      "SubSGD iter. 185/499: loss=12.9342847339296, w0=75.60000000000015, w1=14.322796394737662\n",
      "SubSGD iter. 186/499: loss=22.21715510568656, w0=74.90000000000015, w1=13.266110721971756\n",
      "SubSGD iter. 187/499: loss=13.351950853872395, w0=74.20000000000014, w1=13.679474295577835\n",
      "SubSGD iter. 188/499: loss=29.75154213620249, w0=73.50000000000014, w1=11.954179916222856\n",
      "SubSGD iter. 189/499: loss=14.300110125586475, w0=74.20000000000014, w1=11.283496292951126\n",
      "SubSGD iter. 190/499: loss=12.470749960409062, w0=74.90000000000015, w1=11.312195126101974\n",
      "SubSGD iter. 191/499: loss=17.365295509525843, w0=74.20000000000014, w1=12.249773006415241\n",
      "SubSGD iter. 192/499: loss=17.58308325184424, w0=73.50000000000014, w1=13.105854996150269\n",
      "SubSGD iter. 193/499: loss=12.442373445923751, w0=72.80000000000014, w1=12.986716012871536\n",
      "SubSGD iter. 194/499: loss=12.638560225764042, w0=72.10000000000014, w1=13.126301023938941\n",
      "SubSGD iter. 195/499: loss=20.588259064332856, w0=71.40000000000013, w1=11.850487502802292\n",
      "SubSGD iter. 196/499: loss=19.334332908087788, w0=70.70000000000013, w1=12.710658854709871\n",
      "SubSGD iter. 197/499: loss=13.780201807329705, w0=71.40000000000013, w1=13.245727880980345\n",
      "SubSGD iter. 198/499: loss=12.873570254458652, w0=70.70000000000013, w1=12.838075700834079\n",
      "SubSGD iter. 199/499: loss=13.93544092131807, w0=71.40000000000013, w1=12.439994790077145\n",
      "SubSGD iter. 200/499: loss=14.761567473854619, w0=70.70000000000013, w1=12.893969789302616\n",
      "SubSGD iter. 201/499: loss=12.617995695976214, w0=70.00000000000013, w1=12.914040630617732\n",
      "SubSGD iter. 202/499: loss=12.376717804373273, w0=70.70000000000013, w1=13.055246495175618\n",
      "SubSGD iter. 203/499: loss=12.679831470177767, w0=71.40000000000013, w1=13.352889567902153\n",
      "SubSGD iter. 204/499: loss=31.711151521580234, w0=72.10000000000014, w1=14.896795041666541\n",
      "SubSGD iter. 205/499: loss=18.76835933976385, w0=72.80000000000014, w1=14.051003218448292\n",
      "SubSGD iter. 206/499: loss=14.012181880868837, w0=72.10000000000014, w1=13.529931914652053\n",
      "SubSGD iter. 207/499: loss=13.234892323147333, w0=71.40000000000013, w1=13.77630691067036\n",
      "SubSGD iter. 208/499: loss=16.197789451106573, w0=72.10000000000014, w1=13.1107088436181\n",
      "SubSGD iter. 209/499: loss=17.1710577978549, w0=71.40000000000013, w1=12.132556472314604\n",
      "SubSGD iter. 210/499: loss=13.035566830427715, w0=72.10000000000014, w1=11.85264269055443\n",
      "SubSGD iter. 211/499: loss=12.458827838354079, w0=72.80000000000014, w1=11.994007820380691\n",
      "SubSGD iter. 212/499: loss=21.972176176819506, w0=72.10000000000014, w1=13.076257213717746\n",
      "SubSGD iter. 213/499: loss=14.386225750355889, w0=71.40000000000013, w1=12.415338046070202\n",
      "SubSGD iter. 214/499: loss=14.00554346746101, w0=72.10000000000014, w1=11.94883314720534\n",
      "SubSGD iter. 215/499: loss=12.377085185803034, w0=72.80000000000014, w1=11.98380946969087\n",
      "SubSGD iter. 216/499: loss=20.308839462876197, w0=73.50000000000014, w1=10.698637732500703\n",
      "SubSGD iter. 217/499: loss=16.962993277622044, w0=72.80000000000014, w1=11.569746586878068\n",
      "SubSGD iter. 218/499: loss=14.358657843754091, w0=73.50000000000014, w1=10.912076517382236\n",
      "SubSGD iter. 219/499: loss=14.977623093360089, w0=72.80000000000014, w1=11.572220406539182\n",
      "SubSGD iter. 220/499: loss=13.38483741658945, w0=73.50000000000014, w1=11.099675804117405\n",
      "SubSGD iter. 221/499: loss=14.551531822918928, w0=74.20000000000014, w1=11.65882532402918\n",
      "SubSGD iter. 222/499: loss=13.736628927599906, w0=74.90000000000015, w1=12.036723588376873\n",
      "SubSGD iter. 223/499: loss=12.376896595423187, w0=74.20000000000014, w1=12.103973429258094\n",
      "SubSGD iter. 224/499: loss=22.355423361952806, w0=74.90000000000015, w1=13.17628364696419\n",
      "SubSGD iter. 225/499: loss=12.927191374240461, w0=74.20000000000014, w1=12.946968649010667\n",
      "SubSGD iter. 226/499: loss=12.738886472816187, w0=74.90000000000015, w1=13.082844692931234\n",
      "SubSGD iter. 227/499: loss=14.09831409629581, w0=75.60000000000015, w1=13.437110920790087\n",
      "SubSGD iter. 228/499: loss=20.21186535824956, w0=74.90000000000015, w1=12.44526702199929\n",
      "SubSGD iter. 229/499: loss=14.151668942664255, w0=74.20000000000014, w1=13.007225237278083\n",
      "SubSGD iter. 230/499: loss=12.771215095478636, w0=73.50000000000014, w1=13.262966021789245\n",
      "SubSGD iter. 231/499: loss=16.46947092282494, w0=72.80000000000014, w1=12.452026118347339\n",
      "SubSGD iter. 232/499: loss=27.860203833771596, w0=72.10000000000014, w1=13.799780487513194\n",
      "SubSGD iter. 233/499: loss=13.954628257963405, w0=71.40000000000013, w1=14.152274320727745\n",
      "SubSGD iter. 234/499: loss=13.050964802908181, w0=72.10000000000014, w1=13.91043853502364\n",
      "SubSGD iter. 235/499: loss=14.86548797409989, w0=71.40000000000013, w1=14.362817633087818\n",
      "SubSGD iter. 236/499: loss=13.307339740324288, w0=72.10000000000014, w1=14.73861088203022\n",
      "SubSGD iter. 237/499: loss=13.202785597821377, w0=71.40000000000013, w1=14.959733232747892\n",
      "SubSGD iter. 238/499: loss=14.104059050807429, w0=70.70000000000013, w1=15.272336380311314\n",
      "SubSGD iter. 239/499: loss=12.710562011317842, w0=70.00000000000013, w1=14.931511106471039\n",
      "SubSGD iter. 240/499: loss=20.83243120238563, w0=70.70000000000013, w1=14.061454686712391\n",
      "SubSGD iter. 241/499: loss=22.175420105068984, w0=71.40000000000013, w1=12.997800420054622\n",
      "SubSGD iter. 242/499: loss=22.154182757689387, w0=72.10000000000014, w1=11.784499806307862\n",
      "SubSGD iter. 243/499: loss=13.303508996923288, w0=71.40000000000013, w1=12.080260001354338\n",
      "SubSGD iter. 244/499: loss=12.37726406242524, w0=72.10000000000014, w1=12.156356989961044\n",
      "SubSGD iter. 245/499: loss=16.50422828061232, w0=71.40000000000013, w1=11.160076882442581\n",
      "SubSGD iter. 246/499: loss=14.15666402914194, w0=70.70000000000013, w1=11.580474024278264\n",
      "SubSGD iter. 247/499: loss=12.588151042724457, w0=70.00000000000013, w1=11.589112575499827\n",
      "SubSGD iter. 248/499: loss=13.283260200538454, w0=70.70000000000013, w1=12.121842455414045\n",
      "SubSGD iter. 249/499: loss=12.534437301585555, w0=71.40000000000013, w1=12.381825530906351\n",
      "SubSGD iter. 250/499: loss=15.924623031815246, w0=72.10000000000014, w1=11.661637076848141\n",
      "SubSGD iter. 251/499: loss=12.532136132197461, w0=72.80000000000014, w1=11.511524936713188\n",
      "SubSGD iter. 252/499: loss=17.063730100400413, w0=72.10000000000014, w1=10.432096475118676\n",
      "SubSGD iter. 253/499: loss=12.884853239430246, w0=71.40000000000013, w1=10.651879393203629\n",
      "SubSGD iter. 254/499: loss=12.376983798485222, w0=72.10000000000014, w1=10.712293251559878\n",
      "SubSGD iter. 255/499: loss=12.569045388092318, w0=71.40000000000013, w1=10.802348996836324\n",
      "SubSGD iter. 256/499: loss=14.282601438516117, w0=70.70000000000013, w1=11.254415997383319\n",
      "SubSGD iter. 257/499: loss=16.47440333580772, w0=70.00000000000013, w1=11.891542262798566\n",
      "SubSGD iter. 258/499: loss=22.958388072780593, w0=69.30000000000013, w1=10.047661980364735\n",
      "SubSGD iter. 259/499: loss=12.985092208135878, w0=70.00000000000013, w1=9.876364968531579\n",
      "SubSGD iter. 260/499: loss=12.566414593744032, w0=70.70000000000013, w1=9.828782622259883\n",
      "SubSGD iter. 261/499: loss=15.12472915472535, w0=70.00000000000013, w1=10.395077431343704\n",
      "SubSGD iter. 262/499: loss=20.039253523476795, w0=69.30000000000013, w1=11.328689371983637\n",
      "SubSGD iter. 263/499: loss=18.10415626758752, w0=70.00000000000013, w1=10.438681510779249\n",
      "SubSGD iter. 264/499: loss=18.94970920954307, w0=70.70000000000013, w1=9.300868666476417\n",
      "SubSGD iter. 265/499: loss=13.432984827154398, w0=71.40000000000013, w1=9.938185456478196\n",
      "SubSGD iter. 266/499: loss=21.547729905624788, w0=70.70000000000013, w1=11.094345909752755\n",
      "SubSGD iter. 267/499: loss=12.442882903538406, w0=70.00000000000013, w1=10.76819305425583\n",
      "SubSGD iter. 268/499: loss=16.446629774208123, w0=70.70000000000013, w1=11.750370834524256\n",
      "SubSGD iter. 269/499: loss=17.31878921753938, w0=71.40000000000013, w1=10.879310661618762\n",
      "SubSGD iter. 270/499: loss=12.452881279832965, w0=72.10000000000014, w1=11.07203564644285\n",
      "SubSGD iter. 271/499: loss=16.239363842166366, w0=72.80000000000014, w1=10.160112064886714\n",
      "SubSGD iter. 272/499: loss=16.016011787655522, w0=73.50000000000014, w1=10.984960438456902\n",
      "SubSGD iter. 273/499: loss=13.116201809758982, w0=72.80000000000014, w1=11.341673350414887\n",
      "SubSGD iter. 274/499: loss=14.468684295941278, w0=73.50000000000014, w1=11.9183733908919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 275/499: loss=12.732762565140591, w0=74.20000000000014, w1=12.103637855295187\n",
      "SubSGD iter. 276/499: loss=17.17138047383634, w0=73.50000000000014, w1=12.937745454882595\n",
      "SubSGD iter. 277/499: loss=16.84010451467599, w0=72.80000000000014, w1=12.065624671002258\n",
      "SubSGD iter. 278/499: loss=21.229812435077967, w0=72.10000000000014, w1=13.099646353135636\n",
      "SubSGD iter. 279/499: loss=14.800869874757444, w0=72.80000000000014, w1=13.676613733742487\n",
      "SubSGD iter. 280/499: loss=20.533002214594383, w0=73.50000000000014, w1=12.567303190223281\n",
      "SubSGD iter. 281/499: loss=16.098729549314314, w0=74.20000000000014, w1=13.223160975246916\n",
      "SubSGD iter. 282/499: loss=13.462925191316675, w0=73.50000000000014, w1=13.621872598144899\n",
      "SubSGD iter. 283/499: loss=14.938829274489436, w0=74.20000000000014, w1=14.125880215243438\n",
      "SubSGD iter. 284/499: loss=16.997102065144727, w0=74.90000000000015, w1=14.751407862953034\n",
      "SubSGD iter. 285/499: loss=20.771464985626693, w0=74.20000000000014, w1=13.785501728517287\n",
      "SubSGD iter. 286/499: loss=21.770799210186343, w0=73.50000000000014, w1=12.626261290163336\n",
      "SubSGD iter. 287/499: loss=15.319328742195346, w0=74.20000000000014, w1=11.859549334304456\n",
      "SubSGD iter. 288/499: loss=15.261477177331741, w0=74.90000000000015, w1=12.430249300800439\n",
      "SubSGD iter. 289/499: loss=22.973457685450263, w0=75.60000000000015, w1=13.47847758637041\n",
      "SubSGD iter. 290/499: loss=13.99241794897245, w0=74.90000000000015, w1=14.014337389294488\n",
      "SubSGD iter. 291/499: loss=20.651140840440068, w0=74.20000000000014, w1=12.997793857047808\n",
      "SubSGD iter. 292/499: loss=21.962412819283696, w0=74.90000000000015, w1=13.985431659717237\n",
      "SubSGD iter. 293/499: loss=13.0167682425208, w0=75.60000000000015, w1=14.14650335805278\n",
      "SubSGD iter. 294/499: loss=12.667220652116837, w0=76.30000000000015, w1=13.79061518783357\n",
      "SubSGD iter. 295/499: loss=12.607675764627357, w0=75.60000000000015, w1=13.740441700194843\n",
      "SubSGD iter. 296/499: loss=14.068284982720922, w0=74.90000000000015, w1=13.351346768660733\n",
      "SubSGD iter. 297/499: loss=21.786246765847938, w0=74.20000000000014, w1=14.423961359836376\n",
      "SubSGD iter. 298/499: loss=14.384936676444216, w0=74.90000000000015, w1=14.812377071452257\n",
      "SubSGD iter. 299/499: loss=14.104385336134705, w0=75.60000000000015, w1=15.127786913519813\n",
      "SubSGD iter. 300/499: loss=13.325163984141161, w0=74.90000000000015, w1=15.519409691696213\n",
      "SubSGD iter. 301/499: loss=12.687696794173366, w0=74.20000000000014, w1=15.386208137478873\n",
      "SubSGD iter. 302/499: loss=13.463441030544049, w0=74.90000000000015, w1=15.639074199798038\n",
      "SubSGD iter. 303/499: loss=16.44832514651699, w0=74.20000000000014, w1=15.034258696094954\n",
      "SubSGD iter. 304/499: loss=12.949268233582973, w0=74.90000000000015, w1=15.202142727110585\n",
      "SubSGD iter. 305/499: loss=17.921493518780068, w0=74.20000000000014, w1=15.957303324685565\n",
      "SubSGD iter. 306/499: loss=16.72416762699756, w0=73.50000000000014, w1=15.310567838433549\n",
      "SubSGD iter. 307/499: loss=19.315176029101334, w0=74.20000000000014, w1=16.059793298008895\n",
      "SubSGD iter. 308/499: loss=14.445792256616722, w0=74.90000000000015, w1=16.416468506334574\n",
      "SubSGD iter. 309/499: loss=16.73385072732306, w0=75.60000000000015, w1=15.663918218906758\n",
      "SubSGD iter. 310/499: loss=26.917103405285307, w0=76.30000000000015, w1=16.661131050038332\n",
      "SubSGD iter. 311/499: loss=22.757730275663732, w0=75.60000000000015, w1=15.776342615830165\n",
      "SubSGD iter. 312/499: loss=24.965416442774732, w0=76.30000000000015, w1=16.685341685330922\n",
      "SubSGD iter. 313/499: loss=20.80623431483416, w0=77.00000000000016, w1=15.621264100088718\n",
      "SubSGD iter. 314/499: loss=16.517195211558242, w0=76.30000000000015, w1=15.111470511716277\n",
      "SubSGD iter. 315/499: loss=37.266080682655215, w0=75.60000000000015, w1=13.267590229282446\n",
      "SubSGD iter. 316/499: loss=21.08655886344174, w0=74.90000000000015, w1=12.193981296152177\n",
      "SubSGD iter. 317/499: loss=17.074183108540375, w0=74.20000000000014, w1=13.050508208038574\n",
      "SubSGD iter. 318/499: loss=20.939143586247773, w0=73.50000000000014, w1=14.065985452875697\n",
      "SubSGD iter. 319/499: loss=13.924335523317408, w0=72.80000000000014, w1=14.474357959334808\n",
      "SubSGD iter. 320/499: loss=12.756580725731954, w0=73.50000000000014, w1=14.667291803867494\n",
      "SubSGD iter. 321/499: loss=12.475395026002934, w0=74.20000000000014, w1=14.508263397072978\n",
      "SubSGD iter. 322/499: loss=15.22078518641274, w0=73.50000000000014, w1=15.070221612351771\n",
      "SubSGD iter. 323/499: loss=25.03979741709038, w0=72.80000000000014, w1=13.78850568648475\n",
      "SubSGD iter. 324/499: loss=22.54095675402219, w0=72.10000000000014, w1=12.488326825142215\n",
      "SubSGD iter. 325/499: loss=12.410092481399955, w0=72.80000000000014, w1=12.582984981860616\n",
      "SubSGD iter. 326/499: loss=27.84992306151914, w0=72.10000000000014, w1=13.918691848120394\n",
      "SubSGD iter. 327/499: loss=13.338580289233727, w0=71.40000000000013, w1=13.465195709395871\n",
      "SubSGD iter. 328/499: loss=17.57581570397477, w0=72.10000000000014, w1=12.656752082600269\n",
      "SubSGD iter. 329/499: loss=20.68387964566187, w0=71.40000000000013, w1=13.584228105734521\n",
      "SubSGD iter. 330/499: loss=12.941877010306381, w0=72.10000000000014, w1=13.357905519922214\n",
      "SubSGD iter. 331/499: loss=22.853044094561035, w0=72.80000000000014, w1=14.463698557816867\n",
      "SubSGD iter. 332/499: loss=13.639460406287041, w0=72.10000000000014, w1=14.013737403471778\n",
      "SubSGD iter. 333/499: loss=13.162628343347862, w0=71.40000000000013, w1=13.598421233844887\n",
      "SubSGD iter. 334/499: loss=13.12551649209716, w0=72.10000000000014, w1=13.329665332866382\n",
      "SubSGD iter. 335/499: loss=19.11351030150566, w0=71.40000000000013, w1=14.124259452156416\n",
      "SubSGD iter. 336/499: loss=17.633406132170396, w0=70.70000000000013, w1=13.147013180192067\n",
      "SubSGD iter. 337/499: loss=19.44553571124762, w0=71.40000000000013, w1=12.202697425143302\n",
      "SubSGD iter. 338/499: loss=13.33774239027379, w0=72.10000000000014, w1=11.852625314744397\n",
      "SubSGD iter. 339/499: loss=15.738252306978076, w0=72.80000000000014, w1=11.070588476795082\n",
      "SubSGD iter. 340/499: loss=14.580904190581792, w0=72.10000000000014, w1=10.28955892181232\n",
      "SubSGD iter. 341/499: loss=15.001428348904225, w0=72.80000000000014, w1=11.032129992041606\n",
      "SubSGD iter. 342/499: loss=13.827971340598673, w0=72.10000000000014, w1=11.48775400774655\n",
      "SubSGD iter. 343/499: loss=12.448672910185543, w0=72.80000000000014, w1=11.391434527705036\n",
      "SubSGD iter. 344/499: loss=21.520889057293846, w0=72.10000000000014, w1=12.494692238707184\n",
      "SubSGD iter. 345/499: loss=13.136073144245834, w0=72.80000000000014, w1=12.852102833600515\n",
      "SubSGD iter. 346/499: loss=15.095235567614305, w0=72.10000000000014, w1=13.398117859493372\n",
      "SubSGD iter. 347/499: loss=14.808814189769496, w0=71.40000000000013, w1=12.698580701252077\n",
      "SubSGD iter. 348/499: loss=13.508527386149476, w0=70.70000000000013, w1=12.974227776897838\n",
      "SubSGD iter. 349/499: loss=12.967844601591873, w0=71.40000000000013, w1=12.76927003908305\n",
      "SubSGD iter. 350/499: loss=12.61397954657963, w0=70.70000000000013, w1=12.435434499893944\n",
      "SubSGD iter. 351/499: loss=14.298894465101638, w0=71.40000000000013, w1=13.05268619952467\n",
      "SubSGD iter. 352/499: loss=12.827612717665971, w0=72.10000000000014, w1=12.848287235111032\n",
      "SubSGD iter. 353/499: loss=15.409337175303065, w0=71.40000000000013, w1=12.038640012220766\n",
      "SubSGD iter. 354/499: loss=13.275991885070235, w0=72.10000000000014, w1=12.476198125007762\n",
      "SubSGD iter. 355/499: loss=12.410940096179042, w0=71.40000000000013, w1=12.451254777184301\n",
      "SubSGD iter. 356/499: loss=17.324896065975654, w0=72.10000000000014, w1=11.590840605432852\n",
      "SubSGD iter. 357/499: loss=17.511625611272542, w0=72.80000000000014, w1=12.491480454022026\n",
      "SubSGD iter. 358/499: loss=14.458877206142544, w0=72.10000000000014, w1=11.828946787945677\n",
      "SubSGD iter. 359/499: loss=12.932752958471035, w0=72.80000000000014, w1=11.526766163035012\n",
      "SubSGD iter. 360/499: loss=14.212283744152911, w0=73.50000000000014, w1=12.060384165865356\n",
      "SubSGD iter. 361/499: loss=16.512125635457192, w0=74.20000000000014, w1=12.777859680339892\n",
      "SubSGD iter. 362/499: loss=14.282717819281185, w0=74.90000000000015, w1=13.200481868197734\n",
      "SubSGD iter. 363/499: loss=20.086637134226716, w0=74.20000000000014, w1=12.153508846956857\n",
      "SubSGD iter. 364/499: loss=22.83247726348473, w0=74.90000000000015, w1=13.248546340630146\n",
      "SubSGD iter. 365/499: loss=12.706965782753594, w0=75.60000000000015, w1=12.89206574071181\n",
      "SubSGD iter. 366/499: loss=15.423737167075958, w0=74.90000000000015, w1=13.606584345572562\n",
      "SubSGD iter. 367/499: loss=13.250310114808299, w0=75.60000000000015, w1=13.819502355528371\n",
      "SubSGD iter. 368/499: loss=16.88966846043749, w0=76.30000000000015, w1=14.384560164111955\n",
      "SubSGD iter. 369/499: loss=14.985643590899418, w0=77.00000000000016, w1=13.606180388189031\n",
      "SubSGD iter. 370/499: loss=26.323792504565517, w0=76.30000000000015, w1=12.289733595344357\n",
      "SubSGD iter. 371/499: loss=14.001835089513918, w0=75.60000000000015, w1=12.910713546723205\n",
      "SubSGD iter. 372/499: loss=15.346068882071876, w0=76.30000000000015, w1=12.029654998342334\n",
      "SubSGD iter. 373/499: loss=12.451874613279061, w0=75.60000000000015, w1=12.281056229746927\n",
      "SubSGD iter. 374/499: loss=17.39994920180706, w0=76.30000000000015, w1=12.951450647700401\n",
      "SubSGD iter. 375/499: loss=16.02882270592852, w0=75.60000000000015, w1=12.334760813576937\n",
      "SubSGD iter. 376/499: loss=12.449004880631529, w0=74.90000000000015, w1=12.53974245923129\n",
      "SubSGD iter. 377/499: loss=12.441600441224042, w0=74.20000000000014, w1=12.697772661050546\n",
      "SubSGD iter. 378/499: loss=12.419184771603804, w0=73.50000000000014, w1=12.633385251195769\n",
      "SubSGD iter. 379/499: loss=18.679304424272324, w0=74.20000000000014, w1=13.48180094937845\n",
      "SubSGD iter. 380/499: loss=15.32477079107904, w0=73.50000000000014, w1=14.092766716799492\n",
      "SubSGD iter. 381/499: loss=12.493809553564587, w0=72.80000000000014, w1=14.196025761037612\n",
      "SubSGD iter. 382/499: loss=12.574377623272639, w0=72.10000000000014, w1=14.300445138933386\n",
      "SubSGD iter. 383/499: loss=12.418598391944524, w0=72.80000000000014, w1=14.390537540638094\n",
      "SubSGD iter. 384/499: loss=12.558536170551433, w0=72.10000000000014, w1=14.188001571661436\n",
      "SubSGD iter. 385/499: loss=20.58492518459893, w0=72.80000000000014, w1=13.162746523880624\n",
      "SubSGD iter. 386/499: loss=22.806424139074068, w0=72.10000000000014, w1=14.205713900971826\n",
      "SubSGD iter. 387/499: loss=19.548710194859595, w0=71.40000000000013, w1=13.123280227508303\n",
      "SubSGD iter. 388/499: loss=17.468108828809704, w0=70.70000000000013, w1=13.786381918839373\n",
      "SubSGD iter. 389/499: loss=12.806137107808663, w0=71.40000000000013, w1=13.635588244425728\n",
      "SubSGD iter. 390/499: loss=12.425874352371434, w0=72.10000000000014, w1=13.771289704723925\n",
      "SubSGD iter. 391/499: loss=17.259453711070456, w0=72.80000000000014, w1=12.96986888023599\n",
      "SubSGD iter. 392/499: loss=12.60476696403795, w0=73.50000000000014, w1=12.763004977812006\n",
      "SubSGD iter. 393/499: loss=14.784376372446923, w0=74.20000000000014, w1=13.281445454799174\n",
      "SubSGD iter. 394/499: loss=12.525348975944322, w0=74.90000000000015, w1=13.03960966909507\n",
      "SubSGD iter. 395/499: loss=12.724419476440978, w0=75.60000000000015, w1=13.133084265892752\n",
      "SubSGD iter. 396/499: loss=12.442179637868751, w0=76.30000000000015, w1=13.063837833463742\n",
      "SubSGD iter. 397/499: loss=14.64983151668171, w0=77.00000000000016, w1=12.239137928900476\n",
      "SubSGD iter. 398/499: loss=15.141238792521955, w0=76.30000000000015, w1=13.031057982164892\n",
      "SubSGD iter. 399/499: loss=12.460679461776168, w0=75.60000000000015, w1=13.050114754597512\n",
      "SubSGD iter. 400/499: loss=29.367827538004374, w0=74.90000000000015, w1=11.34240826514521\n",
      "SubSGD iter. 401/499: loss=14.665643432955497, w0=75.60000000000015, w1=11.82618209739137\n",
      "SubSGD iter. 402/499: loss=15.622062525644816, w0=74.90000000000015, w1=12.617175822065262\n",
      "SubSGD iter. 403/499: loss=13.86774198703657, w0=74.20000000000014, w1=12.181951681620697\n",
      "SubSGD iter. 404/499: loss=15.702635135151384, w0=73.50000000000014, w1=12.88823473566319\n",
      "SubSGD iter. 405/499: loss=17.239503879359162, w0=72.80000000000014, w1=13.648282579818076\n",
      "SubSGD iter. 406/499: loss=13.254500528991564, w0=73.50000000000014, w1=13.9650287948459\n",
      "SubSGD iter. 407/499: loss=17.406257643923052, w0=74.20000000000014, w1=13.083970246465029\n",
      "SubSGD iter. 408/499: loss=12.495167317418797, w0=74.90000000000015, w1=12.854822631016136\n",
      "SubSGD iter. 409/499: loss=12.438791369186731, w0=75.60000000000015, w1=12.819945870641824\n",
      "SubSGD iter. 410/499: loss=12.981027110058978, w0=74.90000000000015, w1=12.609338187385253\n",
      "SubSGD iter. 411/499: loss=13.460805923712707, w0=74.20000000000014, w1=12.246919604762809\n",
      "SubSGD iter. 412/499: loss=15.56689468554293, w0=74.90000000000015, w1=11.375859431857315\n",
      "SubSGD iter. 413/499: loss=12.991143388309691, w0=75.60000000000015, w1=11.565897537175987\n",
      "SubSGD iter. 414/499: loss=23.4100386827655, w0=76.30000000000015, w1=12.669301344030215\n",
      "SubSGD iter. 415/499: loss=15.586865268616931, w0=75.60000000000015, w1=13.444688029582322\n",
      "SubSGD iter. 416/499: loss=13.796579850679269, w0=74.90000000000015, w1=13.955721783471443\n",
      "SubSGD iter. 417/499: loss=25.62237496559078, w0=74.20000000000014, w1=12.595812587967593\n",
      "SubSGD iter. 418/499: loss=18.300525646118064, w0=74.90000000000015, w1=11.47020036279978\n",
      "SubSGD iter. 419/499: loss=12.49811282513038, w0=75.60000000000015, w1=11.160471878579829\n",
      "SubSGD iter. 420/499: loss=14.389182683048938, w0=74.90000000000015, w1=10.617944924216614\n",
      "SubSGD iter. 421/499: loss=13.181545546047296, w0=74.20000000000014, w1=11.087805081782061\n",
      "SubSGD iter. 422/499: loss=14.442240646207921, w0=73.50000000000014, w1=11.712330317707949\n",
      "SubSGD iter. 423/499: loss=13.053385271858474, w0=72.80000000000014, w1=12.033214907667633\n",
      "SubSGD iter. 424/499: loss=12.391656014071714, w0=73.50000000000014, w1=12.067134130599603\n",
      "SubSGD iter. 425/499: loss=20.614463291253635, w0=72.80000000000014, w1=13.099270811250832\n",
      "SubSGD iter. 426/499: loss=28.424407200069655, w0=73.50000000000014, w1=11.336338240747812\n",
      "SubSGD iter. 427/499: loss=32.81549615697265, w0=72.80000000000014, w1=13.066895381720617\n",
      "SubSGD iter. 428/499: loss=19.86420437667364, w0=73.50000000000014, w1=11.948445289175893\n",
      "SubSGD iter. 429/499: loss=12.884557526032001, w0=72.80000000000014, w1=11.621259406098483\n",
      "SubSGD iter. 430/499: loss=12.441210476794454, w0=72.10000000000014, w1=11.666911936164777\n",
      "SubSGD iter. 431/499: loss=13.880847577923822, w0=72.80000000000014, w1=12.184582438794692\n",
      "SubSGD iter. 432/499: loss=15.872837843683353, w0=73.50000000000014, w1=11.364955897014157\n",
      "SubSGD iter. 433/499: loss=12.51753660080933, w0=74.20000000000014, w1=11.130341459322482\n",
      "SubSGD iter. 434/499: loss=14.345209937844464, w0=73.50000000000014, w1=11.7397241849174\n",
      "SubSGD iter. 435/499: loss=12.406447541542422, w0=72.80000000000014, w1=11.641671631758808\n",
      "SubSGD iter. 436/499: loss=12.406348039871583, w0=72.10000000000014, w1=11.651444830226351\n",
      "SubSGD iter. 437/499: loss=18.970546182514628, w0=71.40000000000013, w1=12.536228305995909\n",
      "SubSGD iter. 438/499: loss=14.97410087310938, w0=70.70000000000013, w1=13.010039216159633\n",
      "SubSGD iter. 439/499: loss=16.06427774220408, w0=71.40000000000013, w1=12.356309112707377\n",
      "SubSGD iter. 440/499: loss=12.732617614257505, w0=72.10000000000014, w1=12.171000234619612\n",
      "SubSGD iter. 441/499: loss=25.08384837555506, w0=71.40000000000013, w1=13.372679373212298\n",
      "SubSGD iter. 442/499: loss=17.17929380037729, w0=72.10000000000014, w1=14.175902199181893\n",
      "SubSGD iter. 443/499: loss=16.94836867939597, w0=72.80000000000014, w1=13.426435664957252\n",
      "SubSGD iter. 444/499: loss=12.377485139249684, w0=72.10000000000014, w1=13.35591350717172\n",
      "SubSGD iter. 445/499: loss=15.851711230114084, w0=72.80000000000014, w1=14.022411532811908\n",
      "SubSGD iter. 446/499: loss=12.376426803766156, w0=72.10000000000014, w1=13.965156818531902\n",
      "SubSGD iter. 447/499: loss=20.004188141019558, w0=71.40000000000013, w1=12.82590244736683\n",
      "SubSGD iter. 448/499: loss=14.476727054015965, w0=72.10000000000014, w1=13.412421068119848\n",
      "SubSGD iter. 449/499: loss=20.481279714147163, w0=72.80000000000014, w1=12.324594247973533\n",
      "SubSGD iter. 450/499: loss=27.955052686526948, w0=73.50000000000014, w1=13.732295510054463\n",
      "SubSGD iter. 451/499: loss=21.74510731902567, w0=72.80000000000014, w1=14.714837683792307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 452/499: loss=12.937961745578821, w0=73.50000000000014, w1=14.948630420885413\n",
      "SubSGD iter. 453/499: loss=13.13364449249795, w0=74.20000000000014, w1=15.186124858613637\n",
      "SubSGD iter. 454/499: loss=12.542558789743751, w0=73.50000000000014, w1=15.33509111570313\n",
      "SubSGD iter. 455/499: loss=20.7716261881476, w0=74.20000000000014, w1=16.159123315328205\n",
      "SubSGD iter. 456/499: loss=29.35269952331911, w0=73.50000000000014, w1=14.783341366911896\n",
      "SubSGD iter. 457/499: loss=17.09540333614375, w0=72.80000000000014, w1=15.445090333533265\n",
      "SubSGD iter. 458/499: loss=25.612311411383065, w0=73.50000000000014, w1=14.180430344510107\n",
      "SubSGD iter. 459/499: loss=12.74605518171439, w0=72.80000000000014, w1=13.945440492209956\n",
      "SubSGD iter. 460/499: loss=14.848207645214421, w0=73.50000000000014, w1=14.462181057636183\n",
      "SubSGD iter. 461/499: loss=14.195575342420534, w0=72.80000000000014, w1=14.892401866895037\n",
      "SubSGD iter. 462/499: loss=12.60130419763511, w0=72.10000000000014, w1=14.680792427260965\n",
      "SubSGD iter. 463/499: loss=16.991892689704834, w0=71.40000000000013, w1=13.837299556106672\n",
      "SubSGD iter. 464/499: loss=21.0002644423582, w0=72.10000000000014, w1=12.79342552169326\n",
      "SubSGD iter. 465/499: loss=17.03100152141105, w0=72.80000000000014, w1=13.583694288476\n",
      "SubSGD iter. 466/499: loss=23.123077912763534, w0=73.50000000000014, w1=14.656004506182096\n",
      "SubSGD iter. 467/499: loss=12.378949590616068, w0=74.20000000000014, w1=14.590597004016098\n",
      "SubSGD iter. 468/499: loss=12.713105981375094, w0=73.50000000000014, w1=14.803550572651961\n",
      "SubSGD iter. 469/499: loss=17.34914082214246, w0=74.20000000000014, w1=13.983417345384842\n",
      "SubSGD iter. 470/499: loss=12.820176573373196, w0=73.50000000000014, w1=14.234818576789435\n",
      "SubSGD iter. 471/499: loss=25.098423798170593, w0=72.80000000000014, w1=15.346509055738046\n",
      "SubSGD iter. 472/499: loss=14.77733721538363, w0=72.10000000000014, w1=14.78198299268723\n",
      "SubSGD iter. 473/499: loss=13.503137468477764, w0=72.80000000000014, w1=15.146129548601339\n",
      "SubSGD iter. 474/499: loss=22.936917750807012, w0=72.10000000000014, w1=16.07423760975362\n",
      "SubSGD iter. 475/499: loss=18.21911520281089, w0=72.80000000000014, w1=15.333203923198411\n",
      "SubSGD iter. 476/499: loss=12.81597833500957, w0=73.50000000000014, w1=15.530367558209406\n",
      "SubSGD iter. 477/499: loss=18.33078544894107, w0=72.80000000000014, w1=14.709063131726861\n",
      "SubSGD iter. 478/499: loss=14.943580811506413, w0=73.50000000000014, w1=15.209832951366582\n",
      "SubSGD iter. 479/499: loss=18.033396493025684, w0=72.80000000000014, w1=14.390857524284495\n",
      "SubSGD iter. 480/499: loss=12.443267067159182, w0=73.50000000000014, w1=14.283489340718793\n",
      "SubSGD iter. 481/499: loss=16.702240238605317, w0=74.20000000000014, w1=13.484595838614519\n",
      "SubSGD iter. 482/499: loss=13.008430669203138, w0=74.90000000000015, w1=13.08768342343358\n",
      "SubSGD iter. 483/499: loss=21.511221127772213, w0=74.20000000000014, w1=11.918572566826548\n",
      "SubSGD iter. 484/499: loss=16.50515858201513, w0=73.50000000000014, w1=11.044897534171074\n",
      "SubSGD iter. 485/499: loss=12.65630357721191, w0=72.80000000000014, w1=10.777070285950094\n",
      "SubSGD iter. 486/499: loss=12.687748325404502, w0=73.50000000000014, w1=11.007788307535579\n",
      "SubSGD iter. 487/499: loss=12.415994954152666, w0=74.20000000000014, w1=10.849587664254063\n",
      "SubSGD iter. 488/499: loss=12.457434257055757, w0=74.90000000000015, w1=10.86923800768133\n",
      "SubSGD iter. 489/499: loss=14.706940130558857, w0=75.60000000000015, w1=11.377755387805138\n",
      "SubSGD iter. 490/499: loss=15.472913741786769, w0=74.90000000000015, w1=12.181195542396695\n",
      "SubSGD iter. 491/499: loss=13.67424797973505, w0=74.20000000000014, w1=12.684938168427617\n",
      "SubSGD iter. 492/499: loss=14.307675687809493, w0=73.50000000000014, w1=13.221237499095412\n",
      "SubSGD iter. 493/499: loss=20.188284880179573, w0=74.20000000000014, w1=14.128163556414174\n",
      "SubSGD iter. 494/499: loss=28.863881097270017, w0=74.90000000000015, w1=15.365215679676146\n",
      "SubSGD iter. 495/499: loss=20.03006528168607, w0=74.20000000000014, w1=16.227844125466703\n",
      "SubSGD iter. 496/499: loss=12.376544059522733, w0=73.50000000000014, w1=16.23693896152052\n",
      "SubSGD iter. 497/499: loss=18.93351200232242, w0=74.20000000000014, w1=15.39114713830227\n",
      "SubSGD iter. 498/499: loss=22.519955032383177, w0=73.50000000000014, w1=14.327428557272107\n",
      "SubSGD iter. 499/499: loss=18.25404453347008, w0=72.80000000000014, w1=13.433776853560442\n",
      "SubSGD: execution time=1.044 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4dd65b9eba48029bdcd079ba87a006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widgâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses, subsgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
