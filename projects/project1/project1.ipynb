{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "1ed951f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, sys, types\n",
    "import costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "7d01e4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27b22fc",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "58647f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "9a36d703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30) (250000,) (250000, 30)\n"
     ]
    }
   ],
   "source": [
    "def standardize(x):\n",
    "    ''' fill your code in here...\n",
    "    '''\n",
    "    centered_data = x - np.mean(x, axis=0)\n",
    "    std_data = centered_data / np.std(centered_data, axis=0)\n",
    "    \n",
    "    return std_data\n",
    "\n",
    "std_data = standardize(tX)\n",
    "print(tX.shape,y.shape,std_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "3dad4f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.46141372  0.06833197  0.40768027 ...  1.5668      1.55858439\n",
      "   0.4125105 ]\n",
      " [ 0.51670419  0.55250482  0.54013641 ... -0.63936657 -0.63936694\n",
      "  -0.27381996]\n",
      " [-2.33785898  3.19515553  1.09655998 ... -0.63936657 -0.63936694\n",
      "  -0.29396985]\n",
      " ...\n",
      " [ 0.38016991  0.31931645 -0.13086367 ... -0.63936657 -0.63936694\n",
      "  -0.31701723]\n",
      " [ 0.35431502 -0.84532397 -0.30297338 ... -0.63936657 -0.63936694\n",
      "  -0.74543941]\n",
      " [-2.33785898  0.66533608 -0.25352276 ... -0.63936657 -0.63936694\n",
      "  -0.74543941]] \n",
      "\n",
      " [-2.63465694e-15  4.50019089e-15 -3.48448848e-15  7.19675786e-15\n",
      " -2.72244716e-14 -3.48856766e-15  1.33668259e-14  2.16429719e-14\n",
      "  6.39742126e-15  2.86409207e-15 -7.00447966e-15  4.45924897e-15\n",
      " -1.36393998e-14 -5.96492045e-15  1.35646161e-16  7.13136217e-17\n",
      "  2.58030370e-14 -1.06327391e-16 -1.87188487e-16  8.24369382e-15\n",
      "  1.41040513e-16 -9.00283004e-15 -6.01698247e-16 -5.68357095e-15\n",
      "  3.38428841e-15 -1.72635239e-15 -1.00691633e-14  2.10324860e-14\n",
      " -5.81535886e-15 -8.76751116e-16] \n",
      "\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(std_data, \"\\n\\n\", np.mean(std_data, axis=0), \"\\n\\n\", np.std(std_data, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "ce333791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    e = y - np.dot(tx,w)\n",
    "    N = y.shape[0]\n",
    "   \n",
    "    return 1/(2*N) * np.dot(e.T,e)\n",
    "    # TODO: compute loss by MSE\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "b0d021d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.arange(30)\n",
    "e = y - np.dot(std_data,w)\n",
    "loss = compute_loss(y, std_data, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "0cf7d481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32588.7364874172"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "22374eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    N = y.shape[0]\n",
    "    e = y - np.dot(tx,w)\n",
    "    return -1/N*np.dot(tx.T,e)\n",
    "    # TODO: compute gradient vector\n",
    "    # ***************************************************\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "bbb1a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "        \n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        \n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD \n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        # ***************************************************\n",
    "  \n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        w = w - gamma*grad\n",
    "        # ***************************************************\n",
    "   \n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}, grad={grad}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1], grad=grad))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "5654c149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/99: loss=16.358081204805362, w0=0.18772873440046625, w1=0.7854681789347494, grad=[ 1.50318515 -0.24467692  1.12427797  4.01195002  4.4656708   4.25771838\n",
      "  4.46717746 -1.43767457  2.22997938  4.83818145  1.04859214  2.85396246\n",
      "  4.46626909  1.93026288  0.77735748  0.89674193  2.43701176  0.50397191\n",
      " -0.11253434  2.39924188  0.13355828  4.49765922  5.05846268  4.41636671\n",
      "  4.25259423  4.25182551  4.49684596  4.4665237   4.4662274   4.72142401]\n",
      "GD iter. 1/99: loss=2.856081348392213, w0=0.1928121505122634, w1=0.6738388062910308, grad=[-0.05083416  1.11629373  1.07763443 -0.90214301 -1.28270277 -1.31684105\n",
      " -1.28099704  1.18499193 -0.15496953 -0.75469707  0.50203706 -0.55032329\n",
      " -1.28224257 -0.10962495  0.63488262  0.77850212  0.45013989  0.32378135\n",
      " -0.07528045 -0.51374014  0.07170394 -0.75201261 -0.98076177 -0.5729482\n",
      " -0.51574246 -0.5164484  -1.2833479  -1.28200013 -1.28223147 -0.96717654]\n",
      "GD iter. 2/99: loss=1.716187225050259, w0=0.16864320909058675, w1=0.6091541202356847, grad=[ 0.24168941  0.64684686  0.81595827  0.16910465  0.08586415  0.01445634\n",
      "  0.08733024  0.45454238  0.33160724  0.42994992  0.43709321  0.22280798\n",
      "  0.08629142  0.30794133  0.56091477  0.70641227  0.63561955  0.27339457\n",
      " -0.05249414  0.12114699  0.07360279  0.36912004  0.37656037  0.47792625\n",
      "  0.48469682  0.48407433  0.09091618  0.08649778  0.08628896  0.29404821]\n",
      "GD iter. 3/99: loss=1.3580221218164565, w0=0.15585307345740462, w1=0.5466855716497528, grad=[ 0.12790136  0.62468549  0.65770234 -0.10219609 -0.1748689  -0.23546166\n",
      " -0.17350358  0.50771045  0.17751053  0.09869503  0.30871154  0.04853842\n",
      " -0.17448451  0.17685542  0.48720254  0.63413555  0.3907453   0.20904955\n",
      " -0.03186272 -0.0393301   0.06234782  0.06296496  0.05748257  0.17519164\n",
      "  0.19253444  0.19197864 -0.17219487 -0.17430207 -0.1744838  -0.0118945 ]\n",
      "GD iter. 4/99: loss=1.1342637729990948, w0=0.14422990366527064, w1=0.49406165870103147, grad=[ 0.1162317   0.52623913  0.51944879 -0.0586186  -0.07812574 -0.13871806\n",
      " -0.07686209  0.40253539  0.17670982  0.12686101  0.22980191  0.08878238\n",
      " -0.07777131  0.17688415  0.42677399  0.57108596  0.29136431  0.16083008\n",
      " -0.0153526  -0.01323093  0.05525846  0.09337411  0.11922771  0.19189974\n",
      "  0.20566246  0.20516673 -0.07568272 -0.07761002 -0.07777001  0.04372076]\n",
      "GD iter. 5/99: loss=0.978938558386024, w0=0.1352832959872989, w1=0.4475911704656653, grad=[ 0.08946608  0.46470488  0.41123969 -0.07891202 -0.07149531 -0.12964114\n",
      " -0.07030934  0.34922113  0.1474141   0.08544178  0.16654602  0.08009537\n",
      " -0.07116558  0.15251223  0.37461286  0.514155    0.19590957  0.11980874\n",
      " -0.00181474 -0.02517756  0.04829605  0.05726091  0.09543721  0.14612784\n",
      "  0.15934585  0.15890264 -0.06953952 -0.07102154 -0.07116225  0.0218872 ]\n",
      "GD iter. 6/99: loss=0.8679156751658555, w0=0.1280245542747006, w1=0.40657430855325627, grad=[ 0.07258742  0.41016862  0.32377505 -0.07939479 -0.05131827 -0.10763886\n",
      " -0.05019687  0.29732221  0.12948866  0.06879803  0.12028247  0.08220873\n",
      " -0.05100813  0.13791472  0.33011751  0.46317817  0.12797969  0.08624204\n",
      "  0.00909018 -0.02565201  0.04227116  0.0436092   0.09186526  0.12243229\n",
      "  0.13434567  0.13394884 -0.04962463 -0.05087853 -0.05100254  0.02086944]\n",
      "GD iter. 7/99: loss=0.7866605317982975, w0=0.12216165799625417, w1=0.3699612088010131, grad=[ 0.05862896  0.366131    0.25359727 -0.08065123 -0.0394408  -0.09389951\n",
      " -0.03837244  0.25597122  0.11286472  0.05332589  0.08583706  0.08158802\n",
      " -0.03914654  0.12449662  0.29195791  0.41741691  0.07566333  0.0585886\n",
      "  0.01779732 -0.02687072  0.03690239  0.03042056  0.08451952  0.10004288\n",
      "  0.11082097  0.1104651  -0.03794037 -0.03902898 -0.0391384   0.01745022]\n",
      "GD iter. 8/99: loss=0.725802119649685, w0=0.11735071857091071, w1=0.3370352486028005, grad=[ 0.04810939  0.3292596   0.19707411 -0.07919351 -0.02965119 -0.08238186\n",
      " -0.02862709  0.22119395  0.09903767  0.04269816  0.06063724  0.08131525\n",
      " -0.0293699   0.11354315  0.25919879  0.37634607  0.03659684  0.03595328\n",
      "  0.02464537 -0.02675028  0.03216362  0.02130228  0.07880365  0.08240994\n",
      "  0.09205649  0.09173687 -0.02827433 -0.02926241 -0.02935907  0.01620344]\n",
      "GD iter. 9/99: loss=0.6791895462437714, w0=0.11334567472418693, w1=0.3072010413021494, grad=[ 0.04005044  0.29834207  0.15152128 -0.07666996 -0.02240152 -0.07350936\n",
      " -0.0214147   0.19232099  0.08706443  0.03480782  0.04238841  0.08070871\n",
      " -0.02213095  0.10424075  0.23100887  0.33946637  0.007408    0.01749459\n",
      "  0.02993122 -0.02623243  0.02797846  0.01439254  0.07344413  0.06775779\n",
      "  0.07634225  0.07605476 -0.02110423 -0.02203192 -0.02211738  0.01558696]\n",
      "GD iter. 10/99: loss=0.64272485380712, w0=0.10994783212275289, w1=0.27998861151923965, grad=[ 0.03397843  0.2721243   0.11475772 -0.07332564 -0.01686286 -0.06646196\n",
      " -0.01590779  0.16820966  0.07669354  0.02916637  0.02940216  0.07994251\n",
      " -0.01660126  0.09636113  0.20669626  0.3063389  -0.01415353  0.00252692\n",
      "  0.03390677 -0.0254188   0.02428936  0.00932478  0.06867502  0.05570304\n",
      "  0.06329619  0.06303723 -0.0156139  -0.01650934 -0.01658497  0.0155829 ]\n",
      "GD iter. 11/99: loss=0.6136343787472128, w0=0.10700372437466447, w1=0.2550186655030298, grad=[ 0.02944108  0.24969946  0.08505662 -0.06954722 -0.01267144 -0.06087124\n",
      " -0.01174372  0.14805293  0.06762169  0.02515287  0.02036846  0.07899825\n",
      " -0.01241744  0.0896283   0.18567486  0.27657014 -0.02990229 -0.00952963\n",
      "  0.03678758 -0.02449287  0.02104153  0.00558706  0.0643863   0.04570898\n",
      "  0.05238923  0.05215566 -0.01145019 -0.01233151 -0.01239848  0.01592984]\n",
      "GD iter. 12/99: loss=0.5900090567484877, w0=0.10439447719950332, w1=0.23198443286964007, grad=[ 0.02609247  0.23034233  0.06103837 -0.06555913 -0.0094908  -0.05639597\n",
      " -0.0085869   0.13115618  0.05963686  0.02235485  0.014289    0.0779019\n",
      " -0.00924333  0.08384327  0.1674503   0.24980896 -0.04121848 -0.01916054\n",
      "  0.03875753 -0.02354391  0.01818615  0.00284712  0.06054646  0.03740105\n",
      "  0.04324732  0.04303639 -0.00828393 -0.00916247 -0.0092218   0.01649707]\n",
      "GD iter. 13/99: loss=0.5705117622051346, w0=0.10202914571217651, w1=0.21063569606935892, grad=[ 0.02365331  0.21348737  0.04160163 -0.06152712 -0.00707919 -0.0527878\n",
      " -0.00619627  0.11695477  0.05256669  0.02044827  0.01040082  0.0766695\n",
      " -0.00683742  0.0788414   0.15160497  0.22574168 -0.04916884 -0.02677323\n",
      "  0.03997348 -0.02263055  0.0156792   0.00084313  0.05710833  0.03046801\n",
      "  0.03555761  0.03536688 -0.00587866 -0.00676084 -0.00681342  0.01717985]\n",
      "GD iter. 14/99: loss=0.5541891330076366, w0=0.09983884731216806, w1=0.19076688612502607, grad=[ 0.02190298  0.1986881   0.02586534 -0.05756064 -0.00525006 -0.04985301\n",
      " -0.0043858   0.10498261  0.04627573  0.01919213  0.00812313  0.07531897\n",
      " -0.0050133   0.07449092  0.13778607  0.20408803 -0.05457296 -0.03270934\n",
      "  0.04056906 -0.02178327  0.0134812  -0.00061862  0.05403152  0.02466315\n",
      "  0.02906981  0.02889714 -0.00405148 -0.00494039 -0.00498697  0.01790833]\n",
      "GD iter. 15/99: loss=0.540349242564276, w0=0.09777211488423646, w1=0.17220775990927564, grad=[ 0.02066732  0.18559126  0.01312367 -0.05373143 -0.00386255 -0.04744336\n",
      " -0.00301508  0.09485665  0.04065516  0.0184049   0.00701492  0.07386707\n",
      " -0.00363029  0.07068472  0.12569541  0.18459762 -0.05806074 -0.03725531\n",
      "  0.04065797 -0.02101577  0.01155682 -0.00168373  0.05127785  0.01978735\n",
      "  0.02358026  0.02342377 -0.00266407 -0.00356049 -0.00360175  0.01863449]\n",
      "GD iter. 16/99: loss=0.5284811140529064, w0=0.09579112567362391, w1=0.15481618458950194, grad=[ 0.01980989  0.17391575  0.00281026 -0.05008352 -0.00280969 -0.04544459\n",
      " -0.00197744  0.08626161  0.03561697  0.01795091  0.00674257  0.07232984\n",
      " -0.00258148  0.06733549  0.11508085  0.16704668 -0.06011533 -0.04065119\n",
      "  0.04033679 -0.02033074  0.00987449 -0.00246068  0.04881261  0.01568\n",
      "  0.01892325  0.01878127 -0.00161116 -0.00251434 -0.00255087  0.01932691]\n",
      "GD iter. 17/99: loss=0.5182010691298807, w0=0.09386867641009862, w1=0.13847251545583963, grad=[ 0.01922449  0.16343669 -0.00553019 -0.04664143 -0.00201036 -0.04376863\n",
      " -0.00119205  0.07893802  0.03108904  0.01772909  0.00705476  0.0707223\n",
      " -0.00178586  0.06437169  0.10572905  0.15123518 -0.06110668 -0.04309808\n",
      "  0.03968738 -0.01972435  0.00840604 -0.00303015  0.04660432  0.0122109\n",
      "  0.01496329  0.01483434 -0.00081275 -0.00172102 -0.00175333  0.01996592]\n",
      "GD iter. 18/99: loss=0.5092164624097397, w0=0.09198576128892827, w1=0.1230752074401892, grad=[ 0.01882915  0.15397308 -0.0122643  -0.04341604 -0.00140308 -0.04234742\n",
      " -0.00059763  0.07267218  0.02701162  0.01766452  0.00776322  0.06905843\n",
      " -0.00118202  0.0617344   0.09745944  0.13698429 -0.06131744 -0.04476435\n",
      "  0.03877898 -0.01918912  0.00712638 -0.0034519   0.0446246   0.00927408\n",
      "  0.01158918  0.01147195 -0.00020802 -0.00111915 -0.00114769  0.02054017]\n",
      "GD iter. 19/99: loss=0.5013008064098698, w0=0.09012964212836873, w1=0.10853737679653615, grad=[ 0.01856119  0.14537831 -0.01768788 -0.04040884 -0.00094127 -0.04112828\n",
      " -0.00014775  0.067288    0.02333457  0.01770205  0.00872787  0.06735105\n",
      " -0.0007234   0.05937484  0.09011907  0.12413412 -0.06096295 -0.04579091\n",
      "  0.03766999 -0.01871592  0.00601319 -0.00376994  0.04284801  0.00678294\n",
      "  0.00870929  0.00860262  0.00024925 -0.00066223 -0.0006874   0.02104421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 20/99: loss=0.4942764498532172, w0=0.0882923175250646, w1=0.09478409718369293, grad=[ 0.01837325  0.1375328  -0.02204008 -0.03761508 -0.00058961 -0.04007032\n",
      "  0.00019274  0.06264025  0.02001537  0.01780141  0.00984531  0.06561191\n",
      " -0.00037474  0.05725249  0.08357827  0.11254162 -0.06020673 -0.04629558\n",
      "  0.0364095  -0.0182952   0.00504663 -0.00401647  0.04125182  0.00466644\n",
      "  0.00624782  0.00615066  0.00059419 -0.00031506 -0.00033721  0.02147657]\n",
      "GD iter. 21/99: loss=0.48800233000914156, w0=0.08646931477038586, w1=0.08175026412523166, grad=[ 1.82300275e-02  1.30338331e-01 -2.55146210e-02 -3.50260262e-02\n",
      " -3.21361866e-04 -3.91417105e-02  4.50479170e-04  5.86090329e-02\n",
      "  1.70175181e-02  1.79335220e-02  1.10400955e-02  6.38515919e-02\n",
      " -1.09337673e-04  5.53334826e-02  7.77270594e-02  1.02078820e-01\n",
      " -5.91723922e-02 -4.63767853e-02  3.50386383e-02 -1.79177418e-02\n",
      "  4.20908200e-03 -4.21488279e-03  3.98157884e-02  2.86601416e-03\n",
      "  4.14177154e-03  4.05321436e-03  8.53530039e-04 -5.09466755e-05\n",
      " -7.03837610e-05  2.18384698e-02]\n",
      "GD iter. 22/99: loss=0.4823651803051713, w0=0.08465874191669426, w1=0.0693789003885375, grad=[ 1.81057285e-02  1.23713637e-01 -2.82687904e-02 -3.26305263e-02\n",
      " -1.16291002e-04 -3.83175851e-02  6.45620643e-04  5.50952384e-02\n",
      "  1.43093455e-02  1.80776558e-02  1.22579114e-02  6.20796328e-02\n",
      "  9.30324345e-05  5.35894389e-02  7.24720576e-02  9.26311982e-02\n",
      " -5.79529303e-02 -4.61166908e-02  3.35916973e-02 -1.75751138e-02\n",
      "  3.48493261e-03 -4.38200515e-03  3.85219002e-02  1.33319370e-03\n",
      "  2.33862711e-03  2.25783813e-03  1.04757876e-03  1.50289712e-04\n",
      "  1.33291950e-04  2.21328872e-02]\n",
      "GD iter. 23/99: loss=0.4772731243163469, w0=0.08286054882545063, w1=0.05761980425160072, grad=[ 1.79819309e-02  1.17590961e-01 -3.04305694e-02 -3.04161658e-02\n",
      "  4.09314353e-05 -3.75784578e-02  7.93413718e-04  5.20168375e-02\n",
      "  1.18631342e-02  1.82193405e-02  1.34604154e-02  6.03045167e-02\n",
      "  2.47673079e-04  5.19964682e-02  6.77338975e-02  8.40962228e-02\n",
      " -5.66178265e-02 -4.55837848e-02  3.20971276e-02 -1.72598780e-02\n",
      "  2.86033491e-03 -4.52977612e-03  3.73541741e-02  2.76933173e-05\n",
      "  7.94390002e-04  7.20629312e-04  1.19179256e-03  3.03931743e-04\n",
      "  2.89130771e-04  2.23638364e-02]\n",
      "GD iter. 24/99: loss=0.47265094175668093, w0=0.08107595548088546, w1=0.046428465195067325, grad=[ 0.01784593  0.11191339 -0.03210434 -0.02837004  0.0001619  -0.03690899\n",
      "  0.00090539  0.04930583  0.0096544   0.01834874  0.01462125  0.05853375\n",
      "  0.00036616  0.05053439  0.06344506  0.07638208 -0.05521857 -0.04483509\n",
      "  0.03057839 -0.01696566  0.00232303 -0.00466653  0.03629843 -0.00108412\n",
      " -0.00052793 -0.00059532  0.00129793  0.00042154  0.00040872  0.02253591]\n",
      "GD iter. 25/99: loss=0.4684365229086597, w0=0.07930701366175205, w1=0.03576518884830163, grad=[ 0.01768942  0.10663276 -0.03337543 -0.02647928  0.00025539 -0.03629705\n",
      "  0.00099028  0.04690576  0.00766138  0.01845943  0.01572299  0.05677392\n",
      "  0.00045727  0.0491861   0.05954802  0.06940651 -0.05379291 -0.04391799\n",
      "  0.02905468 -0.01668714  0.00186218 -0.00479795  0.03534212 -0.00203042\n",
      " -0.00165955 -0.00172117  0.00137495  0.00051185  0.00050082  0.02265394]\n",
      "GD iter. 26/99: loss=0.4645781797954731, w0=0.07755627448893025, w1=0.025594386278174952, grad=[ 0.01750739  0.10170803 -0.03431378 -0.02473141  0.00032804 -0.035733\n",
      "  0.00105466  0.04476966  0.00586456  0.01854751  0.01675485  0.05503075\n",
      "  0.0005276   0.04793705  0.05599373  0.06309582 -0.05236814 -0.0428718\n",
      "  0.02754158 -0.01641997  0.00146819 -0.00492779  0.0344741  -0.00283494\n",
      " -0.00262692 -0.00268331  0.00142967  0.00058149  0.00057207  0.02272278]\n",
      "GD iter. 27/99: loss=0.46103258346454296, w0=0.07582653963949888, w1=0.015883992882799788, grad=[ 0.01729735  0.09710393 -0.03497686 -0.02311456  0.00038486 -0.03520912\n",
      "  0.00110353  0.04285838  0.00424641  0.01861088  0.01771088  0.05330919\n",
      "  0.00058218  0.04677486  0.05274029  0.05738393 -0.05096359 -0.04172908\n",
      "  0.02605159 -0.01616066  0.00113261 -0.0050584   0.03368453 -0.00351773\n",
      " -0.00345252 -0.00350416  0.0014673   0.00063543  0.00062749  0.02274715]\n",
      "GD iter. 28/99: loss=0.45776316481608564, w0=0.07412067833270534, w1=0.006604989964038703, grad=[ 0.01705861  0.09279003 -0.03541199 -0.02161759  0.00042964 -0.03471924\n",
      "  0.00114064  0.0411392   0.00279106  0.01864875  0.01858865  0.05161345\n",
      "  0.00062479  0.04568891  0.04975185  0.05221157 -0.04959264 -0.04051667\n",
      "  0.0245946  -0.01590653  0.00084795 -0.00519112  0.03296467 -0.00409577\n",
      " -0.00415554 -0.00420287  0.0014918   0.00067747  0.00067085  0.02273154]\n",
      "GD iter. 29/99: loss=0.4547388628277663, w0=0.07244149571471087, w1=-0.00226899194394929, grad=[ 0.01679183  0.08873982 -0.03565825 -0.02023016  0.00046526 -0.03425833\n",
      "  0.00116883  0.03958475  0.00148414  0.01866123  0.01938822  0.04994711\n",
      "  0.00065829  0.04467013  0.0469977   0.04752559 -0.04826412 -0.0392567\n",
      "  0.02317829 -0.0156555   0.00060764 -0.00532659  0.0323068  -0.00458355\n",
      " -0.0047524  -0.0047958   0.00150621  0.00071044  0.00070503  0.02268016]\n",
      "GD iter. 30/99: loss=0.45193313609040564, w0=0.0707916411354136, w1=-0.010762004293985251, grad=[ 0.01649855  0.08493012 -0.03574801 -0.01894273  0.00049388 -0.0338223\n",
      "  0.00119025  0.03817205  0.00031256  0.01864908  0.0201114   0.04831312\n",
      "  0.00068485  0.04371072  0.04445144  0.04327826 -0.04698355 -0.03796729\n",
      "  0.02180849 -0.01540608  0.00040586 -0.00546498  0.03170407 -0.0049934\n",
      " -0.00525719 -0.00529702  0.00151284  0.00073652  0.0007322   0.02259691]\n",
      "GD iter. 31/99: loss=0.4493231760086375, w0=0.06917354713785025, w1=-0.018896059038614584, grad=[ 0.01618094  0.08134055 -0.03570814 -0.01774656  0.00051714 -0.03340782\n",
      "  0.00120651  0.03688175 -0.0007356   0.01861346  0.02076115  0.04671393\n",
      "  0.00070609  0.04280397  0.0420904   0.03942674 -0.04575394 -0.03666323\n",
      "  0.02048948 -0.01515722  0.00023751 -0.0056061   0.03115041 -0.00533591\n",
      " -0.00568205 -0.00571862  0.00151349  0.00075732  0.000754    0.02248539]\n",
      "GD iter. 32/99: loss=0.4468892759283886, w0=0.06758939186517862, w1=-0.026691364436585194, grad=[ 1.58415527e-02  7.79530540e-02 -3.55609769e-02 -1.66336764e-02\n",
      "  5.36280350e-04 -3.30121149e-02  1.21884667e-03  3.56975405e-02\n",
      " -1.67123177e-03  1.85557886e-02  2.13411973e-02  4.51514808e-02\n",
      "  7.23263852e-04  4.19440843e-02  3.98949830e-02  3.59325329e-02\n",
      " -4.45765034e-02 -3.53565308e-02  1.92242102e-02 -1.49082144e-02\n",
      "  9.81038272e-05 -5.74958242e-03  3.06404536e-02 -5.62018140e-03\n",
      " -6.03744582e-03 -6.07106078e-03  1.50951061e-03  7.74080050e-04\n",
      "  7.71669164e-04  2.23488725e-02]\n",
      "GD iter. 33/99: loss=0.444614321875341, w0=0.06604107911223962, w1=-0.03416652603130429, grad=[ 1.54831275e-02  7.47516159e-02 -3.53251784e-02 -1.55968335e-02\n",
      "  5.52241659e-04 -3.26328779e-02  1.22817445e-03  3.46056042e-02\n",
      " -2.50428573e-03  1.84776487e-02  2.18556775e-02  4.36272999e-02\n",
      "  7.37293991e-04  4.11260675e-02  3.78482829e-02  3.27610539e-02\n",
      " -4.34511483e-02 -3.40568815e-02  1.80145869e-02 -1.46586804e-02\n",
      " -1.63135432e-05 -5.89489987e-03  3.01694249e-02 -5.85403429e-03\n",
      " -6.33244597e-03 -6.36336421e-03  1.50195945e-03  7.87726748e-04\n",
      "  7.86143386e-04  2.21903402e-02]\n",
      "GD iter. 34/99: loss=0.4424833788912549, w0=0.06453023147458636, w1=-0.04133871903583857, grad=[ 0.01510848  0.07172193 -0.03501634 -0.01462945  0.00056573 -0.03226818\n",
      "  0.00123519  0.03359423 -0.00324387  0.01838067  0.02230894  0.04214254\n",
      "  0.00074889  0.04034559  0.03593563  0.0298812  -0.04237687 -0.03277203\n",
      "  0.0168616  -0.01440844 -0.0001092  -0.00604148  0.0297331  -0.00604423\n",
      " -0.00657491 -0.00660337  0.00149165  0.00079896  0.00079814  0.02201249]\n",
      "GD iter. 35/99: loss=0.4404833530613101, w0=0.06305819303246486, w1=-0.04822383708275787, grad=[ 0.01472038  0.06885118 -0.03464752 -0.01372557  0.00057729 -0.0319164\n",
      "  0.00124042  0.03265345 -0.00389832  0.01826651  0.02270538  0.040698\n",
      "  0.00075859  0.03959889  0.03414428  0.02726499 -0.04135203 -0.03150809\n",
      "  0.01576552 -0.0141575  -0.0001836  -0.00618871  0.02932771 -0.00619663\n",
      " -0.00677166 -0.00679788  0.00147921  0.00080833  0.00080819  0.02181775]\n",
      "GD iter. 36/99: loss=0.43860271386616184, w0=0.061626038795671356, w1=-0.054836621303359395, grad=[ 0.01432154  0.06612784 -0.03422972 -0.01287981  0.00058733 -0.03157615\n",
      "  0.00124427  0.03177475 -0.00447528  0.01813678  0.02304933  0.03929421\n",
      "  0.0007668   0.03888272  0.03246311  0.02488725 -0.0403746  -0.03026987\n",
      "  0.01472601 -0.01390599 -0.00024216 -0.006336    0.02894994 -0.0063163\n",
      " -0.00692866 -0.00695283  0.00146513  0.00081621  0.00081671  0.02160832]\n",
      "GD iter. 37/99: loss=0.4368312649035597, w0=0.06023458876455616, w1=-0.06119077292454166, grad=[ 0.0139145   0.06354152 -0.03377216 -0.01208729  0.00059615 -0.03124624\n",
      "  0.00124703  0.03095085 -0.00498173  0.01799302  0.02334495  0.03793143\n",
      "  0.00077383  0.03819424  0.03088241  0.02272531 -0.03944228 -0.02906103\n",
      "  0.01374226 -0.01365413 -0.0002872  -0.00648277  0.02859682 -0.00640765\n",
      " -0.00705107 -0.00707338  0.00144979  0.00082294  0.000824    0.02138617]\n",
      "GD iter. 38/99: loss=0.4351599536113655, w0=0.0588844249630596, w1=-0.06729905196302333, grad=[ 0.01350164  0.06108279 -0.03328264 -0.01134362  0.00060399 -0.03092567\n",
      "  0.00124893  0.0301755  -0.0054241   0.01783673  0.02359624  0.03660971\n",
      "  0.00077991  0.03753096  0.02939364  0.02075874 -0.03855267 -0.02788433\n",
      "  0.01281309 -0.0134022  -0.00032077 -0.00662847  0.02826573 -0.00647451\n",
      " -0.00714343 -0.00716404  0.00143349  0.00082872  0.00083032  0.02115307]\n",
      "GD iter. 39/99: loss=0.43358071260284525, w0=0.05757591019636917, w1=-0.07317336411756709, grad=[ 0.01308515  0.05874312 -0.03276775 -0.01064482  0.00061103 -0.03061354\n",
      "  0.00125014  0.02944331 -0.00580825  0.01766929  0.02380693  0.03532888\n",
      "  0.00078522  0.03689074  0.0279893   0.01896915 -0.03770332 -0.02674174\n",
      "  0.01193701 -0.01315054 -0.00034462 -0.00677262  0.02795432 -0.00652021\n",
      " -0.0072097  -0.00722875  0.00141646  0.00083375  0.00083583  0.02091059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 40/99: loss=0.4320863267511844, w0=0.05630920759881957, w1=-0.07882483758109204, grad=[ 0.01266703  0.05651473 -0.03223304 -0.00998729  0.00061742 -0.03030909\n",
      "  0.0012508   0.02874963 -0.00613956  0.017492    0.02398055  0.03408864\n",
      "  0.0007899   0.03627167  0.02666274  0.01733993 -0.03689178 -0.02563463\n",
      "  0.01111232 -0.01289948 -0.00036029 -0.00691474  0.02766052 -0.00654769\n",
      " -0.00725332 -0.00727097  0.00139889  0.00083816  0.00084068  0.02066018]\n",
      "GD iter. 41/99: loss=0.4306703213453648, w0=0.055084300283489075, w1=-0.08426389119358163, grad=[ 0.01224907  0.05439054 -0.03168324 -0.0093678   0.00062325 -0.03001166\n",
      "  0.00125101  0.02809044 -0.00642296  0.01730608  0.02412035  0.03288853\n",
      "  0.00079405  0.0356721   0.02540808  0.01585611 -0.03611571 -0.02456385\n",
      "  0.01033713 -0.01264938 -0.00036913 -0.00705445  0.02738247 -0.00655946\n",
      " -0.00727735 -0.0072937   0.00138093  0.00084204  0.00084497  0.02040309]\n",
      "GD iter. 42/99: loss=0.42932686756617305, w0=0.05390101059695239, w1=-0.08950029511625356, grad=[ 0.0118329   0.05236404 -0.03112234 -0.0087834   0.00062861 -0.02972066\n",
      "  0.00125083  0.02746223 -0.00666295  0.01711263  0.02422936  0.03172798\n",
      "  0.00079775  0.03509057  0.02422008  0.01450419 -0.03537282 -0.02352983\n",
      "  0.00960945 -0.01240059 -0.00037229 -0.00719138  0.02711854 -0.00655775\n",
      " -0.00728444 -0.0072996   0.0013627   0.00084549  0.00084879  0.02014046]\n",
      "GD iter. 43/99: loss=0.4280507022598744, w0=0.052759018631671636, w1=-0.0945432250142572, grad=[ 0.01141992  0.0504293  -0.03055373 -0.00823144  0.00063356 -0.02943556\n",
      "  0.00125033  0.02686196 -0.00686367  0.0169127   0.02431034  0.03060631\n",
      "  0.00080106  0.03452579  0.02309404  0.01327196 -0.03466096 -0.02253263\n",
      "  0.00892722 -0.01215343 -0.00037078 -0.00732522  0.02686726 -0.0065445\n",
      " -0.00727691 -0.00729099  0.00134429  0.00084855  0.00085219  0.01987333]\n",
      "GD iter. 44/99: loss=0.4268370595634115, w0=0.05165787976330329, w1=-0.09940131057899147, grad=[ 0.01101139  0.04858086 -0.02998028 -0.00770951  0.00063815 -0.02915591\n",
      "  0.00124956  0.02628698 -0.00702888  0.01670721  0.02436585  0.02952278\n",
      "  0.00080404  0.03397664  0.02202575  0.0121484  -0.03397808 -0.02157208\n",
      "  0.00828833 -0.01190824 -0.00036546 -0.00745571  0.02662733 -0.00652141\n",
      " -0.00725681 -0.00726991  0.00132577  0.00085128  0.00085524  0.0196026 ]\n",
      "GD iter. 45/99: loss=0.4256816123931037, w0=0.05059704106784387, w1=-0.10408267909407144, grad=[ 0.01060839  0.04681369 -0.02940443 -0.00721542  0.00064241 -0.02888131\n",
      "  0.00124854  0.02573494 -0.00716205  0.01649703  0.02439822  0.02847656\n",
      "  0.00080671  0.03344209  0.02101143  0.01112354 -0.03332228 -0.02064776\n",
      "  0.00769066 -0.01166531 -0.00035707 -0.0075826   0.02639759 -0.00648995\n",
      " -0.00722594 -0.00723815  0.00130721  0.00085372  0.00085797  0.01932908]\n",
      "GD iter. 46/99: loss=0.4245804221745953, w0=0.04957585653972304, w1=-0.10859499464491211, grad=[ 0.01021185  0.04512316 -0.02882825 -0.00674719  0.00064639 -0.0286114\n",
      "  0.00124731  0.02520381 -0.00726632  0.01628295  0.02440959  0.02746679\n",
      "  0.00080911  0.03292127  0.02004764  0.01018837 -0.03269177 -0.01975909\n",
      "  0.00713209 -0.01142491 -0.00034627 -0.0077057   0.026177   -0.00645141\n",
      " -0.00718587 -0.00719726  0.00128865  0.00085589  0.0008604   0.01905351]\n",
      "GD iter. 47/99: loss=0.4235298954852264, w0=0.04859360108124703, w1=-0.11294549348611331, grad=[ 0.00982255  0.04350499 -0.02825348 -0.00630303  0.00065009 -0.02834584\n",
      "  0.00124587  0.02469179 -0.00734458  0.01606568  0.0244019   0.02649254\n",
      "  0.00081126  0.03241337  0.01913127  0.00933474 -0.03208489 -0.01890535\n",
      "  0.00661055 -0.01118729 -0.00033359 -0.00782487  0.02596464 -0.00640694\n",
      " -0.00713799 -0.00714862  0.00127015  0.00085781  0.00086257  0.01877655]\n",
      "GD iter. 48/99: loss=0.42252674651626343, w0=0.04764948326986548, w1=-0.11714101600961926, grad=[ 0.00944118  0.04195523 -0.02768161 -0.0058813   0.00065355 -0.02808437\n",
      "  0.00124426  0.0241973  -0.00739947  0.01584588  0.02437692  0.02555289\n",
      "  0.00081319  0.03191769  0.01825948  0.00855526 -0.03150011 -0.0180857\n",
      "  0.00612398 -0.01095267 -0.0003195  -0.00793995  0.02575971 -0.00635751\n",
      " -0.0070835  -0.00709345  0.00125172  0.00085951  0.0008645   0.01849877]\n",
      "GD iter. 49/99: loss=0.4215679644534352, w0=0.046742656935503224, w1=-0.12118803569713567, grad=[ 0.00906826  0.0404702  -0.02711387 -0.00548049  0.00065676 -0.02782671\n",
      "  0.00124248  0.02371892 -0.00743339  0.01562412  0.02433628  0.02464687\n",
      "  0.00081489  0.03143358  0.01742967  0.00784326 -0.030936   -0.01729925\n",
      "  0.00567041 -0.01072125 -0.0003044  -0.00805087  0.02556145 -0.00630399\n",
      " -0.00702347 -0.0070328   0.0012334   0.00086099  0.00086619  0.0182207 ]\n",
      "GD iter. 50/99: loss=0.4206507850292156, w0=0.045872231597998345, w1=-0.12509268539024773, grad=[ 0.00870425  0.0390465  -0.02655131 -0.00509925  0.00065976 -0.02757265\n",
      "  0.00124054  0.02325544 -0.00744854  0.01540095  0.02428143  0.02377349\n",
      "  0.00081639  0.03096048  0.01663946  0.00719267 -0.03039126 -0.01654503\n",
      "  0.0052479  -0.0104932  -0.00028863 -0.00815753  0.02536924 -0.00624712\n",
      " -0.00695883 -0.0069676   0.0012152   0.00086227  0.00086766  0.0179428 ]\n",
      "GD iter. 51/99: loss=0.41977266562649296, w0=0.045037281826370006, w1=-0.12886078116934938, grad=[ 0.0083495   0.03768096 -0.02599481 -0.00473634  0.00066254 -0.02732197\n",
      "  0.00123844  0.02280573 -0.00744691  0.01517685  0.02421372  0.02293179\n",
      "  0.00081769  0.03049785  0.01588666  0.006598   -0.02986467 -0.01582204\n",
      "  0.00485461 -0.01026867 -0.00027245 -0.00825988  0.02518249 -0.00618757\n",
      " -0.00689041 -0.00689865  0.00119715  0.00086336  0.00086893  0.01766549]\n",
      "GD iter. 52/99: loss=0.41893126341636316, w0=0.044236855588670666, w1=-0.1324978440964424, grad=[ 0.00800426  0.03637063 -0.02544509 -0.00439061  0.00066512 -0.02707449\n",
      "  0.0012362   0.02236883 -0.00743033  0.01495226  0.02413435  0.02212077\n",
      "  0.00081881  0.03004525  0.01516922  0.00605429 -0.02935512 -0.01512927\n",
      "  0.00448874 -0.01004779 -0.00025612 -0.00835788  0.02500068 -0.00612591\n",
      " -0.0068189  -0.00682667  0.00117926  0.00086426  0.00087     0.01738914]\n",
      "GD iter. 53/99: loss=0.4181244160973151, w0=0.04346998166471155, w1=-0.13600912004601462, grad=[ 0.00766874  0.03511276 -0.02490276 -0.00406103  0.00066749 -0.02683004\n",
      "  0.00123382  0.02194387 -0.00740045  0.01472756  0.02404443  0.02133945\n",
      "  0.00081974  0.02960223  0.01448528  0.005557   -0.02886158 -0.01446566\n",
      "  0.0041486  -0.00983066 -0.00023982 -0.00845152  0.02482335 -0.00606262\n",
      " -0.00674495 -0.00675228  0.00116153  0.00086499  0.00087087  0.01711408]\n",
      "GD iter. 54/99: loss=0.4173501248725579, w0=0.042735676194895926, w1=-0.13939959782165437, grad=[ 0.00734305  0.03390478 -0.0243683  -0.00374663  0.00066967 -0.02658848\n",
      "  0.0012313   0.02153006 -0.00735881  0.01450311  0.02394497  0.02058686\n",
      "  0.00082049  0.02916841  0.01383309  0.00510203 -0.02838312 -0.01383019\n",
      "  0.00383255 -0.00961736 -0.00022371 -0.0085408   0.0246501  -0.00599814\n",
      " -0.00666908 -0.00667603  0.00114399  0.00086553  0.00087156  0.01684061]\n",
      "GD iter. 55/99: loss=0.41660653935958714, w0=0.04203294843745955, w1=-0.1426740257331022, grad=[ 0.00702728  0.03274428 -0.02384211 -0.00344654  0.00067167 -0.02634966\n",
      "  0.00122864  0.02112671 -0.00730675  0.01427922  0.02383687  0.01986204\n",
      "  0.00082107  0.02874343  0.01321101  0.00468567 -0.02791885 -0.01322181\n",
      "  0.00353906 -0.00940796 -0.00020792 -0.00862572  0.02448056 -0.00593284\n",
      " -0.00659179 -0.00659838  0.00112663  0.00086591  0.00087206  0.01656898]\n",
      "GD iter. 56/99: loss=0.4158919441735087, w0=0.04136080580415238, w1=-0.14583692678851135, grad=[ 0.00672143  0.03162901 -0.0233245  -0.00315994  0.00067347 -0.02611347\n",
      "  0.00122585  0.0207332  -0.00724555  0.01405619  0.02372096  0.01916404\n",
      "  0.00082148  0.02832696  0.01261753  0.00430452 -0.027468   -0.01263951\n",
      "  0.00326664 -0.00920252 -0.00019256 -0.0087063   0.02431441 -0.00586704\n",
      " -0.00651348 -0.00651974  0.00110945  0.00086611  0.00087239  0.01629943]\n",
      "GD iter. 57/99: loss=0.4152047469650211, w0=0.04071825824122448, w1=-0.14889261263931175, grad=[ 0.00642548  0.03055686 -0.02281572 -0.0028861   0.0006751  -0.02587979\n",
      "  0.00122292  0.02034897 -0.00717633  0.01383427  0.02359801  0.01849194\n",
      "  0.00082172  0.0279187   0.01205121  0.00395552 -0.02702981 -0.01208228\n",
      "  0.00301391 -0.00900107 -0.0001777  -0.00878258  0.02415136 -0.00580101\n",
      " -0.00643451 -0.00644048  0.00109247  0.00086615  0.00087254  0.01603216]\n",
      "GD iter. 58/99: loss=0.41454346772671713, w0=0.04010432201882596, w1=-0.15184519639986976, grad=[ 0.00613936  0.02952584 -0.02231595 -0.00262431  0.00067654 -0.02564853\n",
      "  0.00121987  0.01997351 -0.00710013  0.01361368  0.02346869  0.01784482\n",
      "  0.0008218   0.02751836  0.01151073  0.00363585 -0.02660362 -0.01154913\n",
      "  0.00277955 -0.00880363 -0.0001634  -0.00885461  0.02399115 -0.005735\n",
      " -0.0063552  -0.00636089  0.00107569  0.00086602  0.00087252  0.01576735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 59/99: loss=0.41390672920874166, w0=0.03951802298784238, w1=-0.1546986044508024, grad=[ 0.00586299  0.02853408 -0.02182532 -0.00237395  0.0006778  -0.0254196\n",
      "  0.00121668  0.01960639 -0.00701788  0.01339462  0.02333364  0.0172218\n",
      "  0.00082171  0.02712568  0.01099482  0.00334299 -0.0261888  -0.0110391\n",
      "  0.00256233 -0.0086102  -0.00014972 -0.00892242  0.02383355 -0.0056692\n",
      " -0.00627581 -0.00628126  0.00105909  0.00086574  0.00087233  0.01550517]\n",
      "GD iter. 60/99: loss=0.41329324830774594, w0=0.03895839935894745, w1=-0.15745658732306173, grad=[ 0.00559624  0.02757983 -0.02134392 -0.00213443  0.00067889 -0.02519291\n",
      "  0.00121336  0.01924718 -0.00693043  0.01317728  0.02319341  0.016622\n",
      "  0.00082146  0.02674042  0.01050231  0.0030746  -0.02578477 -0.01055126\n",
      "  0.00236106 -0.0084208  -0.00013668 -0.00898608  0.02367837 -0.00560379\n",
      " -0.00619658 -0.00620181  0.0010427   0.00086529  0.00087198  0.01524574]\n",
      "GD iter. 61/99: loss=0.41270182831232516, w0=0.0384245040543975, w1=-0.1601227297495556, grad=[ 0.00533895  0.02666142 -0.02087179 -0.00190518  0.0006798  -0.0249684\n",
      "  0.00120992  0.01889552 -0.00683854  0.01296182  0.02304855  0.01604457\n",
      "  0.00082104  0.02636235  0.01003207  0.00282859 -0.02539101 -0.01008469\n",
      "  0.00217467 -0.00823541 -0.0001243  -0.00904564  0.02352542 -0.00553891\n",
      " -0.0061177  -0.00612273  0.00102649  0.00086468  0.00087145  0.0149892 ]\n",
      "GD iter. 62/99: loss=0.4121313519043369, w0=0.037915406678916845, w1=-0.16270045996191548, grad=[ 0.00509097  0.0257773  -0.02040895 -0.00168571  0.00068054 -0.02474599\n",
      "  0.00120635  0.01855109 -0.00674292  0.01274838  0.02289951  0.01548871\n",
      "  0.00082047  0.02599124  0.00958306  0.00260303 -0.02500703 -0.00963851\n",
      "  0.0020021  -0.008054   -0.0001126  -0.00910118  0.02337453 -0.00547468\n",
      " -0.00603935 -0.00604419  0.00101048  0.00086392  0.00087077  0.01473565]\n",
      "GD iter. 63/99: loss=0.41158077482918104, w0=0.03743019515199717, w1=-0.16519305830191672, grad=[ 0.00485212  0.02492598 -0.01995537 -0.00147555  0.00068111 -0.02452562\n",
      "  0.00120264  0.01821356 -0.00664419  0.01253707  0.02274674  0.0149536\n",
      "  0.00081974  0.02562691  0.00915427  0.00239617 -0.02463238 -0.00921188\n",
      "  0.0018424  -0.00787655 -0.00010157 -0.00915277  0.02322557 -0.0054112\n",
      " -0.00596167 -0.00596635  0.00099466  0.00086299  0.00086992  0.01448517]\n",
      "GD iter. 64/99: loss=0.41104912015974354, w0=0.036967977040102414, w1=-0.16760366520986292, grad=[ 4.62218112e-03  2.41060691e-02 -1.95110336e-02 -1.27423729e-03\n",
      "  6.81501898e-04 -2.43072503e-02  1.19881512e-03  1.78826739e-02\n",
      " -6.54291062e-03  1.23280136e-02  2.25906328e-02  1.44384706e-02\n",
      "  8.18850553e-04  2.52691454e-02  8.74475996e-03  2.20640914e-03\n",
      " -2.42666225e-02 -8.80395361e-03  1.69464953e-03 -7.70302372e-03\n",
      " -9.12179508e-05 -9.20047570e-03  2.30784121e-02 -5.34856841e-03\n",
      " -5.88478471e-03 -5.88930779e-03  9.79030248e-04  8.61914054e-04\n",
      "  8.68905764e-04  1.42378513e-02]\n",
      "GD iter. 65/99: loss=0.4105354730885322, w0=0.036527880623662926, w1=-0.1699352886458574, grad=[ 4.40096416e-03  2.33162344e-02 -1.90758623e-02 -1.08137273e-03\n",
      "  6.81727624e-04 -2.40908155e-02  1.19485865e-03  1.75581769e-02\n",
      " -6.43960017e-03  1.21213001e-02  2.24315478e-02  1.39425776e-02\n",
      "  8.17804697e-04  2.49177734e-02  8.35364154e-03  2.03230195e-03\n",
      " -2.39093851e-02 -8.41395316e-03  1.55800967e-03 -7.53338186e-03\n",
      " -8.15247433e-05 -9.24437803e-03  2.29329375e-02 -5.28684124e-03\n",
      " -5.80879235e-03 -5.81317901e-03  9.63583684e-04  8.60679549e-04\n",
      "  8.67735664e-04  1.39937530e-02]\n",
      "GD iter. 66/99: loss=0.4100389761909582, w0=0.03610905573037469, w1=-0.17219081099419845, grad=[ 4.18824893e-03  2.25552235e-02 -1.86497808e-02 -8.96566798e-04\n",
      "  6.81784671e-04 -2.38762750e-02  1.19077516e-03  1.72398403e-02\n",
      " -6.33471612e-03  1.19170107e-02  2.22698204e-02  1.34651914e-02\n",
      "  8.16602986e-04  2.45726186e-02  7.98006262e-03  1.87251713e-03\n",
      " -2.35603028e-02 -8.04110970e-03  1.43168260e-03 -7.36757728e-03\n",
      " -7.24789311e-05 -9.28455687e-03  2.27890480e-02 -5.22607615e-03\n",
      " -5.73378060e-03 -5.73804395e-03  9.48319555e-04  8.59290626e-04\n",
      "  8.66407095e-04  1.37529308e-02]\n",
      "GD iter. 67/99: loss=0.40955882510988245, w0=0.03571067436319589, w1=-0.17437299549607127, grad=[ 3.98381367e-03  2.18218450e-02 -1.82326949e-02 -7.19457391e-04\n",
      "  6.81674167e-04 -2.36635875e-02  1.18656507e-03  1.69274547e-02\n",
      " -6.22867176e-03  1.17152154e-02  2.21057556e-02  1.30056099e-02\n",
      "  8.15246359e-04  2.42335151e-02  7.62321765e-03  1.72584342e-03\n",
      " -2.32190401e-02 -7.68468727e-03  1.31492651e-03 -7.20555952e-03\n",
      " -6.40626319e-05 -9.32109362e-03  2.26466545e-02 -5.16631608e-03\n",
      " -5.65981956e-03 -5.66397157e-03  9.33234579e-04  8.57748201e-04\n",
      "  8.64921300e-04  1.35154293e-02]\n",
      "GD iter. 68/99: loss=0.40909426461769793, w0=0.03533193114855433, w1=-0.1764844922511952, grad=[ 3.78743215e-03  2.11149676e-02 -1.78244973e-02 -5.49704729e-04\n",
      "  6.81397256e-04 -2.34527148e-02  1.18222884e-03  1.66208281e-02\n",
      " -6.12183799e-03  1.15159732e-02  2.19396333e-02  1.25631549e-02\n",
      "  8.13735776e-04  2.39003050e-02  7.28234120e-03  1.59117511e-03\n",
      " -2.28852850e-02 -7.34397837e-03  1.20704900e-03 -7.04727433e-03\n",
      " -5.62552102e-05 -9.35407111e-03  2.25056775e-02 -5.10759369e-03\n",
      " -5.58696599e-03 -5.59101760e-03  9.18325292e-04  8.56053216e-04\n",
      "  8.63279516e-04  1.32812842e-02]\n",
      "GD iter. 69/99: loss=0.4086445850175044, w0=0.03497204362763901, w1=-0.17852784382505463, grad=[ 3.59887521e-03  2.04335157e-02 -1.74250704e-02 -3.86989488e-04\n",
      "  6.80955106e-04 -2.32436224e-02  1.17776702e-03  1.63197840e-02\n",
      " -6.01454707e-03  1.13193338e-02  2.17717106e-02  1.21371716e-02\n",
      "  8.12072236e-04  2.35728369e-02  6.95670554e-03  1.46750251e-03\n",
      " -2.25587463e-02 -7.01830334e-03  1.10740429e-03 -6.89266425e-03\n",
      " -4.90339420e-05 -9.38357318e-03  2.23660466e-02 -5.04993271e-03\n",
      " -5.51526502e-03 -5.51922622e-03  9.03588091e-04  8.54206649e-04\n",
      "  8.61482998e-04  1.30505232e-02]\n",
      "GD iter. 70/99: loss=0.408209118849495, w0=0.03463025241123689, w1=-0.18050549049474288, grad=[ 3.41791216e-03  1.97764667e-02 -1.70342874e-02 -2.31011123e-04\n",
      "  6.80348932e-04 -2.30362784e-02  1.17318021e-03  1.60241597e-02\n",
      " -5.90709609e-03  1.11253378e-02  2.16022238e-02  1.17270282e-02\n",
      "  8.10256787e-04  2.32509658e-02  6.64561830e-03  1.35390334e-03\n",
      " -2.22391523e-02 -6.70700972e-03  1.01539054e-03 -6.74166912e-03\n",
      " -4.23745812e-05 -9.40968443e-03  2.22276992e-02 -4.99334921e-03\n",
      " -5.44475158e-03 -5.44863151e-03  8.89019279e-04  8.52209533e-04\n",
      "  8.59533029e-04  1.28231664e-02]\n",
      "GD iter. 71/99: loss=0.40778723787260684, w0=0.034305821216381455, w1=-0.18241977516323085, grad=[ 3.24431195e-03  1.91428467e-02 -1.66520145e-02 -8.14863612e-05\n",
      "  6.79580003e-04 -2.28306536e-02  1.16846911e-03  1.57338046e-02\n",
      " -5.79975006e-03  1.09340183e-02  2.14313901e-02  1.13321157e-02\n",
      "  8.08290537e-04  2.29345525e-02  6.34842048e-03  1.24953484e-03\n",
      " -2.19262488e-02 -6.40947153e-03  9.30447229e-04 -6.59422658e-03\n",
      " -3.62518399e-05 -9.43248984e-03  2.20905797e-02 -4.93785258e-03\n",
      " -5.37545175e-03 -5.37925871e-03  8.74615093e-04  8.50062964e-04\n",
      "  8.57430936e-04  1.25992273e-02]\n",
      "GD iter. 72/99: loss=0.4073783502948966, w0=0.033998036801088316, w1=-0.18427294796899313, grad=[ 3.07784415e-03  1.85317281e-02 -1.62781121e-02  6.18521679e-05\n",
      "  6.78649655e-04 -2.26267212e-02  1.16363450e-03  1.54485792e-02\n",
      " -5.69274484e-03  1.07454010e-02  2.12594097e-02  1.09518472e-02\n",
      "  8.06174668e-04  2.26234634e-02  6.06448449e-03  1.15362672e-03\n",
      " -2.16197977e-02 -6.12508850e-03  8.52052676e-04 -6.45027251e-03\n",
      " -3.06397935e-05 -9.45207459e-03  2.19546386e-02 -4.88344651e-03\n",
      " -5.30738386e-03 -5.31112546e-03  8.60371742e-04  8.47768111e-04\n",
      "  8.55178099e-04  1.23787131e-02]\n",
      "GD iter. 73/99: loss=0.4069818982290852, w0=0.03370620881165231, w1=-0.18606717061534797, grad=[ 2.91827989e-03  1.79422265e-02 -1.59124358e-02  1.99257171e-04\n",
      "  6.77559292e-04 -2.24244564e-02  1.15867728e-03  1.51683539e-02\n",
      " -5.58628973e-03  1.05595050e-02  2.10864669e-02  1.05856572e-02\n",
      "  8.03910437e-04  2.23175698e-02  5.79321244e-03  1.06547472e-03\n",
      " -2.13195755e-02 -5.85328527e-03  7.79721616e-04 -6.30974137e-03\n",
      " -2.55122210e-05 -9.46852378e-03  2.18198320e-02 -4.83012980e-03\n",
      " -5.24055954e-03 -5.24424270e-03  8.46285428e-04  8.45326222e-04\n",
      "  8.52775958e-04  1.21616258e-02]\n",
      "GD iter. 74/99: loss=0.4065973553522751, w0=0.03342966955535494, w1=-0.1878045204415575, grad=[ 2.76539256e-03  1.73734983e-02 -1.55548380e-02  3.30967809e-04\n",
      "  6.76310399e-04 -2.22238362e-02  1.15359842e-03  1.48930076e-02\n",
      " -5.48056989e-03  1.03763437e-02  2.09127317e-02  1.02330015e-02\n",
      "  8.01499184e-04  2.20167478e-02  5.53403451e-03  9.84434738e-04\n",
      " -2.10253724e-02 -5.59351058e-03  7.13002924e-04 -6.17256658e-03\n",
      " -2.08428877e-05 -9.48192227e-03  2.16861209e-02 -4.77789708e-03\n",
      " -5.17498461e-03 -5.17861564e-03  8.32352368e-04  8.42738626e-04\n",
      "  8.50226025e-04  1.19479620e-02]\n",
      "GD iter. 75/99: loss=0.40622422475112974, w0=0.03316777370996777, w1=-0.1894869942556666, grad=[ 2.61895845e-03  1.68247381e-02 -1.52051680e-02  4.57210714e-04\n",
      "  6.74904543e-04 -2.20248395e-02  1.14839902e-03  1.46224273e-02\n",
      " -5.37574854e-03  1.01959251e-02  2.07383612e-02  9.89335623e-03\n",
      "  7.98942338e-04  2.17208777e-02  5.28640745e-03  9.09917533e-04\n",
      " -2.07369907e-02 -5.34523643e-03  6.51477435e-04 -6.03868082e-03\n",
      " -1.66057794e-05 -9.49235448e-03  2.15534707e-02 -4.72673945e-03\n",
      " -5.11065988e-03 -5.11424453e-03  8.18568818e-04  8.40006745e-04\n",
      "  8.47529881e-04  1.17377140e-02]\n",
      "GD iter. 76/99: loss=0.4058620369357598, w0=0.03291989798012231, w1=-0.19111651194720108, grad=[ 2.47875730e-03  1.62951769e-02 -1.48632737e-02  5.78200909e-04\n",
      "  6.73343378e-04 -2.18274463e-02  1.14308027e-03  1.43565068e-02\n",
      " -5.27196900e-03  1.00182521e-02  2.05635003e-02  9.56621730e-03\n",
      "  7.96241415e-04  2.14298442e-02  5.04981320e-03  8.41383923e-04\n",
      " -2.04542439e-02 -5.10795724e-03  5.94755870e-04 -5.90801628e-03\n",
      " -1.27752934e-05 -9.49990425e-03  2.14218503e-02 -4.67664502e-03\n",
      " -5.04758182e-03 -5.05112534e-03  8.04931085e-04  8.37132089e-04\n",
      "  8.44689188e-04  1.15308700e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 77/99: loss=0.4055103480073154, w0=0.032685440709437345, w1=-0.19269491989617912, grad=[ 2.34457271e-03  1.57840795e-02 -1.45290017e-02  6.94142647e-04\n",
      "  6.71628642e-04 -2.16316384e-02  1.13764346e-03  1.40951461e-02\n",
      " -5.16935650e-03  9.84332370e-03  2.03882829e-02  9.25110003e-03\n",
      "  7.93398025e-04  2.11435360e-02  4.82375760e-03  7.78340384e-04\n",
      " -2.01769561e-02 -4.88118903e-03  5.42476879e-04 -5.78050492e-03\n",
      " -9.32639330e-06 -9.50465471e-03  2.12912323e-02 -4.62759943e-03\n",
      " -4.98574323e-03 -4.98925039e-03  7.91435543e-04  8.34116259e-04\n",
      "  8.41705684e-04  1.13274141e-02]\n",
      "GD iter. 78/99: loss=0.40516873796580566, w0=0.032463821456238474, w1=-0.19422399419339395, grad=[ 2.21619253e-03  1.52907430e-02 -1.42021981e-02  8.05230168e-04\n",
      "  6.69762165e-04 -2.14373985e-02  1.13209001e-03  1.38382511e-02\n",
      " -5.06801986e-03  9.67113452e-03  2.02128328e-02  8.94753833e-03\n",
      "  7.90413867e-04  2.08618452e-02  4.60776916e-03  7.20335075e-04\n",
      " -1.99049607e-02 -4.66446862e-03  4.94305180e-04 -5.65607868e-03\n",
      " -6.23473129e-06 -9.50668816e-03  2.11615924e-02 -4.57958624e-03\n",
      " -4.92513376e-03 -4.92860889e-03  7.78078645e-04  8.30960949e-04\n",
      "  8.38581192e-04  1.11273276e-02]\n",
      "GD iter. 79/99: loss=0.40483680914601466, w0=0.03225448053976003, w1=-0.19570544368557974, grad=[ 2.09340916e-03  1.48144949e-02 -1.38827090e-02  9.11648396e-04\n",
      "  6.67745862e-04 -2.12447104e-02  1.12642141e-03  1.35857323e-02\n",
      " -4.96805303e-03  9.50167582e-03  2.00372649e-02  8.65508418e-03\n",
      "  7.87290735e-04  2.05846677e-02  4.40139796e-03  6.66954216e-04\n",
      " -1.96381001e-02 -4.45735281e-03  4.49929811e-04 -5.53466964e-03\n",
      " -3.47674400e-06 -9.50608596e-03  2.10329085e-02 -4.53258732e-03\n",
      " -4.86574036e-03 -4.86918741e-03  7.64856931e-04  8.27667946e-04\n",
      "  8.35317614e-04  1.09305882e-02]\n",
      "GD iter. 80/99: loss=0.4045141847705912, w0=0.032056878562876256, w1=-0.19714091285786284, grad=[ 1.97601977e-03  1.43546917e-02 -1.35703811e-02  1.01357357e-03\n",
      "  6.65581736e-04 -2.10535590e-02  1.12063928e-03  1.33375052e-02\n",
      " -4.86953654e-03  9.33493562e-03  1.98616852e-02  8.37330697e-03\n",
      "  7.84030509e-04  2.03119028e-02  4.20421457e-03  6.17818787e-04\n",
      " -1.93762243e-02 -4.25941759e-03  4.09062478e-04 -5.41621024e-03\n",
      " -1.02972462e-06 -9.50292846e-03  2.09051612e-02 -4.48658320e-03\n",
      " -4.80754776e-03 -4.81097031e-03  7.51767037e-04  8.24239126e-04\n",
      "  8.31916931e-04  1.07371712e-02]\n",
      "GD iter. 81/99: loss=0.4042005076104308, w0=0.031870495916656805, w1=-0.19853198456481086, grad=[ 1.86382646e-03  1.39107171e-02 -1.32650618e-02  1.11117381e-03\n",
      "  6.63271877e-04 -2.08639298e-02  1.11474530e-03  1.30934888e-02\n",
      " -4.77253871e-03  9.17089902e-03  1.96861921e-02  8.10179296e-03\n",
      "  7.80635159e-04  2.00434527e-02  4.01580905e-03  5.72581533e-04\n",
      " -1.91191913e-02 -4.07025735e-03  3.71436002e-04 -5.30063335e-03\n",
      "  1.12812475e-06 -9.49729493e-03  2.07783330e-02 -4.44155331e-03\n",
      " -4.75053879e-03 -4.75394009e-03  7.38805699e-04  8.20676455e-04\n",
      "  8.28381209e-04  1.05470494e-02]\n",
      "GD iter. 82/99: loss=0.40389543874342865, w0=0.031694832271371653, w1=-0.19988018262041213, grad=[ 1.75663645e-03  1.34819806e-02 -1.29665998e-02  1.20460965e-03\n",
      "  6.60818457e-04 -2.06758093e-02  1.10874128e-03  1.28536061e-02\n",
      " -4.67711685e-03  9.00954856e-03  1.95108768e-02  7.84014466e-03\n",
      "  7.77106741e-04  1.97792229e-02  3.83579002e-03  5.30924226e-04\n",
      " -1.88668656e-02 -3.88948418e-03  3.36802860e-04 -5.18787247e-03\n",
      "  3.01765748e-06 -9.48926347e-03  2.06524083e-02 -4.39747626e-03\n",
      " -4.69469474e-03 -4.69807774e-03  7.25969761e-04  8.16981982e-04\n",
      "  8.24712587e-04  1.03601931e-02]\n",
      "GD iter. 83/99: loss=0.403598656403508, w0=0.03152940605797531, w1=-0.20118697425643292, grad=[ 1.65426213e-03  1.30679164e-02 -1.26748452e-02  1.29403454e-03\n",
      "  6.58223727e-04 -2.04891844e-02  1.10262912e-03  1.26177832e-02\n",
      " -4.58331836e-03  8.85086445e-03  1.93358239e-02  7.58798024e-03\n",
      "  7.73447394e-04  1.95191219e-02  3.66378378e-03  4.92555175e-04\n",
      " -1.86191180e-02 -3.71672711e-03  3.04933815e-04 -5.07786176e-03\n",
      "  4.65876106e-06 -9.47891098e-03  2.05273730e-02 -4.35433003e-03\n",
      " -4.63999562e-03 -4.64336300e-03  7.13256177e-04  8.13157842e-04\n",
      "  8.20913283e-04  1.01765710e-02]\n",
      "GD iter. 84/99: loss=0.4033098549125832, w0=0.03137375394357053, w1=-0.20245377245780308, grad=[ 1.55652114e-03  1.26679820e-02 -1.23896499e-02  1.37959524e-03\n",
      "  6.55490016e-04 -2.03040430e-02  1.09641077e-03  1.23859491e-02\n",
      " -4.49118164e-03  8.69482480e-03  1.91611116e-02  7.34493298e-03\n",
      "  7.69659335e-04  1.92630608e-02  3.49943346e-03  4.57206962e-04\n",
      " -1.83758254e-02 -3.55163141e-03  2.75616631e-04 -4.97053622e-03\n",
      "  6.07034770e-06 -9.46631310e-03  2.04032145e-02 -4.31209220e-03\n",
      " -4.58642045e-03 -4.58977463e-03  7.00662012e-04  8.09206249e-04\n",
      "  8.16985585e-04  9.99614972e-03]\n",
      "GD iter. 85/99: loss=0.4030287436887898, w0=0.03122743030388257, w1=-0.20368193818295985, grad=[ 1.46323640e-03  1.22816573e-02 -1.21108676e-02  1.46143227e-03\n",
      "  6.52619729e-04 -2.01203731e-02  1.09008830e-03  1.21580354e-02\n",
      " -4.40073704e-03  8.54140591e-03  1.89868128e-02  7.11065070e-03\n",
      "  7.65744859e-04  1.90109535e-02  3.34239825e-03  4.24634368e-04\n",
      " -1.81368699e-02 -3.39385791e-03  2.48654874e-04 -4.86583167e-03\n",
      "  7.27035481e-06 -9.45154417e-03  2.02799212e-02 -4.27074004e-03\n",
      " -4.53394742e-03 -4.53729059e-03  6.88184445e-04  8.05129490e-04\n",
      "  8.12931853e-04  9.81889453e-03]\n",
      "GD iter. 86/99: loss=0.4027550463249051, w0=0.03109000669535846, w1=-0.20487278247643012, grad=[ 1.37423609e-03  1.19084429e-02 -1.18383542e-02  1.53968023e-03\n",
      "  6.49615337e-04 -1.99381634e-02  1.08366385e-03  1.19339763e-02\n",
      " -4.31200765e-03  8.39058241e-03  1.88129949e-02  6.88479522e-03\n",
      "  7.61706329e-04  1.87627164e-02  3.19235261e-03  3.94612486e-04\n",
      " -1.79021389e-02 -3.24308233e-03  2.23866784e-04 -4.76368490e-03\n",
      "  8.27575372e-06 -9.43467724e-03  2.01574829e-02 -4.23025072e-03\n",
      " -4.48255413e-03 -4.48588827e-03  6.75820771e-04  8.00929928e-04\n",
      "  8.08754511e-04  9.64476919e-03]\n",
      "GD iter. 87/99: loss=0.4024884997314328, w0=0.030961071329137623, w1=-0.20602756848034262, grad=[ 1.28935366e-03  1.15478600e-02 -1.15719678e-02  1.61446822e-03\n",
      "  6.46479379e-04 -1.97574029e-02  1.07713961e-03  1.17137079e-02\n",
      " -4.22501000e-03  8.24232748e-03  1.86397207e-02  6.66704179e-03\n",
      "  7.57546180e-04  1.85182687e-02  3.04898563e-03  3.66935003e-04\n",
      " -1.76715244e-02 -3.09899468e-03  2.01084228e-04 -4.66403369e-03\n",
      "  9.10256508e-06 -9.41578397e-03  2.00358899e-02 -4.19060138e-03\n",
      " -4.43221775e-03 -4.43554464e-03  6.63568397e-04  7.96609989e-04\n",
      "  8.04456046e-04  9.47373625e-03]\n",
      "GD iter. 88/99: loss=0.40222885333930186, w0=0.03084022854881389, w1=-0.2071475133510291, grad=[ 1.20842780e-03  1.11994487e-02 -1.13115689e-02  1.68592006e-03\n",
      "  6.43214453e-04 -1.95780812e-02  1.07051787e-03  1.14971683e-02\n",
      " -4.13975480e-03  8.09661304e-03  1.84670482e-02  6.45707866e-03\n",
      "  7.53266906e-04  1.82775315e-02  2.91200028e-03  3.41412625e-04\n",
      " -1.74449226e-02 -2.96129861e-03  1.80151715e-04 -4.56681687e-03\n",
      "  9.76587973e-06 -9.39493470e-03  1.99151337e-02 -4.15176925e-03\n",
      " -4.38291514e-03 -4.38623638e-03  6.51424844e-04  7.92172164e-04\n",
      "  8.00039004e-04  9.30575721e-03]\n",
      "GD iter. 89/99: loss=0.4019758683575749, w0=0.030727098313621647, w1=-0.20823379008639134, grad=[ 1.13130235e-03  1.08627674e-02 -1.10570203e-02  1.75415464e-03\n",
      "  6.39823215e-04 -1.94001880e-02  1.06380097e-03  1.12842974e-02\n",
      " -4.05624750e-03  7.95340986e-03  1.82950314e-02  6.25460650e-03\n",
      "  7.48871060e-04  1.80404286e-02  2.78111283e-03  3.17871642e-04\n",
      " -1.72222342e-02 -2.82971085e-03  1.60925484e-04 -4.47197433e-03\n",
      "  1.02798839e-05 -9.37219835e-03  1.97952064e-02 -4.11373173e-03\n",
      " -4.33462299e-03 -4.33794003e-03  6.39387745e-04  7.87619004e-04\n",
      "  7.95505984e-04  9.14079261e-03]\n",
      "GD iter. 90/99: loss=0.40172931708194476, w0=0.030621315688424623, w1=-0.2092875292692737, grad=[ 1.05782625e-03  1.05373918e-02 -1.08081875e-02  1.81928618e-03\n",
      "  6.36308373e-04 -1.92237133e-02  1.05699130e-03  1.10750367e-02\n",
      " -3.97448889e-03  7.81268776e-03  1.81237204e-02  6.05933793e-03\n",
      "  7.44361251e-04  1.78068859e-02  2.65605222e-03  2.96152621e-04\n",
      " -1.70033632e-02 -2.70396063e-03  1.43272649e-04 -4.37944708e-03\n",
      "  1.06578875e-05 -9.34764247e-03  1.96761005e-02 -4.07646648e-03\n",
      " -4.28731792e-03 -4.29063208e-03  6.27454845e-04  7.82953109e-04\n",
      "  7.90859633e-04  8.97880222e-03]\n",
      "GD iter. 91/99: loss=0.40148898225016694, w0=0.03052253034166244, w1=-0.2103098207316839, grad=[ 9.87853468e-04  1.02229146e-02 -1.05649384e-02  1.88142447e-03\n",
      "  6.32672683e-04 -1.90486475e-02  1.05009131e-03  1.08693291e-02\n",
      " -3.89447556e-03  7.67441571e-03  1.79531616e-02  5.87099709e-03\n",
      "  7.39740135e-04  1.75768313e-02  2.53655948e-03  2.76109205e-04\n",
      " -1.67882177e-02 -2.58378914e-03  1.27070405e-04 -4.28917727e-03\n",
      "  1.09123556e-05 -9.32133320e-03  1.95578092e-02 -4.03995145e-03\n",
      " -4.24097662e-03 -4.24428904e-03  6.15623995e-04  7.78177133e-04\n",
      "  7.86102646e-04  8.81974511e-03]\n",
      "GD iter. 92/99: loss=0.4012546564408935, w0=0.0304304060522133, w1=-0.21130171514434315, grad=[ 9.21242894e-04  9.91894413e-03 -1.03271436e-02  1.94067507e-03\n",
      "  6.28918942e-04 -1.88749810e-02  1.04310350e-03  1.06671189e-02\n",
      " -3.81620042e-03  7.53856194e-03  1.77833980e-02  5.68931913e-03\n",
      "  7.35010412e-04  1.73501951e-02  2.42238719e-03  2.57607021e-04\n",
      " -1.65767089e-02 -2.46894901e-03  1.12205286e-04 -4.20110819e-03\n",
      "  1.10549410e-05 -9.29333530e-03  1.94403263e-02 -4.00416495e-03\n",
      " -4.19557584e-03 -4.19888758e-03  6.03893152e-04  7.73293771e-04\n",
      "  7.81237757e-04  8.66357980e-03]\n",
      "GD iter. 93/99: loss=0.40102614151266097, w0=0.030344620225958463, w1=-0.2122642255357206, grad=[ 8.57858263e-04  9.62510391e-03 -1.00946763e-02  1.99713958e-03\n",
      "  6.25049987e-04 -1.87027046e-02  1.03603041e-03  1.04683515e-02\n",
      " -3.73965306e-03  7.40509409e-03  1.76144696e-02  5.51404980e-03\n",
      "  7.30174822e-04  1.71269094e-02  2.31329896e-03  2.40522680e-04\n",
      " -1.63687510e-02 -2.35920383e-03  9.85724798e-05 -4.11518431e-03\n",
      "  1.10965184e-05 -9.26371210e-03  1.93236456e-02 -3.96908569e-03\n",
      " -4.15109254e-03 -4.15440453e-03  5.92260377e-04  7.68305757e-04\n",
      "  7.76267736e-04  8.51026432e-03]\n",
      "GD iter. 94/99: loss=0.40080324808005996, w0=0.030264863422682652, w1=-0.21319832874440584, grad=[ 7.97568033e-04  9.34103209e-03 -9.86741230e-03  2.05091580e-03\n",
      "  6.21068687e-04 -1.85318092e-02  1.02887462e-03  1.02729737e-02\n",
      " -3.66482017e-03  7.27397929e-03  1.74464134e-02  5.34494504e-03\n",
      "  7.25236141e-04  1.69069081e-02  2.20906890e-03  2.24742859e-04\n",
      " -1.61642615e-02 -2.25432764e-03  8.60751838e-05 -4.03135125e-03\n",
      "  1.10472193e-05 -9.23252557e-03  1.92077615e-02 -3.93469281e-03\n",
      " -4.10750392e-03 -4.11081698e-03  5.80723831e-04  7.63215863e-04\n",
      "  7.71195388e-04  8.35975634e-03]\n",
      "GD iter. 95/99: loss=0.40058579502435665, w0=0.030190838893812046, w1=-0.21410496680840296, grad=[ 7.40245289e-04  9.06638064e-03 -9.64523017e-03  2.10209792e-03\n",
      "  6.16977941e-04 -1.83622857e-02  1.02163876e-03  1.00809331e-02\n",
      " -3.59168590e-03  7.14518424e-03  1.72792635e-02  5.18177054e-03\n",
      "  7.20197173e-04  1.66901271e-02  2.10948120e-03  2.10163469e-04\n",
      " -1.59631606e-02 -2.15410449e-03  7.46240126e-05 -3.94955583e-03\n",
      "  1.09164675e-05 -9.19983625e-03  1.90926686e-02 -3.90096589e-03\n",
      " -4.06478745e-03 -4.06810232e-03  5.69281770e-04  7.58026888e-04\n",
      "  7.66023540e-04  8.21201324e-03]\n",
      "GD iter. 96/99: loss=0.40037360903604086, w0=0.030122262131375713, w1=-0.21498504829467996, grad=[ 6.85767624e-04  8.80081486e-03 -9.42801104e-03  2.15077666e-03\n",
      "  6.12780673e-04 -1.81941255e-02  1.01432544e-03  9.89217824e-03\n",
      " -3.52023214e-03  7.01867531e-03  1.71130517e-02  5.02430134e-03\n",
      "  7.15060750e-04  1.64765043e-02  2.01432965e-03  1.96688888e-04\n",
      " -1.57653710e-02 -2.05832798e-03  6.41364439e-05 -3.86974603e-03\n",
      "  1.07130139e-05 -9.16570332e-03  1.89783619e-02 -3.86788503e-03\n",
      " -4.02292094e-03 -4.02623827e-03  5.57932545e-04  7.52741657e-04\n",
      "  7.60755048e-04  8.06699221e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 97/99: loss=0.4001665241869958, w0=0.03005886042847531, w1=-0.21583944957208004, grad=[ 6.34017029e-04  8.54401277e-03 -9.21563869e-03  2.19703952e-03\n",
      "  6.08479825e-04 -1.80273197e-02  1.00693736e-03  9.70665873e-03\n",
      " -3.45043880e-03  6.89441865e-03  1.69478071e-02  4.87232149e-03\n",
      "  7.09829724e-04  1.62659789e-02  1.92341720e-03  1.84231256e-04\n",
      " -1.55708180e-02 -1.96680088e-03  5.45363055e-05 -3.79187100e-03\n",
      "  1.04449718e-05 -9.13018457e-03  1.88648362e-02 -3.83543077e-03\n",
      " -3.98188255e-03 -3.98520292e-03  5.46674598e-04  7.47363019e-04\n",
      "  7.55392783e-04  7.92465029e-03]\n",
      "GD iter. 98/99: loss=0.39996438153015823, w0=0.03000037245146033, w1=-0.21666901603049454, grad=[ 5.84879770e-04  8.29566458e-03 -9.00799953e-03  2.24097081e-03\n",
      "  6.04078358e-04 -1.78618598e-02  9.99477180e-04  9.52432493e-03\n",
      " -3.38228412e-03  6.77238017e-03  1.67835568e-02  4.72562366e-03\n",
      "  7.04506965e-04  1.60584921e-02  1.83655555e-03  1.72709841e-04\n",
      " -1.53794293e-02 -1.87933469e-03  4.57532987e-05 -3.71588107e-03\n",
      "  1.01198509e-05 -9.09333644e-03  1.87520869e-02 -3.80358419e-03\n",
      " -3.94165085e-03 -3.94497476e-03  5.35506455e-04  7.41893835e-04\n",
      "  7.49939632e-04  7.78494445e-03]\n",
      "GD iter. 99/99: loss=0.3997670287247004, w0=0.029946547823929907, w1=-0.2174745632490054, grad=[ 5.38246275e-04  8.05547219e-03 -8.80498257e-03  2.28265189e-03\n",
      "  5.99579241e-04 -1.76977371e-02  9.91947615e-04  9.34512798e-03\n",
      " -3.31574485e-03  6.65252572e-03  1.66203256e-02  4.58400879e-03\n",
      "  6.99095353e-04  1.58539866e-02  1.75356478e-03  1.62050448e-04\n",
      " -1.51911349e-02 -1.79574927e-03  3.77225564e-05 -3.64172770e-03\n",
      "  9.74459122e-06 -9.05521400e-03  1.86401093e-02 -3.77232688e-03\n",
      " -3.90220479e-03 -3.90553269e-03  5.24426728e-04  7.36336984e-04\n",
      "  7.44398494e-04  7.64783167e-03]\n",
      "GD: execution time=2.179 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.rand(30)\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, std_data, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e3cb2",
   "metadata": {},
   "source": [
    "# Test set GD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "25952b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a3342",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_star = gd_ws[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "19cf3873",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_data_test = standardize(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d7741eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'data/predictions.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w_star, std_data_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "85b082fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([350000, 350001, 350002, ..., 918235, 918236, 918237])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2667db",
   "metadata": {},
   "source": [
    "# Test build_poly + GD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37643759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\n",
    "    \n",
    "    Args:\n",
    "        x: numpy array of shape (N,), N is the number of samples.\n",
    "        degree: integer.\n",
    "        \n",
    "    Returns:\n",
    "        poly: numpy array of shape (N,d+1)\n",
    "        \n",
    "    >>> build_poly(np.array([0.0, 1.5]), 2)\n",
    "    array([[1.  , 0.  , 0.  ],\n",
    "           [1.  , 1.5 , 2.25]])\n",
    "    \"\"\"\n",
    "    poly = x\n",
    "    \n",
    "    for i in range(2,degree+1):\n",
    "       \n",
    "        poly = np.c_[poly,x**i]\n",
    "    return poly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a6b46e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_data_poly = build_poly(std_data, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bfe1544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 60) (250000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(std_data_poly.shape,std_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703124d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "\n",
    "gamma = 0.001\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.rand(60)\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "std_data_poly = build_poly(std_data, 2)\n",
    "gd_losses, gd_ws = gradient_descent(y, std_data_poly, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a4b798",
   "metadata": {},
   "source": [
    "# Subset test (from train set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "30f3824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing. If ratio times the number of samples is not round\n",
    "    you can use np.floor. Also check the documentation for np.random.permutation,\n",
    "    it could be useful.\n",
    "    \n",
    "    Args:\n",
    "        x: numpy array of shape (N,), N is the number of samples.\n",
    "        y: numpy array of shape (N,).\n",
    "        ratio: scalar in [0,1]\n",
    "        seed: integer.\n",
    "        \n",
    "    Returns:\n",
    "        x_tr: numpy array containing the train data.\n",
    "        x_te: numpy array containing the test data.\n",
    "        y_tr: numpy array containing the train labels.\n",
    "        y_te: numpy array containing the test labels.\n",
    "        \n",
    "    >>> split_data(np.arange(13), np.arange(13), 0.8, 1)\n",
    "    (array([ 2,  3,  4, 10,  1,  6,  0,  7, 12,  9]), array([ 8, 11,  5]), array([ 2,  3,  4, 10,  1,  6,  0,  7, 12,  9]), array([ 8, 11,  5]))\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data based on the given ratio: TODO\n",
    "    l = int(np.floor(len(y)*ratio))\n",
    "    y = np.random.permutation(y)\n",
    "    np.random.seed(seed)\n",
    "    x = np.random.permutation(x)\n",
    "    \n",
    "    return (x[0:l],x[l:],y[0:l],y[l:])\n",
    "    \n",
    "    \n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8a423281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_demo(x, y, degree, ratio,w_initial,max_iters,gamma, seed):\n",
    "    \"\"\"polynomial regression with different split ratios and different degrees.\n",
    "    \n",
    "    Returns:\n",
    "      x_tr: numpy array\n",
    "      x_te: numpy array\n",
    "      y_tr: numpy array\n",
    "      y_te: numpy array\n",
    "      weights: weights from the least squares optimization\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data, and return train and test data: TODO\n",
    "    x_tr,x_te,y_tr,y_te=split_data(x, y, ratio, seed)\n",
    "    # ***************************************************\n",
    " \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    poly_tr = build_poly(x_tr, degree)\n",
    "    poly_te = build_poly(x_te, degree)\n",
    "    print(poly_tr.shape,poly_te.shape)\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # calculate weight through least square: TODO\n",
    "    gd_losses, gd_ws = gradient_descent(y_tr, poly_tr, w_initial, max_iters, gamma)\n",
    "    w_star = gd_ws[-1]\n",
    "    mse = gd_losses[-1]\n",
    "    # ***************************************************\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # calculate RMSE for train and test data,\n",
    "    # and store them in rmse_tr and rmse_te respectively: TODO\n",
    "    rmse_tr = np.sqrt(2*mse)\n",
    "    rmse_te = np.sqrt(2*compute_loss(y_te,poly_te,w_star))\n",
    "\n",
    "    print(\"proportion={p}, degree={d}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "          p=ratio, d=degree, tr=rmse_tr, te=rmse_te))\n",
    "    \n",
    "    return x_tr,x_te,y_tr,y_te,w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b198c6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 30) (50000, 30)\n",
      "GD iter. 0/99: loss=16.376663430489458, w0=0.1883951481842507, w1=0.7850225752986119, grad=[ 1.49652101 -0.24022089  1.1307976   4.01383457  4.47145343  4.26042395\n",
      "  4.47300376 -1.43849211  2.24083611  4.84138264  1.04387497  2.85428882\n",
      "  4.47205951  1.93591183  0.7743556   0.90528196  2.43795121  0.50687157\n",
      " -0.11160217  2.40409543  0.13464384  4.50056777  5.06362249  4.41786106\n",
      "  4.25409642  4.25332065  4.50228515  4.47231234  4.47201256  4.72369981]\n",
      "GD iter. 1/99: loss=2.860720360906129, w0=0.19344985537235632, w1=0.6733962421342872, grad=[-0.05054707  1.11626333  1.07960194 -0.90467366 -1.28734089 -1.32066359\n",
      " -1.28564511  1.18589234 -0.1592137  -0.75734927  0.50320399 -0.55144487\n",
      " -1.2868821  -0.11144872  0.63595906  0.77981566  0.4547628   0.32466196\n",
      " -0.07814995 -0.51637814  0.07373769 -0.75627891 -0.98588378 -0.57687657\n",
      " -0.51957307 -0.52027384 -1.28794168 -1.28664372 -1.28686759 -0.97093113]\n",
      "GD iter. 2/99: loss=1.7149898922386475, w0=0.1692999219034902, w1=0.6087706362580133, grad=[ 0.24149933  0.64625606  0.81705465  0.17098702  0.08743576  0.01586297\n",
      "  0.08890334  0.45268363  0.33189821  0.43232392  0.43686306  0.22432921\n",
      "  0.08786373  0.30876977  0.5609267   0.70829773  0.63921165  0.27423456\n",
      " -0.05357029  0.12215304  0.07513808  0.37037204  0.3778894   0.47876983\n",
      "  0.48542682  0.48480612  0.09248375  0.08806677  0.08786252  0.2958526 ]\n",
      "GD iter. 3/99: loss=1.355910840359806, w0=0.1565336898887682, w1=0.5463585456100392, grad=[ 0.12766232  0.62412091  0.65780549 -0.10210102 -0.17567498 -0.23617843\n",
      " -0.17431103  0.50666755  0.17607964  0.09896709  0.30853743  0.04885806\n",
      " -0.17529041  0.17643619  0.4873233   0.63506434  0.39325164  0.20944516\n",
      " -0.03266549 -0.03967777  0.06367957  0.06238065  0.05643118  0.17432224\n",
      "  0.19161275  0.19105901 -0.17298725 -0.1751112  -0.17528803 -0.01204171]\n",
      "GD iter. 4/99: loss=1.1320711793987681, w0=0.1449213653567206, w1=0.493816072479914, grad=[ 0.11612325  0.52542473  0.51893715 -0.05803424 -0.07820118 -0.13877495\n",
      " -0.07693846  0.40113554  0.17554787  0.12769156  0.22949041  0.08939357\n",
      " -0.07784648  0.17655567  0.42677728  0.57162008  0.29337304  0.1609915\n",
      " -0.01572971 -0.0132643   0.05634789  0.09353271  0.11897546  0.19169928\n",
      "  0.20537942  0.20488528 -0.0757413  -0.07768818 -0.07784363  0.04432411]\n",
      "GD iter. 5/99: loss=0.9769003540634397, w0=0.13598400897337976, w1=0.447428831738819, grad=[ 0.08937356  0.46387241  0.41032431 -0.0785966  -0.07177002 -0.12989351\n",
      " -0.07058519  0.34795596  0.14605839  0.08598969  0.16617557  0.08049807\n",
      " -0.07144006  0.15194605  0.37457365  0.51432609  0.19740557  0.11976464\n",
      " -0.00194484 -0.02540249  0.04917391  0.05729935  0.09501333  0.14582326\n",
      "  0.15896677  0.15852492 -0.06979581 -0.07129882 -0.07143517  0.02232667]\n",
      "GD iter. 6/99: loss=0.8661154836276715, w0=0.12872980524454122, w1=0.40649664366233035, grad=[ 0.07254204  0.40932188  0.32261461 -0.07913921 -0.05150906 -0.10782582\n",
      " -0.0503888   0.29611832  0.12813319  0.06931645  0.11987391  0.08254714\n",
      " -0.05119869  0.13725787  0.33003915  0.46310284  0.12913255  0.08605118\n",
      "  0.00913964 -0.02591782  0.04295781  0.04373988  0.09155503  0.12223365\n",
      "  0.13407467  0.13367897 -0.04979715 -0.05107169 -0.05119154  0.02137209]\n",
      "GD iter. 7/99: loss=0.785114650879601, w0=0.12286868683842735, w1=0.3699646195617942, grad=[ 0.05861118  0.36532024  0.25230598 -0.0805126  -0.03961858 -0.09408815\n",
      " -0.03855134  0.25488841  0.11150753  0.05375609  0.08540927  0.08182564\n",
      " -0.0393241   0.12375519  0.29185512  0.4171631   0.07654573  0.05828868\n",
      "  0.01796142 -0.02720558  0.03742065  0.0305616   0.08424539  0.0998812\n",
      "  0.11059372  0.11023879 -0.03810125 -0.03920894 -0.03931437  0.01792894]\n",
      "GD iter. 8/99: loss=0.7244946363093309, w0=0.11805707798331691, w1=0.3371143100236, grad=[ 0.04811609  0.3285031   0.19573168 -0.07914792 -0.02979569 -0.08255312\n",
      " -0.02877266  0.22023495  0.09771377  0.04306838  0.06020266  0.08146996\n",
      " -0.02951418  0.11275268  0.25908033  0.37597037  0.03728133  0.03557642\n",
      "  0.02487975 -0.02713157  0.03253527  0.02146118  0.07858394  0.08229525\n",
      "  0.09188322  0.09156437 -0.02840392 -0.0294089  -0.02950173  0.01666713]\n",
      "GD iter. 9/99: loss=0.6780910207931482, w0=0.1140495957734007, w1=0.3073489305730942, grad=[ 0.04007482  0.29765379  0.15018278 -0.07671097 -0.02251979 -0.07366943\n",
      " -0.02153396  0.19149035  0.08578864  0.03512297  0.04195505  0.08078607\n",
      " -0.02224899  0.10342181  0.23088201  0.3390105   0.0079452   0.01706557\n",
      "  0.03020287 -0.0266504   0.02822409  0.01455877  0.07326978  0.06768049\n",
      "  0.07621378  0.07592691 -0.02120996 -0.02215199 -0.02223378  0.01602531]\n",
      "GD iter. 10/99: loss=0.6418022604417392, w0=0.11064801336258435, w1=0.280197922752651, grad=[ 0.03401582  0.27151008  0.1134598  -0.07344047 -0.0169565  -0.06661161\n",
      " -0.01600234  0.16750287  0.07547791  0.02943658  0.02897503  0.0799523\n",
      " -0.01669467  0.09553188  0.20656637  0.30583442 -0.01372398  0.00206526\n",
      "  0.03419225 -0.02586101  0.02442792  0.0094962   0.06854299  0.05565955\n",
      "  0.06320869  0.06295023 -0.01569754 -0.0166046  -0.0166767   0.01599478]\n",
      "GD iter. 11/99: loss=0.6128560733877134, w0=0.1076992792850561, w1=0.2552818429580727, grad=[ 0.02948734  0.2491608   0.08382262 -0.06972435 -0.01274414 -0.06101306\n",
      " -0.01181723  0.14746256  0.06647314  0.02538568  0.01995044  0.07894928\n",
      " -0.0124899   0.08880222  0.18554593  0.27604046 -0.02955089 -0.0100088\n",
      "  0.03707041 -0.02494887  0.02109021  0.00576165  0.06429155  0.04569483\n",
      "  0.05233776  0.05210458 -0.01151544 -0.01240566 -0.01246923  0.01631445]\n",
      "GD iter. 12/99: loss=0.5893469636919125, w0=0.10508484661461398, w1=0.23229409839736334, grad=[ 0.02614433  0.22987745  0.05988202 -0.06578728 -0.00954584 -0.05653213\n",
      " -0.00864265  0.13067261  0.05855935  0.0225571   0.01388151  0.07780285\n",
      " -0.00929812  0.08303064  0.16732523  0.24927129 -0.04092303 -0.01964557\n",
      "  0.03902653 -0.02400436  0.01816031  0.00302419  0.06048437  0.03741244\n",
      "  0.04322748  0.04301684 -0.00833403 -0.0092188  -0.00927485  0.01685512]\n",
      "GD iter. 13/99: loss=0.5699420620815308, w0=0.1027140292817628, w1=0.21098485331107003, grad=[ 0.02370817  0.21309245  0.04052983 -0.06179602 -0.00711974 -0.05292035\n",
      " -0.00623743  0.11656756  0.05156181  0.02062561  0.01000433  0.07652837\n",
      " -0.0068777   0.07804972  0.15148578  0.22520841 -0.04891288 -0.0272553\n",
      "  0.04022136 -0.02308755  0.01559241  0.00102239  0.05707437  0.03050137\n",
      "  0.03556517  0.03537463 -0.00591665 -0.00680253 -0.00685195  0.01751259]\n",
      "GD iter. 14/99: loss=0.5536921901149149, w0=0.10051814142106362, w1=0.19114904671597652, grad=[ 0.02195888  0.19835807  0.02488025 -0.05786108 -0.00527893 -0.04998371\n",
      " -0.00441518  0.1046812   0.04534344  0.01934924  0.00773752  0.07514307\n",
      " -0.0050419   0.07372549  0.1376741   0.20356791 -0.05434438 -0.03318186\n",
      "  0.04079144 -0.02223024  0.01334536 -0.00043721  0.05402147  0.02471517\n",
      "  0.02910081  0.02892826 -0.00408002 -0.00497024 -0.00501379  0.01821734]\n",
      "GD iter. 15/99: loss=0.5399091859471352, w0=0.09844586051987986, w1=0.17261701083432163, grad=[ 0.02072281  0.18532036  0.01222421 -0.05405534 -0.00388217 -0.04757368\n",
      " -0.00303511  0.09463082  0.03979426  0.01854553  0.0066397   0.07366296\n",
      " -0.00364961  0.06994905  0.12559146  0.18409661 -0.05785069 -0.03771348\n",
      "  0.04085256 -0.02144739  0.01138223 -0.00150012  0.05128785  0.01985501\n",
      "  0.02363105  0.0234746  -0.00268545 -0.00358095 -0.00361928  0.01892142]\n",
      "GD iter. 16/99: loss=0.5280852969271639, w0=0.09645946494164598, w1=0.1552472147356974, grad=[ 0.01986396  0.17369796  0.00199322 -0.05042395 -0.00282209 -0.04557569\n",
      " -0.00199015  0.08610178  0.03482545  0.01807808  0.00637708  0.07210332\n",
      " -0.00259357  0.06663172  0.1149853   0.16656861 -0.05991744 -0.04109163\n",
      "  0.04050283 -0.02074282  0.00967003 -0.00227486  0.04883918  0.01576053\n",
      "  0.01899049  0.01884847 -0.00162726 -0.00252746 -0.00256116  0.01959339]\n",
      "GD iter. 17/99: loss=0.5178395219890258, w0=0.09453181717832013, w1=0.1389206138492521, grad=[ 0.01927648  0.16326601 -0.00626931 -0.04699247 -0.00201722 -0.0439014\n",
      " -0.00119913  0.07883537  0.03036435  0.01784512  0.00669826  0.07047846\n",
      " -0.00179241  0.0637009   0.10564199  0.15078232 -0.06091643 -0.04351852\n",
      "  0.03982516 -0.02011367  0.0081793  -0.0028422   0.04664435  0.01230179\n",
      "  0.01504394  0.0149149  -0.00082509 -0.00172847 -0.00175805  0.02021344]\n",
      "GD iter. 18/99: loss=0.5088813936921697, w0=0.09264394902141296, w1=0.12353624261179291, grad=[ 0.01887868  0.15384371 -0.01293075 -0.04377272 -0.00140576 -0.04248253\n",
      " -0.00060044  0.07261875  0.02635086  0.01777122  0.007415    0.06880166\n",
      " -0.00118437  0.06109682  0.09738073  0.13655773 -0.06113169 -0.0451634\n",
      "  0.03888954 -0.01955333  0.00688379 -0.00326196  0.04467536  0.00937307\n",
      "  0.01168055  0.01156317 -0.00021781 -0.0011223  -0.0011482   0.0207701 ]\n",
      "GD iter. 19/99: loss=0.5009861711437571, w0=0.09078313798567683, w1=0.1090077641323231, grad=[ 0.01860811  0.14528478 -0.01828724 -0.04076706 -0.00094083 -0.0412662\n",
      " -0.00014737  0.06727668  0.02273466  0.01780078  0.00838723  0.06708518\n",
      " -0.00072262  0.05877014  0.09004838  0.12373409 -0.06077955 -0.04616785\n",
      "  0.03775483 -0.0190534   0.00576016 -0.00357827  0.04290712  0.00688803\n",
      "  0.00880897  0.0087021   0.00024106 -0.00066216 -0.00068478  0.02125773]\n",
      "GD iter. 20/99: loss=0.4939775893531346, w0=0.08894138180395161, w1=0.09526075923406449, grad=[ 0.01841756  0.13747005 -0.02257797 -0.03797148 -0.00058689 -0.04021135\n",
      "  0.00019549  0.06266474  0.01947313  0.01789316  0.00951166  0.06534016\n",
      " -0.00037167  0.05667986  0.08351518  0.11216774 -0.06002427 -0.0466502\n",
      "  0.03647044 -0.01860493  0.00478766 -0.00382339  0.04131722  0.00477586\n",
      "  0.00635371  0.0062563   0.00058691 -0.0003126  -0.00033228  0.02167473]\n",
      "GD iter. 21/99: loss=0.48771567743330607, w0=0.08711419481402703, w1=0.08223058742132229, grad=[ 1.82718699e-02  1.30301718e-01 -2.59965410e-02 -3.53778930e-02\n",
      " -3.16992296e-04 -3.92860111e-02  4.54951572e-04  5.86638039e-02\n",
      "  1.65297400e-02  1.80190057e-02  1.07129041e-02  6.35767177e-02\n",
      " -1.04598252e-04  5.47918390e-02  7.76710659e-02  1.01730275e-01\n",
      " -5.89899905e-02 -4.67092935e-02  3.50776392e-02 -1.81991991e-02\n",
      "  3.94791636e-03 -4.02079675e-03  3.98856990e-02  2.97822193e-03\n",
      "  4.25204945e-03  4.16318935e-03  8.46621703e-04 -4.67363315e-05\n",
      " -6.37865412e-05  2.20221623e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 22/99: loss=0.4820880235760548, w0=0.08529966370255496, w1=0.06986069125210408, grad=[ 1.81453111e-02  1.23698962e-01 -2.86999738e-02 -3.29757017e-02\n",
      " -1.10734798e-04 -3.84652235e-02  6.51347237e-04  5.51754484e-02\n",
      "  1.38728602e-02  1.81573655e-02  1.19367439e-02  6.18039462e-02\n",
      "  9.89695470e-05  5.30774807e-02  7.24225989e-02  9.23068871e-02\n",
      " -5.77700771e-02 -4.64275657e-02  3.36107867e-02 -1.78281959e-02\n",
      "  3.22461806e-03 -4.18735547e-03  3.85948159e-02  1.44685816e-03\n",
      "  2.45172760e-03  2.37059144e-03  1.04067951e-03  1.55776432e-04\n",
      "  1.41091185e-04  2.23028741e-02]\n",
      "GD iter. 23/99: loss=0.47700341719031214, w0=0.08349771152620233, w1=0.05810124529722979, grad=[ 1.80195218e-02  1.17594460e-01 -3.08159280e-02 -3.07529677e-02\n",
      "  4.73388273e-05 -3.77294169e-02  8.00053910e-04  5.21182787e-02\n",
      "  1.14748416e-02  1.82936016e-02  1.31449169e-02  6.00299429e-02\n",
      "  2.54471662e-04  5.15127771e-02  6.76903966e-02  8.37948628e-02\n",
      " -5.64342707e-02 -4.58737212e-02  3.20983228e-02 -1.74848108e-02\n",
      "  2.60334081e-03 -4.33503849e-03  3.74288158e-02  1.41679654e-04\n",
      "  9.08983602e-04  8.34834380e-04  1.18465589e-03  3.10353208e-04\n",
      "  2.97794469e-04  2.25207696e-02]\n",
      "GD iter. 24/99: loss=0.47238715436448, w0=0.08170952838461683, w1=0.04690807364373324, grad=[ 0.01788183  0.11193172 -0.03244842 -0.02869719  0.00016893 -0.03706319\n",
      "  0.00091271  0.04942486  0.0093113   0.01841775  0.01431114  0.05826188\n",
      "  0.00037359  0.05007749  0.06340693  0.07610229 -0.05503424 -0.04510493\n",
      "  0.03056365 -0.01716293  0.00207133 -0.0044722   0.03637373 -0.00097076\n",
      " -0.00041296 -0.00048078  0.00129041  0.00042866  0.00041801  0.02268035]\n",
      "GD iter. 25/99: loss=0.4681775241465377, w0=0.07993713492617954, w1=0.036241777348455864, grad=[ 0.01772393  0.10666296 -0.03368241 -0.02679583  0.00026289 -0.03645437\n",
      "  0.00099812  0.04703922  0.00736057  0.01852331  0.01541805  0.05650604\n",
      "  0.00046517  0.04875449  0.0595147   0.06914686 -0.05360784 -0.04416867\n",
      "  0.02902588 -0.01685744  0.00161731 -0.00460452  0.03541717 -0.0019185\n",
      " -0.00154512 -0.00160721  0.00136696  0.00051951  0.0005106   0.02278636]\n",
      "GD iter. 26/99: loss=0.464323145142174, w0=0.07818305128700834, w1=0.026067026625778908, grad=[ 0.01754084  0.10174751 -0.03458746 -0.02503669  0.00033591 -0.03589329\n",
      "  0.00106293  0.04491484  0.00560327  0.0186063   0.01645493  0.05476791\n",
      "  0.0005359   0.04752927  0.05596467  0.06285489 -0.05218243 -0.04310432\n",
      "  0.02750047 -0.01656415  0.00123134 -0.00473574  0.03454816 -0.0027251\n",
      " -0.00251379 -0.00257068  0.00142119  0.00058959  0.00058224  0.02284361]\n",
      "GD iter. 27/99: loss=0.46078092294453954, w0=0.07645004932896682, w1=0.016351982815312527, grad=[ 0.01733002  0.09715044 -0.03522068 -0.02340815  0.00039307 -0.03537222\n",
      "  0.00111218  0.04301294  0.004022    0.01866459  0.01741585  0.05305221\n",
      "  0.00059082  0.04638947  0.05271496  0.05716031 -0.05077739 -0.04194446\n",
      "  0.02599981 -0.01627969  0.00090464 -0.00486819  0.03375699 -0.00341049\n",
      " -0.0033413  -0.00339347  0.00145834  0.00064394  0.00063799  0.02285675]\n",
      "GD iter. 28/99: loss=0.457514466843054, w0=0.07474097034886353, w1=0.007067823052814351, grad=[ 0.01709079  0.0928416  -0.03562905 -0.02189924  0.00043819 -0.03488495\n",
      "  0.00114966  0.04130114  0.00260102  0.01869736  0.01829844  0.05136297\n",
      "  0.00063377  0.04532456  0.04972975  0.05200394 -0.04940612 -0.04071595\n",
      "  0.02453366 -0.01600143  0.00062949 -0.0050032   0.03303505 -0.00399154\n",
      " -0.00404671 -0.00409459  0.00148241  0.00068636  0.00068169  0.02283023]\n",
      "GD iter. 29/99: loss=0.45449285052067806, w0=0.07305859394395908, w1=-0.0018116533802478556, grad=[ 0.01682376  0.08879476 -0.03585135 -0.02049978  0.00047415 -0.03442647\n",
      "  0.00117823  0.03975235  0.00132608  0.01870471  0.01910279  0.0497036\n",
      "  0.00066762  0.04432551  0.04697837  0.04733266 -0.04807745 -0.03944088\n",
      "  0.02310957 -0.01572739  0.0003991  -0.00514138  0.03237471 -0.00448264\n",
      " -0.00464631 -0.00469029  0.00149646  0.00071974  0.00071622  0.02276825]\n",
      "GD iter. 30/99: loss=0.4516896333615397, w0=0.07140554657710696, w1=-0.010310353398449837, grad=[ 0.01653047  0.084987   -0.03591964 -0.01920036  0.00050315 -0.03399269\n",
      "  0.00120006  0.03834382  0.00018422  0.01868741  0.01983072  0.04807693\n",
      "  0.00069456  0.04338462  0.04443447  0.04309884 -0.04679688 -0.03813736\n",
      "  0.02173325 -0.01545608  0.00020747 -0.00528285  0.0317692  -0.00489604\n",
      " -0.00515411 -0.00519455  0.0015028   0.00074624  0.00074377  0.02267469]\n",
      "GD iter. 31/99: loss=0.44908208129452104, w0=0.06978424070440237, w1=-0.018450165931226656, grad=[ 1.62130587e-02  8.13981253e-02 -3.58605380e-02 -1.79923415e-02\n",
      "  5.26827255e-04 -3.35802645e-02  1.21677084e-03  3.70564426e-02\n",
      " -8.36361570e-04  1.86466082e-02  2.04852203e-02  4.64852784e-02\n",
      "  7.16228905e-04  4.24952439e-02  4.20753859e-02  3.92597052e-02\n",
      " -4.55674351e-02 -3.68201439e-02  2.04088444e-02 -1.51864662e-02\n",
      "  4.93604573e-05 -5.42740150e-03  3.12125265e-02 -5.24225341e-03\n",
      " -5.58216419e-03 -5.61937570e-03  1.50324229e-03  7.67509147e-04\n",
      "  7.65993895e-04  2.25531218e-02]\n",
      "GD iter. 32/99: loss=0.44665054189151543, w0=0.06819683720577764, w1=-0.026251195240468175, grad=[ 1.58740350e-02  7.80102931e-02 -3.56961438e-02 -1.68678321e-02\n",
      "  5.46435973e-04 -3.31864371e-02  1.22959819e-03  3.58740671e-02\n",
      " -1.74642401e-03  1.85837507e-02  2.10700145e-02  4.49304949e-02\n",
      "  7.33869268e-04  4.16516758e-02  3.98815842e-02  3.57768536e-02\n",
      " -4.43902955e-02 -3.55012047e-02  1.91392163e-02 -1.49178588e-02\n",
      " -7.98434618e-05 -5.57460936e-03  3.06993809e-02 -5.53031517e-03\n",
      " -5.94087166e-03 -5.97514593e-03  1.49914222e-03  7.84777032e-04\n",
      "  7.84127603e-04  2.24068150e-02]\n",
      "GD iter. 33/99: loss=0.4443779397664275, w0=0.06664522537481526, w1=-0.033731959623418115, grad=[ 0.01551612  0.07480764 -0.0354449  -0.01581964  0.00056291 -0.03280891\n",
      "  0.00123946  0.03478305 -0.00255581  0.01850042  0.02158925  0.04341403\n",
      "  0.00074842  0.04084899  0.03783618  0.03261577 -0.04326536 -0.03419018\n",
      "  0.01792615 -0.01464985 -0.00018418 -0.00572393  0.03022504 -0.005768\n",
      " -0.00623924 -0.00627084  0.00149155  0.00079898  0.00079912  0.02223874]\n",
      "GD iter. 34/99: loss=0.4422493665469817, w0=0.06513101594455092, w1=-0.040909561734618155, grad=[ 0.01514209  0.07177602 -0.0351222  -0.01484124  0.00057697 -0.03244576\n",
      "  0.00124706  0.0337718  -0.00327351  0.01839827  0.02204729  0.04193696\n",
      "  0.00076058  0.04008294  0.03592454  0.02974544 -0.0421916  -0.03289477\n",
      "  0.01677053 -0.01438224 -0.00026719 -0.00587474  0.02978533 -0.00596203\n",
      " -0.00648508 -0.00651425  0.00148129  0.00081082  0.00081167  0.0220516 ]\n",
      "GD iter. 35/99: loss=0.4402517457362911, w0=0.06365554360599592, w1=-0.04779983547828982, grad=[ 0.01475472  0.06890274 -0.03474095 -0.01392671  0.00058914 -0.03209538\n",
      "  0.00125292  0.03283048 -0.00390777  0.01827897  0.02244853  0.04050004\n",
      "  0.0007709   0.03934984  0.03413394  0.02713794 -0.04116738 -0.03162105\n",
      "  0.01567255 -0.01411501 -0.00033197 -0.0060264   0.02937651 -0.00611822\n",
      " -0.00668519 -0.00671213  0.00146897  0.00082082  0.00082233  0.02184783]\n",
      "GD iter. 36/99: loss=0.438373557273125, w0=0.06221987626059883, w1=-0.05441747342489416, grad=[ 0.01435667  0.06617638 -0.03431197 -0.01307069  0.00059982 -0.03175638\n",
      "  0.00125744  0.03195068 -0.00446611  0.01814414  0.02279728  0.03910375\n",
      "  0.00077976  0.03864649  0.03245331  0.02476819 -0.04019064 -0.03037376\n",
      "  0.01463177 -0.01384826 -0.00038121 -0.00617828  0.0289953  -0.00624161\n",
      " -0.00684548 -0.00687039  0.00145509  0.00082939  0.0008315   0.02162961]\n",
      "GD iter. 37/99: loss=0.436604609972677, w0=0.060824828873892366, w1=-0.06077613792705051, grad=[ 0.01395047  0.06358665 -0.03384437 -0.01226832  0.00060934 -0.0314276\n",
      "  0.0012609   0.03112519 -0.00495544  0.01799536  0.02309772  0.03774831\n",
      "  0.00078748  0.03797011  0.03087294  0.02261356 -0.03925908 -0.02915652\n",
      "  0.01364733 -0.01358216 -0.00041728 -0.00632977  0.02863876 -0.0063366\n",
      " -0.00697111 -0.00699417  0.00144001  0.00083683  0.00083948  0.02139892]\n",
      "GD iter. 38/99: loss=0.4349358525866369, w0=0.05947098029542237, w1=-0.06688855850026565, grad=[ 0.01353849  0.06112421 -0.03334583 -0.01151521  0.0006179  -0.03110803\n",
      "  0.00126353  0.03034784 -0.00538207  0.01783411  0.02335383  0.03643374\n",
      "  0.00079429  0.0373183   0.02938434  0.02065371 -0.03837028 -0.02797205\n",
      "  0.01271796 -0.01331698 -0.00044222 -0.0064803   0.02830427 -0.00640698\n",
      " -0.00706658 -0.00708796  0.00142402  0.00084337  0.00084652  0.02115754]\n",
      "GD iter. 39/99: loss=0.43335921617697315, w0=0.058158691804485525, w1=-0.07276661755454165, grad=[ 0.01312288  0.05878059 -0.03282281 -0.0108074   0.0006257  -0.03079679\n",
      "  0.00126551  0.02961331 -0.00575179  0.01766182  0.02356936  0.03515987\n",
      "  0.00080035  0.03668894  0.02798002  0.01887029 -0.03752178 -0.02682228\n",
      "  0.01184211 -0.01305299 -0.00045783 -0.00662934  0.02798951 -0.0064561\n",
      " -0.00713584 -0.00715568  0.00140737  0.00084917  0.00085278  0.02090704]\n",
      "GD iter. 40/99: loss=0.4318674820055381, w0=0.05688812645238156, w1=-0.07842142618435004, grad=[ 0.01270565  0.05654809 -0.03228079 -0.01014129  0.00063286 -0.03049313\n",
      "  0.00126695  0.02891701 -0.0060699   0.01747978  0.02374781  0.03392635\n",
      "  0.0008058   0.03608019  0.02665336  0.01724674 -0.03671114 -0.02570853\n",
      "  0.01101803 -0.01279049 -0.00046564 -0.00677641  0.02769243 -0.00648684\n",
      " -0.00718234 -0.00720077  0.00139022  0.00085437  0.00085839  0.02064886]\n",
      "GD iter. 41/99: loss=0.43045417031256367, w0=0.055659268516224825, w1=-0.0838633914240412, grad=[ 0.01228858  0.05441965 -0.03172441 -0.00951363  0.00063948 -0.03019639\n",
      "  0.00126795  0.02825495 -0.00634124  0.01728923  0.02389244  0.03273274\n",
      "  0.00081074  0.03549044  0.02539848  0.01576816 -0.03593599 -0.0246316\n",
      "  0.01024377 -0.01252981 -0.000467   -0.00692109  0.02741117 -0.00650175\n",
      " -0.00720911 -0.00722626  0.00137273  0.00085907  0.00086347  0.02038427]\n",
      "GD iter. 42/99: loss=0.4291134462725723, w0=0.05447194257017809, w1=-0.08910227613610443, grad=[ 0.01187326  0.05238885 -0.03115757 -0.00892149  0.00064565 -0.02990598\n",
      "  0.00126858  0.02762368 -0.00657025  0.01709128  0.02400626  0.03157843\n",
      "  0.00081525  0.03491825  0.02421017  0.01442106 -0.03519405 -0.02359189\n",
      "  0.00951731 -0.01227123 -0.00046307 -0.00706299  0.02714412 -0.00650304\n",
      " -0.00721879 -0.00723478  0.00135499  0.00086334  0.00086809  0.02011441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 43/99: loss=0.4278400401363229, w0=0.05332583182805849, w1=-0.09414725250741254, grad=[ 0.01146111  0.05044976 -0.03058361 -0.00836218  0.00065141 -0.0296214\n",
      "  0.0012689   0.02702019 -0.00676099  0.01688698  0.02409204  0.03046276\n",
      "  0.00081937  0.0343624   0.02308375  0.01319329 -0.03448316 -0.02258944\n",
      "  0.00883654 -0.01201507 -0.00045485 -0.00720179  0.0268898  -0.00649265\n",
      " -0.00721373 -0.00722865  0.0013371   0.00086724  0.00087231  0.0198403 ]\n",
      "GD iter. 44/99: loss=0.42662917913696696, w0=0.052220495525338896, w1=-0.09900694997338541, grad=[ 0.01105336  0.04859697 -0.03000534 -0.00783331  0.00065682 -0.0293422\n",
      "  0.00126895  0.02644185 -0.00691716  0.01667728  0.02415233  0.02938497\n",
      "  0.00082317  0.03382176  0.02201502  0.01207387 -0.03380127 -0.02162402\n",
      "  0.00819931 -0.01176159 -0.00044318 -0.00733719  0.02664691 -0.00647226\n",
      " -0.00719597 -0.00720991  0.00131914  0.0008708   0.00087616  0.01956286]\n",
      "GD iter. 45/99: loss=0.4254765291913984, w0=0.051155385195663344, w1=-0.10368949826393363, grad=[ 0.0106511   0.04682548 -0.02942514 -0.00733266  0.0006619  -0.02906798\n",
      "  0.00126875  0.02588636 -0.00704216  0.01646303  0.02418946  0.02834425\n",
      "  0.00082666  0.03329537  0.02100021  0.01105285 -0.03314646 -0.0206952\n",
      "  0.00760347 -0.01151107 -0.00042882 -0.00746895  0.02641431 -0.00644337\n",
      " -0.0071673  -0.00718035  0.00130115  0.00087407  0.0008797   0.01928291]\n",
      "GD iter. 46/99: loss=0.4243781447894757, w0=0.050129859762771865, w1=-0.10820256616195109, grad=[ 0.01025525  0.04513068 -0.02884504 -0.00685826  0.00066669 -0.02879838\n",
      "  0.00126833  0.0253517  -0.00713909  0.01624504  0.02420555  0.02733973\n",
      "  0.00082987  0.03278236  0.02003589  0.01012125 -0.03251697 -0.01980236\n",
      "  0.00704687 -0.01126372 -0.00041238 -0.00759687  0.02619097 -0.00640726\n",
      " -0.0071293  -0.00714154  0.00128318  0.00087707  0.00088295  0.01900117]\n",
      "GD iter. 47/99: loss=0.4233304257539531, w0=0.049143199417729336, w1=-0.11255339648026214, grad=[ 0.0098666   0.0435083  -0.02826674 -0.00640828  0.00067121 -0.02853309\n",
      "  0.0012677   0.02483609 -0.00721077  0.01602402  0.02420255  0.0263705\n",
      "  0.00083284  0.03228196  0.01911897  0.00927095 -0.03191111 -0.01894476\n",
      "  0.00652742 -0.01101977 -0.00039441 -0.00772076  0.02597595 -0.00636508\n",
      " -0.00708335 -0.00709486  0.00126528  0.00087981  0.00088592  0.01871829]\n",
      "GD iter. 48/99: loss=0.4223300797877282, w0=0.04819461828721921, w1=-0.11674883769259085, grad=[ 0.00948581  0.04195441 -0.02769169 -0.00598107  0.00067546 -0.02827183\n",
      "  0.00126688  0.02433798 -0.00725979  0.01580063  0.02418223  0.02543561\n",
      "  0.00083556  0.03179347  0.0182466   0.00849459 -0.03132737 -0.01812153\n",
      "  0.00604303 -0.0107794  -0.00037535 -0.0078405   0.02576845 -0.00631781\n",
      " -0.00703068 -0.00704151  0.00124746  0.00088232  0.00088864  0.01843487]\n",
      "GD iter. 49/99: loss=0.42137408991405567, w0=0.04728327592429794, w1=-0.12079537259543108, grad=[ 0.00911342  0.04046535 -0.02712108 -0.00557513  0.00067947 -0.02801434\n",
      "  0.00126589  0.02385596 -0.0072885   0.01557546  0.02414619  0.02453413\n",
      "  0.00083805  0.03131628  0.01741621  0.00778551 -0.03076431 -0.01733176\n",
      "  0.00559171 -0.01054277 -0.00035559 -0.00795596  0.02556773 -0.00626632\n",
      " -0.00697235 -0.00698257  0.00122975  0.0008846   0.00089111  0.01815141]\n",
      "GD iter. 50/99: loss=0.4204596860688151, w0=0.04640828767072852, w1=-0.12469914432839863, grad=[ 0.00874988  0.03903772 -0.02655595 -0.00518909  0.00068324 -0.0277604\n",
      "  0.00126471  0.02338882 -0.00729905  0.01534903  0.0240959   0.02366507\n",
      "  0.00084032  0.03084984  0.01662541  0.00713767 -0.03022062 -0.01657447\n",
      "  0.00517151 -0.01031002 -0.00033545 -0.00806707  0.02537315 -0.00621137\n",
      " -0.00690931 -0.00691896  0.00121218  0.00088667  0.00089336  0.01786838]\n",
      "GD iter. 51/99: loss=0.41958432022891384, w0=0.045568733951659286, w1=-0.12846598003898194, grad=[ 0.00839554  0.03766836 -0.02599714 -0.00482167  0.00068679 -0.0275098\n",
      "  0.00126337  0.02293547 -0.0072934   0.01512185  0.0240327   0.02282747\n",
      "  0.00084238  0.03039364  0.01587201  0.00654559 -0.02969509 -0.01584862\n",
      "  0.00478057 -0.01008128 -0.00031519 -0.00817376  0.02518412 -0.00615361\n",
      " -0.00684236 -0.0068515   0.00119474  0.00088852  0.00089537  0.0175862 ]\n",
      "GD iter. 52/99: loss=0.4187456445630787, w0=0.04476366857044272, w1=-0.13210141244216655, grad=[ 0.00805065  0.03635432 -0.02544535 -0.00447174  0.0006901  -0.02726237\n",
      "  0.00126187  0.02249493 -0.00727334  0.01489433  0.0239578   0.02202033\n",
      "  0.00084424  0.02994722  0.01515399  0.00600431 -0.02918662 -0.01515319\n",
      "  0.00441708 -0.00985664 -0.00029504 -0.008276    0.02500013 -0.00609362\n",
      " -0.00677224 -0.00678091  0.00117747  0.00089018  0.00089718  0.01730523]\n",
      "GD iter. 53/99: loss=0.41794149217503423, w0=0.043992126074943516, w1=-0.1356106994951045, grad=[ 0.00771542  0.03509287 -0.02490116 -0.00413823  0.00069321 -0.02701794\n",
      "  0.0012602   0.02206636 -0.00724047  0.01466687  0.02387228  0.02124271\n",
      "  0.00084589  0.02951016  0.01446947  0.00550933 -0.02869417 -0.01448712\n",
      "  0.00407932 -0.00963617 -0.00027516 -0.00837375  0.02482072 -0.00603191\n",
      " -0.00669957 -0.00670782  0.00116036  0.00089163  0.00089877  0.01702579]\n",
      "GD iter. 54/99: loss=0.41716986007799506, w0=0.043253128267669486, w1=-0.13899884238093532, grad=[ 0.00738998  0.03388143 -0.02436504 -0.00382017  0.0006961  -0.02677637\n",
      "  0.00125838  0.02164898 -0.00719629  0.01443982  0.02377717  0.02049362\n",
      "  0.00084735  0.0290821   0.01381671  0.00505655 -0.0282168  -0.01384935\n",
      "  0.00376567 -0.00941994 -0.00025572 -0.00846701  0.02464546 -0.0059689\n",
      " -0.00662491 -0.00663277  0.00114342  0.00089289  0.00090015  0.01674819]\n",
      "GD iter. 55/99: loss=0.41642889409628525, w0=0.042545689931171644, w1=-0.14227060197334818, grad=[ 0.00707438  0.0327176  -0.02383737 -0.00351668  0.00069877 -0.02653751\n",
      "  0.0012564   0.02124209 -0.00714213  0.0142135   0.02367336  0.01977211\n",
      "  0.00084861  0.02866269  0.01319408  0.00464226 -0.02775365 -0.01323884\n",
      "  0.00347456 -0.00920798 -0.00023682 -0.00855579  0.02447401 -0.00590498\n",
      " -0.00654873 -0.00655624  0.00112667  0.00089395  0.00090134  0.01647266]\n",
      "GD iter. 56/99: loss=0.41571687543691993, w0=0.041868823837952054, w1=-0.14543051393392717, grad=[ 0.00676866  0.03159912 -0.02331846 -0.00322693  0.00070124 -0.02630126\n",
      "  0.00125426  0.0208451  -0.00707921  0.0139882   0.02356169  0.01907726\n",
      "  0.00084968  0.02825159  0.01260006  0.00426309 -0.02730391 -0.01265455\n",
      "  0.00320452 -0.00900033 -0.00021855 -0.00864011  0.02430603 -0.00584047\n",
      " -0.00647146 -0.00647864  0.00111009  0.00089482  0.00090232  0.01619945]\n",
      "GD iter. 57/99: loss=0.41503220871313057, w0=0.04122154511100522, w1=-0.14848290257729108, grad=[ 0.00647279  0.03052389 -0.02280852 -0.00295016  0.00070351 -0.0260675\n",
      "  0.00125197  0.02045743 -0.00700864  0.01376415  0.02344291  0.01840814\n",
      "  0.00085056  0.02784852  0.01203323  0.00391596 -0.02686686 -0.01209547\n",
      "  0.00295415 -0.00879699 -0.00020099 -0.00872001  0.02414123 -0.00577565\n",
      " -0.00639346 -0.00640035  0.00109369  0.00089551  0.0009031   0.01592874]\n",
      "GD iter. 58/99: loss=0.4143734112343116, w0=0.040602874997428595, w1=-0.15143189362413922, grad=[ 0.0061867   0.02948991 -0.02230773 -0.00268568  0.00070557 -0.02583614\n",
      "  0.00124952  0.0200786  -0.00693142  0.01354158  0.0233177   0.01776384\n",
      "  0.00085125  0.02745321  0.01149226  0.0035981  -0.02644182 -0.01156059\n",
      "  0.00272212 -0.00859796 -0.00018417 -0.00879551  0.02397935 -0.00571076\n",
      " -0.00631505 -0.00632167  0.00107748  0.000896    0.0009037   0.01566071]\n",
      "GD iter. 59/99: loss=0.41373910340407377, w0=0.04001184411351009, w1=-0.15428142594924585, grad=[ 0.00591031  0.02849532 -0.02181621 -0.00243282  0.00070743 -0.02560709\n",
      "  0.00124692  0.01970816 -0.00684846  0.01332071  0.02318669  0.01714348\n",
      "  0.00085176  0.02706539  0.01097588  0.00330695 -0.02602816 -0.01104895\n",
      "  0.00250719 -0.00840325 -0.00016814 -0.00886668  0.02382016 -0.00564601\n",
      " -0.0062365  -0.00624288  0.00106145  0.00089631  0.0009041   0.01539552]\n",
      "GD iter. 60/99: loss=0.4131280000908545, w0=0.03944749521551944, w1=-0.15703526241993956, grad=[ 0.00564349  0.02753836 -0.02133403 -0.002191    0.0007091  -0.02538027\n",
      "  0.00124417  0.01934569 -0.00676058  0.01310169  0.02305046  0.01654621\n",
      "  0.00085208  0.02668483  0.01048292  0.0030402  -0.02562531 -0.01055961\n",
      "  0.0023082  -0.00821281 -0.00015292 -0.00893357  0.02366345 -0.00558157\n",
      " -0.00615804 -0.00616421  0.00104561  0.00089644  0.0009043   0.01513331]\n",
      "GD iter. 61/99: loss=0.4125389028546667, w0=0.03890888554623054, w1=-0.15969699991045996, grad=[ 0.0053861   0.02661737 -0.02086124 -0.00195965  0.00071056 -0.02515561\n",
      "  0.00124126  0.01899085 -0.00666854  0.01288468  0.02290951  0.01597119\n",
      "  0.00085222  0.0263113   0.01001225  0.00279577 -0.02523274 -0.01009163\n",
      "  0.00212403 -0.00802664 -0.00013851 -0.00899625  0.02350905 -0.00551761\n",
      " -0.00607989 -0.00608586  0.00102994  0.00089639  0.00090433  0.01487417]\n",
      "GD iter. 62/99: loss=0.41197069292968763, w0=0.038395088803079735, w1=-0.1622700785686086, grad=[ 0.00513797  0.02573079 -0.02039783 -0.00173825  0.00071183 -0.02493304\n",
      "  0.00123821  0.01864329 -0.006573    0.01266983  0.02276433  0.0154176\n",
      "  0.00085217  0.0259446   0.00956284  0.00257172 -0.02484997 -0.00964414\n",
      "  0.00195365 -0.00784468 -0.00012492 -0.00905477  0.02335678 -0.00545423\n",
      " -0.00600222 -0.00600801  0.00101446  0.00089615  0.00090416  0.01461822]\n",
      "GD iter. 63/99: loss=0.4114223248760075, w0=0.03790519676988826, w1=-0.16475779040316482, grad=[ 0.00489892  0.02487712 -0.01994378 -0.00152632  0.0007129  -0.0247125\n",
      "  0.001235    0.01830271 -0.00647458  0.01245725  0.02261535  0.01488464\n",
      "  0.00085194  0.02558452  0.00913367  0.00236631 -0.02447653 -0.00921627\n",
      "  0.00179609 -0.00766689 -0.00011214 -0.00910921  0.02320651 -0.00539155\n",
      " -0.00592517 -0.00593079  0.00099915  0.00089573  0.00090381  0.01436554]\n",
      "GD iter. 64/99: loss=0.410892820825401, w0=0.037438320650291866, w1=-0.16716328725348195, grad=[ 0.00466876  0.02405497 -0.01949904 -0.00132341  0.00071378 -0.02449394\n",
      "  0.00123164  0.01796885 -0.00637381  0.01224704  0.02246297  0.01437156\n",
      "  0.00085153  0.02523088  0.00872381  0.00217794 -0.02411201 -0.00880719\n",
      "  0.00165044 -0.00749322 -0.00010015 -0.00915966  0.02305809 -0.00532966\n",
      " -0.00584888 -0.00585435  0.00098402  0.00089514  0.00090327  0.0141162 ]\n",
      "GD iter. 65/99: loss=0.4103812652558094, w0=0.03699359213745487, w1=-0.1694895881964098, grad=[ 4.44728513e-03  2.32630094e-02 -1.90635514e-02 -1.12909938e-03\n",
      "  7.14471487e-04 -2.42773133e-02  1.22813613e-03  1.76414479e-02\n",
      " -6.27119374e-03  1.20393014e-02  2.23075466e-02  1.38776069e-02\n",
      "  8.50945681e-04  2.48834920e-02  8.33236380e-03  2.00517770e-03\n",
      " -2.37560265e-02 -8.41611627e-03  1.51584668e-03 -7.32362575e-03\n",
      " -8.89421767e-05 -9.20618167e-03  2.29114229e-02 -5.26861896e-03\n",
      " -5.77343564e-03 -5.77877468e-03  9.69063369e-04  8.94365827e-04\n",
      "  9.02558582e-04  1.38702603e-02]\n",
      "GD iter. 66/99: loss=0.4098868002375574, w0=0.03657016425131584, w1=-0.1717395864401102, grad=[ 4.23427886e-03  2.24999824e-02 -1.86372212e-02 -9.42985756e-04\n",
      "  7.14969159e-04 -2.40625714e-02  1.22447948e-03  1.73202800e-02\n",
      " -6.16717545e-03  1.18341020e-02  2.21494163e-02  1.34020630e-02\n",
      "  8.50180662e-04  2.45421953e-02  7.95847959e-03  1.84668445e-03\n",
      " -2.34082096e-02 -8.04226286e-03  1.39152143e-03 -7.15803663e-03\n",
      " -7.84871767e-05 -9.24886684e-03  2.27663981e-02 -5.20849807e-03\n",
      " -5.69894171e-03 -5.70415962e-03  9.54273380e-04  8.93415725e-04\n",
      "  9.01662050e-04  1.36277664e-02]\n",
      "GD iter. 67/99: loss=0.40940862110151144, w0=0.03616721197152342, w1=-0.17391605574935942, grad=[ 4.02952280e-03  2.17646931e-02 -1.82199507e-02 -7.64703013e-04\n",
      "  7.15277752e-04 -2.38496745e-02  1.22067446e-03  1.70051372e-02\n",
      " -6.06215188e-03  1.16315095e-02  2.19888841e-02  1.29442323e-02\n",
      "  8.49239151e-04  2.42068250e-02  7.60135278e-03  1.70125463e-03\n",
      " -2.30682274e-02 -7.68489391e-03  1.27671796e-03 -6.99639243e-03\n",
      " -6.87620081e-05 -9.28779714e-03  2.26229251e-02 -5.14933886e-03\n",
      " -5.62546653e-03 -5.63057511e-03  9.39648892e-04  8.92290260e-04\n",
      "  9.00586785e-04  1.33887584e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 68/99: loss=0.40894597248548437, w0=0.03578393269137318, w1=-0.17602165644249715, grad=[ 3.83279280e-03  2.10560069e-02 -1.78116274e-02 -5.93903728e-04\n",
      "  7.15398799e-04 -2.36385856e-02  1.21672194e-03  1.66958279e-02\n",
      " -5.95647987e-03  1.14315786e-02  2.18262303e-02  1.25034422e-02\n",
      "  8.48122502e-04  2.38772267e-02  7.26021752e-03  1.56778486e-03\n",
      " -2.27357675e-02 -7.34329756e-03  1.17074322e-03 -6.83862687e-03\n",
      " -5.97391348e-05 -9.32305814e-03  2.24809221e-02 -5.09117701e-03\n",
      " -5.55306964e-03 -5.55807967e-03  9.25186421e-04  8.90990783e-04\n",
      "  8.99334419e-04  1.31532661e-02]\n",
      "GD iter. 69/99: loss=0.40849814472048, w0=0.035419546515443275, w1=-0.17805894099622044, grad=[ 3.64386176e-03  2.03728455e-02 -1.74121285e-02 -4.30261283e-04\n",
      "  7.15333897e-04 -2.34292705e-02  1.21262289e-03  1.63921754e-02\n",
      " -5.85047861e-03  1.12343544e-02  2.16617122e-02  1.20790436e-02\n",
      "  8.46832141e-04  2.35532526e-02  6.93434557e-03  1.44526753e-03\n",
      " -2.24105386e-02 -7.01679019e-03  1.07295107e-03 -6.68467117e-03\n",
      " -5.13893716e-05 -9.35473656e-03  2.23403167e-02 -5.03403912e-03\n",
      " -5.48179871e-03 -5.48672003e-03  9.10882393e-04  8.89518717e-04\n",
      "  8.97906634e-04  1.29213107e-02]\n",
      "GD iter. 70/99: loss=0.408064470522903, w0=0.03507329642124009, w1=-0.18003035929088268, grad=[ 3.46250094e-03  1.97141829e-02 -1.70213230e-02 -2.73468179e-04\n",
      "  7.15084713e-04 -2.32216981e-02  1.20837841e-03  1.60940168e-02\n",
      " -5.74443297e-03  1.10398733e-02  2.14955664e-02  1.16704098e-02\n",
      "  8.45369573e-04  2.32347611e-02  6.62304403e-03  1.33278222e-03\n",
      " -2.20922687e-02 -6.70471570e-03  9.82739490e-04 -6.53445457e-03\n",
      " -4.36824282e-05 -9.38291994e-03  2.22010440e-02 -4.97794389e-03\n",
      " -5.41169108e-03 -5.41653264e-03  8.96733174e-04  8.87875566e-04\n",
      "  8.96305170e-04  1.26929065e-02]\n",
      "GD iter. 71/99: loss=0.4076443219627974, w0=0.03474444830299134, w1=-0.18193826352579193, grad=[ 3.28848118e-03  1.90790423e-02 -1.66390728e-02 -1.23234513e-04\n",
      "  7.14652993e-04 -2.30158398e-02  1.20398966e-03  1.58012012e-02\n",
      " -5.63859663e-03  1.08481638e-02  2.13280109e-02  1.12769370e-02\n",
      "  8.43736384e-04  2.29216162e-02  6.32565329e-03  1.22948791e-03\n",
      " -2.17807033e-02 -6.40644476e-03  8.99547970e-04 -6.38790472e-03\n",
      " -3.65873691e-05 -9.40769630e-03  2.20630463e-02 -4.92290322e-03\n",
      " -5.34277498e-03 -5.34754498e-03  8.82735107e-04  8.86062918e-04\n",
      "  8.94531833e-04  1.24680610e-02]\n",
      "GD iter. 72/99: loss=0.4072371076815658, w0=0.03443229091375318, w1=-0.18378491283117, grad=[ 3.12157389e-03  1.84664931e-02 -1.62652343e-02  2.07134030e-05\n",
      "  7.14040566e-04 -2.28116691e-02  1.19945795e-03  1.55135885e-02\n",
      " -5.53319486e-03  1.06592476e-02  2.11592461e-02  1.08980426e-02\n",
      "  8.41934254e-04  2.26136873e-02  6.04154511e-03  1.13461588e-03\n",
      " -2.14756039e-02 -6.12137400e-03  8.22854995e-04 -6.24494810e-03\n",
      " -3.00729970e-05 -9.42915388e-03  2.19262722e-02 -4.86892314e-03\n",
      " -5.27507079e-03 -5.27977668e-03  8.68884529e-04  8.84082454e-04\n",
      "  8.92588499e-04  1.22467756e-02]\n",
      "GD iter. 73/99: loss=0.40684227033559445, w0=0.034136135720213165, w1=-0.18557247760089543, grad=[ 2.96155194e-03  1.78756477e-02 -1.58996597e-02  1.58634291e-04\n",
      "  7.13249348e-04 -2.26091616e-02  1.19478468e-03  1.52310483e-02\n",
      " -5.42842714e-03  1.04731394e-02  2.09894571e-02  1.05331659e-02\n",
      "  8.39964950e-04  2.23108488e-02  5.77012095e-03  1.04746328e-03\n",
      " -2.11767466e-02 -5.84892523e-03  7.52175611e-04 -6.10551036e-03\n",
      " -2.41081725e-05 -9.44738089e-03  2.17906760e-02 -4.81600461e-03\n",
      " -5.20859199e-03 -5.21324057e-03  8.55177801e-04  8.81935942e-04\n",
      "  8.90477121e-04  1.20290465e-02]\n",
      "GD iter. 74/99: loss=0.4064592842447873, w0=0.033855316682960365, w1=-0.18730304356787847, grad=[ 2.80819037e-03  1.73056597e-02 -1.55421977e-02  2.90773039e-04\n",
      "  7.12281344e-04 -2.24082949e-02  1.18997138e-03  1.49534588e-02\n",
      " -5.32446950e-03  1.02898487e-02  2.08188145e-02  1.01817666e-02\n",
      "  8.37830336e-04  2.20129796e-02  5.51081029e-03  9.67387342e-04\n",
      " -2.08839210e-02 -5.58854453e-03  6.87059122e-04 -5.96951667e-03\n",
      " -1.86620766e-05 -9.46246531e-03  2.16562166e-02 -4.76414430e-03\n",
      " -5.14334611e-03 -5.14794358e-03  8.41611321e-04  8.79625250e-04\n",
      "  8.88199730e-04  1.18148648e-02]\n",
      "GD iter. 75/99: loss=0.40608765322725743, w0=0.03358918997354407, w1=-0.18897861564187696, grad=[ 2.66126709e-03  1.67557207e-02 -1.51926948e-02  4.17361726e-04\n",
      "  7.11138650e-04 -2.22090480e-02  1.18501965e-03  1.46807059e-02\n",
      " -5.22147670e-03  1.01093792e-02  2.06474759e-02  9.84332474e-03\n",
      "  8.35532369e-04  2.17199628e-02  5.26306918e-03  8.93800064e-04\n",
      " -2.05969288e-02 -5.33970148e-03  6.27086896e-04 -5.83689194e-03\n",
      " -1.37044252e-05 -9.47449466e-03  2.15228578e-02 -4.71333520e-03\n",
      " -5.07933554e-03 -5.08388752e-03  8.28181545e-04  8.77152338e-04\n",
      "  8.85758439e-04  1.16042172e-02]\n",
      "GD iter. 76/99: loss=0.4057269086034136, w0=0.03333713363833806, w1=-0.190601121527729, grad=[ 2.52056335e-03  1.62250589e-02 -1.48509959e-02  5.38620561e-04\n",
      "  7.09823451e-04 -2.20114014e-02  1.17993126e-03  1.44126823e-02\n",
      " -5.11958414e-03  9.93173017e-03  2.04755869e-02  9.51734006e-03\n",
      "  8.33073102e-04  2.14316859e-02  5.02637887e-03  8.26163400e-04\n",
      " -2.03155833e-02 -5.10188827e-03  5.71870276e-04 -5.70756109e-03\n",
      " -9.20564097e-06 -9.48355592e-03  2.13905668e-02 -4.66356720e-03\n",
      " -5.01655820e-03 -5.02106983e-03  8.14884996e-04  8.74519260e-04\n",
      "  8.83155442e-04  1.13970865e-02]\n",
      "GD iter. 77/99: loss=0.40537660735440406, w0=0.03309854721805801, w1=-0.19217241514033484, grad=[ 2.38586420e-03  1.57129361e-02 -1.45169449e-02  6.54758723e-04\n",
      "  7.08338022e-04 -2.18153371e-02  1.17470802e-03  1.41492871e-02\n",
      " -5.01890977e-03  9.75689659e-03  2.03032822e-02  9.20333124e-03\n",
      "  8.30454679e-04  2.11480401e-02  4.80024446e-03  7.63984897e-04\n",
      " -2.00397077e-02 -4.87461883e-03  5.21048604e-04 -5.58144927e-03\n",
      " -5.13698996e-06 -9.48973528e-03  2.12593145e-02 -4.61482757e-03\n",
      " -4.95500820e-03 -4.95948414e-03  8.01718278e-04  8.71728165e-04\n",
      "  8.80393015e-04  1.11934518e-02]\n",
      "GD iter. 78/99: loss=0.4050363304214164, w0=0.03287285133073164, w1=-0.19369427983124216, grad=[ 2.25695887e-03  1.52186469e-02 -1.41903854e-02  7.65975141e-04\n",
      "  7.06684727e-04 -2.16208379e-02  1.16935189e-03  1.38904248e-02\n",
      " -4.91955562e-03  9.58486952e-03  2.01306864e-02  8.90083542e-03\n",
      "  8.27679337e-04  2.08689201e-02  4.58419377e-03  7.06813722e-04\n",
      " -1.97691349e-02 -4.65742807e-03  4.74287359e-04 -5.45848203e-03\n",
      " -1.47068651e-06 -9.49311812e-03  2.11290749e-02 -4.56710141e-03\n",
      " -4.89467639e-03 -4.89912087e-03  7.88678082e-04  8.68781292e-04\n",
      "  8.77473515e-04  1.09932890e-02]\n",
      "GD iter. 79/99: loss=0.4047056811336765, w0=0.0326594872249838, w1=-0.1951684314403589, grad=[ 2.13364106e-03  1.47415161e-02 -1.38711613e-02  8.72459189e-04\n",
      "  7.04866015e-04 -2.14278880e-02  1.16386492e-03  1.36360047e-02\n",
      " -4.82160938e-03  9.41563657e-03  1.99579150e-02  8.60940758e-03\n",
      "  8.24749402e-04  2.05942243e-02  4.37777616e-03  6.54237045e-04\n",
      " -1.95037066e-02 -4.44987099e-03  4.31276390e-04 -5.33858550e-03\n",
      "  1.82002775e-06 -9.49378881e-03  2.09998247e-02 -4.52037201e-03\n",
      " -4.83555083e-03 -4.83996770e-03  7.75761199e-04  8.65680970e-04\n",
      "  8.74399377e-04  1.07965711e-02]\n",
      "GD iter. 80/99: loss=0.40438428375418767, w0=0.032457916309661806, w1=-0.19659652118512227, grad=[ 2.01570915e-03  1.42808974e-02 -1.35591170e-02  9.74391334e-04\n",
      "  7.02884420e-04 -2.12364722e-02  1.15824924e-03  1.33859407e-02\n",
      " -4.72514573e-03  9.24918226e-03  1.97850748e-02  8.32861994e-03\n",
      "  8.21667287e-04  2.03238544e-02  4.18056151e-03  6.05876750e-04\n",
      " -1.92432724e-02 -4.25152192e-03  3.91728259e-04 -5.22168649e-03\n",
      "  4.76082755e-06 -9.49183068e-03  2.08715427e-02 -4.47462121e-03\n",
      " -4.77761724e-03 -4.78200999e-03  7.62964521e-04  8.62429615e-04\n",
      "  8.71173114e-04  1.06032685e-02]\n",
      "GD iter. 81/99: loss=0.40407178213331296, w0=0.032267619665079295, w1=-0.1979801383983751, grad=[ 1.90296645e-03  1.38361721e-02 -1.32540978e-02  1.07194372e-03\n",
      "  7.00742553e-04 -2.10465764e-02  1.15250709e-03  1.31401508e-02\n",
      " -4.63022764e-03  9.08548835e-03  1.96122651e-02  8.05806140e-03\n",
      "  8.18435486e-04  2.00577149e-02  3.99213919e-03  5.61386442e-04\n",
      " -1.89876893e-02 -4.06197372e-03  3.55376679e-04 -5.10771268e-03\n",
      "  7.37626661e-06 -9.48732592e-03  2.07442103e-02 -4.42982968e-03\n",
      " -4.72085937e-03 -4.72523116e-03  7.50285046e-04  8.59029724e-04\n",
      "  8.67797314e-04  1.04133492e-02]\n",
      "GD iter. 82/99: loss=0.40376783846123554, w0=0.032088097540492064, w1=-0.19932081312523015, grad=[ 1.79522125e-03  1.34067473e-02 -1.29559508e-02  1.16528070e-03\n",
      "  6.98443106e-04 -2.08581870e-02  1.14664079e-03  1.28985563e-02\n",
      " -4.53690744e-03  8.92453410e-03  1.94395777e-02  7.79733689e-03\n",
      "  8.15056574e-04  1.97957139e-02  3.81211715e-03  5.20448717e-04\n",
      " -1.87368211e-02 -3.88083705e-03  3.21975046e-04 -4.99659266e-03\n",
      "  9.68975638e-06 -9.48035551e-03  2.06178103e-02 -4.38597715e-03\n",
      " -4.66525933e-03 -4.66961303e-03  7.37719885e-04  8.55483875e-04\n",
      "  8.64274635e-04  1.02267792e-02]\n",
      "GD iter. 83/99: loss=0.4034721321111872, w0=0.03191886884182711, w1=-0.2006200185883267, grad=[ 1.69228699e-03  1.29920546e-02 -1.26645241e-02  1.25455933e-03\n",
      "  6.95988840e-04 -2.06712913e-02  1.14065273e-03  1.26610820e-02\n",
      " -4.44522792e-03  8.76629656e-03  1.92670981e-02  7.54606689e-03\n",
      "  8.11533201e-04  1.95377617e-02  3.64012101e-03  4.82772679e-04\n",
      " -1.84905382e-02 -3.70773959e-03  2.91295066e-04 -4.88825605e-03\n",
      "  1.17235580e-05 -9.47099916e-03  2.04923274e-02 -4.34304268e-03\n",
      " -4.61079789e-03 -4.61513609e-03  7.25266260e-04  8.51794718e-04\n",
      "  8.60607805e-04  1.00435226e-02]\n",
      "GD iter. 84/99: loss=0.4031843585660724, w0=0.03175947061316028, w1=-0.2018791735300955, grad=[ 1.59398229e-03  1.25915494e-02 -1.23796681e-02  1.33992982e-03\n",
      "  6.93382588e-04 -2.04858770e-02  1.13454541e-03  1.24276553e-02\n",
      " -4.35522326e-03  8.61075082e-03  1.90949055e-02  7.30388677e-03\n",
      "  8.07868087e-04  1.92837720e-02  3.47579327e-03  4.48091671e-04\n",
      " -1.82487165e-02 -3.54232536e-03  2.63125462e-04 -4.78263356e-03\n",
      "  1.34987857e-05 -9.45933524e-03  2.03677477e-02 -4.30100480e-03\n",
      " -4.55745473e-03 -4.56177976e-03  7.12921504e-04  8.47964976e-04\n",
      "  8.56799613e-04  9.86354195e-03]\n",
      "GD iter. 85/99: loss=0.40290422842178775, w0=0.03160945751497074, w1=-0.20309964443993184, grad=[ 1.50013098e-03  1.22047091e-02 -1.21012350e-02  1.42153593e-03\n",
      "  6.90627243e-04 -2.03019323e-02  1.12832138e-03  1.21982065e-02\n",
      " -4.26691988e-03  8.45787023e-03  1.89230734e-02  7.07044634e-03\n",
      "  8.04064017e-04  1.90336606e-02  3.31879246e-03  4.16161212e-04\n",
      " -1.80112377e-02 -3.38425403e-03  2.37270772e-04 -4.67965707e-03\n",
      "  1.50354192e-05 -9.44544081e-03  2.02440585e-02 -4.25984171e-03\n",
      " -4.50520866e-03 -4.50952265e-03  7.00683069e-04  8.43997438e-04\n",
      "  8.52852912e-04  9.68679825e-03]\n",
      "GD iter. 86/99: loss=0.402631466461146, w0=0.031468401301786894, w1=-0.20428274767353394, grad=[ 1.41056213e-03  1.18310323e-02 -1.18290793e-02  1.49951539e-03\n",
      "  6.87725761e-04 -2.01194459e-02  1.12198325e-03  1.19726680e-02\n",
      " -4.18033728e-03  8.30762662e-03  1.87516701e-02  6.84540921e-03\n",
      "  8.00123841e-04  1.87873462e-02  3.16879246e-03  3.86757112e-04\n",
      " -1.77779882e-02 -3.23320024e-03  2.13550213e-04 -4.57925965e-03\n",
      "  1.63523232e-05 -9.42939150e-03  2.01212484e-02 -4.21953140e-03\n",
      " -4.45403784e-03 -4.45834269e-03  6.88548518e-04  8.39894951e-04\n",
      "  8.48770608e-04  9.51325134e-03]\n",
      "GD iter. 87/99: loss=0.4023658107928458, w0=0.03133589030147097, w1=-0.20542975147107856, grad=[ 1.32511000e-03  1.14700380e-02 -1.15630579e-02  1.57400020e-03\n",
      "  6.84681152e-04 -1.99384069e-02  1.11553371e-03  1.17509746e-02\n",
      " -4.09548867e-03  8.15999046e-03  1.85807592e-02  6.62845237e-03\n",
      "  7.96050463e-04  1.85447499e-02  3.02548173e-03  3.59673752e-04\n",
      " -1.75488593e-02 -3.08885296e-03  1.91796636e-04 -4.48137565e-03\n",
      "  1.74672741e-05 -9.41126153e-03  1.99993067e-02 -4.18005179e-03\n",
      " -4.40391991e-03 -4.40821734e-03  6.76515525e-04  8.35660423e-04\n",
      "  8.44555659e-04  9.34285995e-03]\n",
      "GD iter. 88/99: loss=0.402107012050417, w0=0.031211528898065753, w1=-0.206541877880378, grad=[ 1.24361403e-03  1.11212641e-02 -1.13030298e-02  1.64511697e-03\n",
      "  6.81496475e-04 -1.97588048e-02  1.10897550e-03  1.15330628e-02\n",
      " -4.01238172e-03  8.01493104e-03  1.84103996e-02  6.41926560e-03\n",
      "  7.91846840e-04  1.83057949e-02  2.88856269e-03  3.34722515e-04\n",
      " -1.73237467e-02 -2.95091488e-03  1.71855532e-04 -4.38594067e-03\n",
      "  1.83969900e-05 -9.39112370e-03  1.98782238e-02 -4.14138084e-03\n",
      " -4.35483219e-03 -4.35912374e-03  6.64581880e-04  8.31296810e-04\n",
      "  8.40211070e-04  9.17558187e-03]\n",
      "GD iter. 89/99: loss=0.40185483264650784, w0=0.031094937019839865, w1=-0.20762030459068329, grad=[ 1.16591878e-03  1.07842671e-02 -1.10488569e-02  1.71298723e-03\n",
      "  6.78174836e-04 -1.95806294e-02  1.10231140e-03  1.13188710e-02\n",
      " -3.93101908e-03  7.87241665e-03  1.82406460e-02  6.21755102e-03\n",
      "  7.87515975e-04  1.80704070e-02  2.75775104e-03  3.11730350e-04\n",
      " -1.71025501e-02 -2.81910185e-03  1.53584121e-04 -4.29289162e-03\n",
      "  1.91571652e-05 -9.36904934e-03  1.97579907e-02 -4.10349661e-03\n",
      " -4.30675176e-03 -4.31103882e-03  6.52745479e-04  8.26807116e-04\n",
      "  8.35739889e-04  9.01137409e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 90/99: loss=0.40160904607828274, w0=0.030985749633915333, w1=-0.2086661666823637, grad=[ 1.09187386e-03  1.04586209e-02 -1.08004036e-02  1.77772768e-03\n",
      "  6.74719380e-04 -1.94038708e-02  1.09554427e-03  1.11083392e-02\n",
      " -3.85139898e-03  7.73241470e-03  1.80715495e-02  6.02302258e-03\n",
      "  7.83060913e-04  1.78385139e-02  2.63277517e-03  2.90538469e-04\n",
      " -1.68851731e-02 -2.69314224e-03  1.36850491e-04 -4.20216675e-03\n",
      "  1.97625076e-05 -9.34510833e-03  1.96385991e-02 -4.06637736e-03\n",
      " -4.25965562e-03 -4.26393941e-03  6.41004325e-04  8.22194386e-04\n",
      "  8.31145201e-04  8.85019294e-03]\n",
      "GD iter. 91/99: loss=0.4013694362800481, w0=0.03088361624863815, w1=-0.20968055829729884, grad=[ 1.02133385e-03  1.01439161e-02 -1.05575369e-02  1.83945049e-03\n",
      "  6.71133289e-04 -1.92285191e-02  1.08867697e-03  1.09014089e-02\n",
      " -3.77351566e-03  7.59489184e-03  1.79031572e-02  5.83540565e-03\n",
      "  7.78484738e-04  1.76100456e-02  2.51337560e-03  2.71001143e-04\n",
      " -1.66715229e-02 -2.57277644e-03  1.21532805e-04 -4.11370562e-03\n",
      "  2.02267774e-05 -9.31936906e-03  1.95200412e-02 -4.03000163e-03\n",
      " -4.21352072e-03 -4.21780237e-03  6.29356527e-04  8.17461702e-04\n",
      "  8.26430126e-04  8.69199418e-03]\n",
      "GD iter. 92/99: loss=0.401135797019551, w0=0.030788200424656395, w1=-0.2106645342344592, grad=[ 9.54158240e-04  9.83975937e-03 -1.03201263e-02  1.89826345e-03\n",
      "  6.67419773e-04 -1.90545650e-02  1.08171243e-03  1.06980227e-02\n",
      " -3.69735988e-03  7.45981413e-03  1.77355130e-02  5.65443651e-03\n",
      "  7.73790563e-04  1.73849340e-02  2.39930440e-03  2.52984616e-04\n",
      " -1.64615100e-02 -2.45775636e-03  1.07518557e-04 -4.02744915e-03\n",
      "  2.05628277e-05 -9.29189847e-03  1.94023097e-02 -3.99434823e-03\n",
      " -4.16832414e-03 -4.17260462e-03  6.17800296e-04  8.12612179e-04\n",
      "  8.21597810e-04  8.53673315e-03]\n",
      "GD iter. 93/99: loss=0.4009079313346941, w0=0.030699179295498244, w1=-0.21161911147482645, grad=[ 8.90211292e-04  9.54577240e-03 -1.00880444e-02  1.95427029e-03\n",
      "  6.63582071e-04 -1.88819992e-02  1.07465358e-03  1.04981247e-02\n",
      " -3.62291929e-03  7.32714708e-03  1.75686577e-02  5.47986196e-03\n",
      "  7.68981531e-04  1.71631131e-02  2.29032468e-03  2.36366100e-04\n",
      " -1.62550482e-02 -2.34784484e-03  9.47038821e-05 -3.94333959e-03\n",
      "  2.07826457e-05 -9.26276200e-03  1.92853979e-02 -3.95939636e-03\n",
      " -4.12404307e-03 -4.12832326e-03  6.06333940e-04  8.07648956e-04\n",
      "  8.16651426e-04  8.38436484e-03]\n",
      "GD iter. 94/99: loss=0.40068565100766873, w0=0.030616243098290797, w1=-0.21254527063950615, grad=[ 8.29361972e-04  9.26159165e-03 -9.86116616e-03  2.00757079e-03\n",
      "  6.59623442e-04 -1.87108125e-02  1.06750340e-03  1.03016602e-02\n",
      " -3.55017880e-03  7.19685580e-03  1.74026291e-02  5.31143886e-03\n",
      "  7.64060807e-04  1.69445186e-02  2.18621011e-03  2.21032863e-04\n",
      " -1.60520541e-02 -2.24281528e-03  8.29929128e-05 -3.86132052e-03\n",
      "  2.08973938e-05 -9.23202366e-03  1.91692993e-02 -3.92512562e-03\n",
      " -4.08065491e-03 -4.08493559e-03  5.94955863e-04  8.02575199e-04\n",
      "  8.11594165e-04  8.23484399e-03]\n",
      "GD iter. 95/99: loss=0.40046877607375636, w0=0.03053909471512847, w1=-0.2134439573846142, grad=[ 7.71483832e-04  8.98686745e-03 -9.63936939e-03  2.05826104e-03\n",
      "  6.55547163e-04 -1.85409959e-02  1.06026488e-03  1.01085753e-02\n",
      " -3.47912094e-03  7.06890507e-03  1.72374621e-02  5.14893378e-03\n",
      "  7.59031573e-04  1.67290881e-02  2.08674442e-03  2.06881393e-04\n",
      " -1.58524472e-02 -2.14245109e-03  7.22971861e-05 -3.78133687e-03\n",
      "  2.09174510e-05 -9.19974595e-03  1.90540076e-02 -3.89151599e-03\n",
      " -4.03813733e-03 -4.04241919e-03  5.83664562e-04  7.97394088e-04\n",
      "  8.06429234e-04  8.08812516e-03]\n",
      "GD iter. 96/99: loss=0.4002571343622709, w0=0.030467449225483785, w1=-0.2143160837362716, grad=[ 7.16454896e-04  8.72126352e-03 -9.42253459e-03  2.10643357e-03\n",
      "  6.51356522e-04 -1.83725405e-02  1.05294104e-03  9.91881727e-03\n",
      " -3.40972611e-03  6.94325943e-03  1.70731894e-02  4.99212258e-03\n",
      "  7.53897027e-04  1.65167610e-02  1.99172097e-03  1.93816633e-04\n",
      " -1.56561498e-02 -2.04654528e-03  6.25350874e-05 -3.70333486e-03\n",
      "  2.08524540e-05 -9.16598995e-03  1.89395171e-02 -3.85854794e-03\n",
      " -3.99646831e-03 -4.00075195e-03  5.72458621e-04  7.92108817e-04\n",
      "  8.01159850e-04  7.94416284e-03]\n",
      "GD iter. 97/99: loss=0.40005056106730846, w0=0.03040103346995264, w1=-0.21516252936881478, grad=[ 6.64157555e-04  8.46445633e-03 -9.21054494e-03  2.15217755e-03\n",
      "  6.47054816e-04 -1.82054377e-02  1.04553489e-03  9.73233422e-03\n",
      " -3.34197294e-03  6.81988324e-03  1.69098408e-02  4.84079002e-03\n",
      "  7.48660375e-04  1.63074784e-02  1.90094229e-03  1.81751279e-04\n",
      " -1.54630864e-02 -1.95490005e-03  5.36313366e-05 -3.62726205e-03\n",
      "  2.07113367e-05 -9.13081528e-03  1.88258219e-02 -3.82620238e-03\n",
      " -3.95562615e-03 -3.95991210e-03  5.61336710e-04  7.86722591e-04\n",
      "  7.95789240e-04  7.80291147e-03]\n",
      "GD iter. 98/99: loss=0.39984889834616305, w0=0.030339585625538235, w1=-0.21598414282912198, grad=[ 6.14478444e-04  8.21613460e-03 -9.00328630e-03  2.19557891e-03\n",
      "  6.42645346e-04 -1.80396787e-02  1.03804946e-03  9.54907518e-03\n",
      " -3.27583845e-03  6.69874077e-03  1.67474441e-02  4.69472942e-03\n",
      "  7.43324827e-04  1.61011830e-02  1.81421969e-03  1.70605143e-04\n",
      " -1.52731841e-02 -1.86732636e-03  4.55165109e-05 -3.55306728e-03\n",
      "  2.05023701e-05 -9.09428013e-03  1.87129167e-02 -3.79446071e-03\n",
      " -3.91558953e-03 -3.91987825e-03  5.50297580e-04  7.81238619e-04\n",
      "  7.90320630e-04  7.66432555e-03]\n",
      "GD iter. 99/99: loss=0.39965199494343195, w0=0.03028285479260167, w1=-0.21678174270976605, grad=[ 5.67308329e-04  7.97599881e-03 -8.80064715e-03  2.23672050e-03\n",
      "  6.38131413e-04 -1.78752550e-02  1.03048779e-03  9.36898998e-03\n",
      " -3.21129835e-03  6.57979625e-03  1.65860252e-02  4.55374234e-03\n",
      "  7.37893596e-04  1.58978191e-02  1.73137287e-03  1.60304560e-04\n",
      " -1.50863724e-02 -1.78364356e-03  3.81266026e-05 -3.48070067e-03\n",
      "  2.02331999e-05 -9.05644127e-03  1.86007961e-02 -3.76330480e-03\n",
      " -3.87633752e-03 -3.88062941e-03  5.39340060e-04  7.75660109e-04\n",
      "  7.84757249e-04  7.52835970e-03]\n",
      "proportion=0.8, degree=1, Training RMSE=0.894, Testing RMSE=0.895\n"
     ]
    }
   ],
   "source": [
    "#RMSE of the train set + test set\n",
    "max_iters = 100\n",
    "w_initial = np.random.rand(30)\n",
    "x_tr,x_te,y_tr,y_te,w_star = train_test_split_demo(std_data, y, 1, 0.8,w_initial,max_iters,0.1, seed=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ef09145a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.7109%\n"
     ]
    }
   ],
   "source": [
    "#Same method competition (% accuracy)\n",
    "#test prediction x_te \n",
    "y_pred = predict_labels(w_star, x_te)\n",
    "#compare y_pred,y_test\n",
    "s = 0\n",
    "for i in range(len(y_te)):\n",
    "    if y_te[i] == y_pred[i]:\n",
    "        s+=1\n",
    "print(\"Accuracy:\"+str(s/len(y_te))+\"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e3679ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae7b6cb",
   "metadata": {},
   "source": [
    "# Cross validation + GD + ridge regression + build poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "30dc1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed=1):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "    \n",
    "    Args:\n",
    "        y:      shape=(N,)\n",
    "        k_fold: K in K-fold, i.e. the fold num\n",
    "        seed:   the random seed\n",
    "\n",
    "    Returns:\n",
    "        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "\n",
    "    >>> build_k_indices(np.array([1., 2., 3., 4.]), 2, 1)\n",
    "    array([[3, 2],\n",
    "           [0, 1]])\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "5320c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ridge_regression import ridge_regression\n",
    "\n",
    "def cross_validation(x, y, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression for a fold corresponding to k_indices\n",
    "    \n",
    "    Args:\n",
    "        y:          shape=(N,)\n",
    "        x:          shape=(N,)\n",
    "        k_indices:  2D array returned by build_k_indices()\n",
    "        k:          scalar, the k-th fold (N.B.: not to confused with k_fold which is the fold nums)\n",
    "        lambda_:    scalar, cf. ridge_regression()\n",
    "        degree:     scalar, cf. build_poly()\n",
    "\n",
    "    Returns:\n",
    "        train and test root mean square errors rmse = sqrt(2 mse)\n",
    "\n",
    "    >>> cross_validation(np.array([1.,2.,3.,4.]), np.array([6.,7.,8.,9.]), np.array([[3,2], [0,1]]), 1, 2, 3)\n",
    "    (0.019866645527598144, 0.3355591436129497)\n",
    "    \"\"\"\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    indice_test_k = k_indices[k]\n",
    "    x_test = x[indice_test_k,:]\n",
    "    y_test = y[indice_test_k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_train = y[tr_indice]\n",
    "    x_train = x[tr_indice]\n",
    "\n",
    "    # ***************************************************\n",
    "   \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # form data with polynomial degree: TODO\n",
    "    poly_tr = build_poly(x_train, degree)\n",
    "    poly_te = build_poly(x_test, degree)\n",
    "\n",
    "    # ***************************************************\n",
    "   \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ridge regression: TODO\n",
    "    w_star = ridge_regression(y_train,poly_tr,lambda_)\n",
    "\n",
    "    # ***************************************************\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # calculate the loss for train and test data: TODO\n",
    "   \n",
    "    loss_tr = np.sqrt(2*compute_loss(y_train, poly_tr, w_star))\n",
    "    loss_te = np.sqrt(2*compute_loss(y_test, poly_te, w_star))\n",
    "    # ***************************************************\n",
    "    \n",
    "    return loss_tr, loss_te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "005b3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots_lab4 import cross_validation_visualization\n",
    "\n",
    "def cross_validation_demo(x,y,degree, k_fold, lambdas):\n",
    "    \"\"\"cross validation over regularisation parameter lambda.\n",
    "    \n",
    "    Args:\n",
    "        degree: integer, degree of the polynomial expansion\n",
    "        k_fold: integer, the number of folds\n",
    "        lambdas: shape = (p, ) where p is the number of values of lambda to test\n",
    "    Returns:\n",
    "        best_lambda : scalar, value of the best lambda\n",
    "        best_rmse : scalar, the associated root mean squared error for the best lambda\n",
    "    \"\"\"\n",
    "    \n",
    "    seed = 1\n",
    "    degree = degree\n",
    "    k_fold = k_fold\n",
    "    lambdas = lambdas\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # cross validation over lambdas: TODO\n",
    "    for lambda_ in lambdas:\n",
    "        rmse_tr_k = []\n",
    "        rmse_te_k = []\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te = cross_validation(x, y, k_indices, k, lambda_, degree)\n",
    "            rmse_tr_k.append(loss_tr)\n",
    "            rmse_te_k.append(loss_te)\n",
    "        rmse_tr.append(np.mean(rmse_tr_k))\n",
    "        rmse_te.append(np.mean(rmse_te_k))\n",
    "    # ***************************************************\n",
    "    \n",
    "    ind_best_rmse = np.where(rmse_te<=np.min(rmse_te))\n",
    "    best_rmse = np.min(rmse_te)\n",
    "    best_lambda = lambdas[ind_best_rmse]\n",
    "    #cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "    #print(\"For polynomial expansion up to degree %.f, the choice of lambda which leads to the best test rmse is %.5f with a test rmse of %.3f\" % (degree, best_lambda, best_rmse))\n",
    "    return best_lambda, best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "628dbabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_degree_selection(x,y,degrees, k_fold, lambdas, seed = 1):\n",
    "    \"\"\"cross validation over regularisation parameter lambda and degree.\n",
    "    \n",
    "    Args:\n",
    "        degrees: shape = (d,), where d is the number of degrees to test \n",
    "        k_fold: integer, the number of folds\n",
    "        lambdas: shape = (p, ) where p is the number of values of lambda to test\n",
    "    Returns:\n",
    "        best_degree : integer, value of the best degree\n",
    "        best_lambda : scalar, value of the best lambda\n",
    "        best_rmse : value of the rmse for the couple (best_degree, best_lambda)\n",
    "        \n",
    "    >>> best_degree_selection(np.arange(2,11), 4, np.logspace(-4, 0, 30))\n",
    "    (7, 0.004520353656360241, 0.2895728056812453)\n",
    "    \"\"\"\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # cross validation over degrees and lambdas: TODO\n",
    "    best_lamba_all_degree = []\n",
    "    best_rmse_all_degree = []\n",
    "    for degree in degrees:\n",
    "        best_lambda_degree, best_rmse_degree = cross_validation_demo(x,y,degree, k_fold, lambdas)\n",
    "        best_lamba_all_degree.append(best_lambda_degree)\n",
    "        best_rmse_all_degree.append(best_rmse_degree)\n",
    "    \n",
    "    ind_best_rmse = np.argmin(best_rmse_all_degree)\n",
    "    \n",
    "    best_rmse = np.min(best_rmse_all_degree)\n",
    "    best_lambda = best_lamba_all_degree[ind_best_rmse] \n",
    "    best_degree = degrees[ind_best_rmse]\n",
    "    # ***************************************************\n",
    "  \n",
    "    \n",
    "    return best_degree, best_lambda, best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "6984f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter\n",
    "degrees = np.arange(2,10)\n",
    "k_fold = 7\n",
    "lambdas = np.logspace(-4, 0, 30)\n",
    "seed = 1\n",
    "\n",
    "#split data train/test\n",
    "x_tr,x_te,y_tr,y_te = split_data(std_data, y, 1, seed)\n",
    "\n",
    "best_degree, best_lambda, best_rmse = best_degree_selection(x_tr,y_tr,degrees, k_fold, lambdas, seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "6385a37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, array([0.72789538]), 0.8397231418858686)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_degree,best_lambda,best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "d44ccd8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34196/3841362338.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_te\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0ms\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy:\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "#TEST intern substest\n",
    "poly_tr = build_poly(x_tr, best_degree)\n",
    "poly_te = build_poly(x_te, best_degree)\n",
    "w_star = ridge_regression(y_tr,poly_tr,best_lambda)\n",
    "#Same method competition (% accuracy)\n",
    "#test prediction x_te \n",
    "y_pred = predict_labels(w_star, poly_te)\n",
    "#compare y_pred,y_test\n",
    "s = 0\n",
    "for i in range(len(y_te)):\n",
    "    if y_te[i] == y_pred[i]:\n",
    "        s+=1\n",
    "print(\"Accuracy:\"+str(s/len(y_te))+\"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a227e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#real test validation\n",
    "#parameter use\n",
    "\"\"\"\n",
    "degrees = np.arange(2,10)\n",
    "k_fold = 7\n",
    "lambdas = np.logspace(-4, 0, 30)\n",
    "seed = 1\n",
    "\"\"\"\n",
    "\"result : best_degree,best_lambda,best_rmse : (3, array([1.]), 0.844390744147517)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2812305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 2 real test validation\n",
    "#parameter use\n",
    "\"\"\"\n",
    "degrees = np.arange(2,7)\n",
    "k_fold = 5\n",
    "lambdas = np.logspace(-4, 0, 30)\n",
    "seed = 1\n",
    "\"\"\"\n",
    "\"result : best_degree,best_lambda,best_rmse : (3, array([0.72789538]), 0.8397231418858686)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "baf37f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "w_star = ridge_regression(y_tr,poly_tr,best_lambda)\n",
    "std_data_test = standardize(tX_test)\n",
    "poly_te = build_poly(std_data_test, best_degree)\n",
    "OUTPUT_PATH = 'data/predictions.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w_star, poly_te)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "7306c546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000,)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
