{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd4481a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, sys, types\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87383a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60683c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementation import *\n",
    "from proj1_helpers import *\n",
    "from plots import gradient_descent_visualization\n",
    "from cross_validation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa3d2a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    ''' fill your code in here...\n",
    "    '''\n",
    "    centered_data = x - np.mean(x, axis=0)\n",
    "    std_data = centered_data / np.std(centered_data, axis=0)\n",
    "    \n",
    "    return std_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "944bbc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_test_prediction(x_te,w_star,y_te):\n",
    "    '''\n",
    "    Give the accuracy over a test subset data from train\n",
    "    '''\n",
    "    #Same method competition (% accuracy)\n",
    "    #test prediction x_te \n",
    "    y_pred = predict_labels(w_star, x_te)\n",
    "    #compare y_pred,y_test\n",
    "    s = 0\n",
    "    for i in range(len(y_te)):\n",
    "        if y_te[i] == y_pred[i]:\n",
    "            s+=1\n",
    "    print(\"Accuracy:\"+str(s/len(y_te))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "17dbde1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(w,tX,ids_test):\n",
    "    '''\n",
    "    Give the .csv file for competition\n",
    "    '''\n",
    "    OUTPUT_PATH = 'data/predictions.csv' # TODO: fill in desired name of output file for submission\n",
    "    y_pred = predict_labels(w, tX)\n",
    "    create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b966f825",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "DATA_TEST_PATH = 'data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "tX = standardize(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e35482",
   "metadata": {},
   "source": [
    "# Test GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e8d4cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gamma = 0.1\n",
    "ratio = 0.3\n",
    "\n",
    "def pred_GD(max_iters,gamma,tX,y,seed,ratio):\n",
    "    x_tr,x_te,y_tr,y_te = split_data(tX, y, 0.3, seed)\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.random.rand(30)\n",
    "\n",
    "    # Start gradient descent.\n",
    "    w, loss = mean_squared_error_gd(y_tr, x_tr, w_initial, max_iters, gamma)\n",
    "\n",
    "    if ratio==1:\n",
    "        #Create the .csv file\n",
    "        _, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "        prediction(w,tX,ids_test)\n",
    "    else :\n",
    "        #Give accuracy on the test subset\n",
    "        subset_test_prediction(x_te,w,y_te)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "31a0eadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=16.421624506640963, w0=0.18603040746340685, w1=0.7854396032157555\n",
      "Gradient Descent(1/99): loss=2.8630142222705284, w0=0.19122230716374575, w1=0.6745180717504576\n",
      "Gradient Descent(2/99): loss=1.717181178985656, w0=0.16680631265426066, w1=0.6100770157799097\n",
      "Gradient Descent(3/99): loss=1.3584849511434571, w0=0.15396711444448918, w1=0.5476820504284975\n",
      "Gradient Descent(4/99): loss=1.1344411685853233, w0=0.1423018218765532, w1=0.49501930064332367\n",
      "Gradient Descent(5/99): loss=0.9788570988358668, w0=0.1333441678084859, w1=0.4484440909458013\n",
      "Gradient Descent(6/99): loss=0.8676517810746178, w0=0.12608211547345044, w1=0.40729125675939715\n",
      "Gradient Descent(7/99): loss=0.7862994211537296, w0=0.1202189174669632, w1=0.37053123012191774\n",
      "Gradient Descent(8/99): loss=0.7254137990585546, w0=0.11540548644574102, w1=0.3374607798849983\n",
      "Gradient Descent(9/99): loss=0.678823218015403, w0=0.11139351888140533, w1=0.30749221065859905\n",
      "Gradient Descent(10/99): loss=0.6424111293691749, w0=0.10798318914134863, w1=0.2801597441806799\n",
      "Gradient Descent(11/99): loss=0.6133897643114208, w0=0.10502079570361805, w1=0.25508591705395645\n",
      "Gradient Descent(12/99): loss=0.589840377438421, w0=0.1023877216682432, w1=0.23196431380354912\n",
      "Gradient Descent(13/99): loss=0.5704197312318613, w0=0.09999356618589568, w1=0.21054417199866107\n",
      "Gradient Descent(14/99): loss=0.5541709449976724, w0=0.09777012779836187, w1=0.19061887177719816\n",
      "Gradient Descent(15/99): loss=0.5404003246839282, w0=0.09566666631828606, w1=0.17201686964001128\n",
      "Gradient Descent(16/99): loss=0.5285962340601541, w0=0.09364607510116839, w1=0.15459464732029438\n",
      "Gradient Descent(17/99): loss=0.5183749877687879, w0=0.09168182386958178, w1=0.1382311905182659\n",
      "Gradient Descent(18/99): loss=0.5094442807648183, w0=0.08975552003104334, w1=0.12282365879137118\n",
      "Gradient Descent(19/99): loss=0.5015781194574703, w0=0.08785497220835964, w1=0.10828397750735025\n",
      "Gradient Descent(20/99): loss=0.4945993838378328, w0=0.08597265892355904, w1=0.09453614614379102\n",
      "Gradient Descent(21/99): loss=0.48836751542293816, w0=0.08410452362602516, w1=0.0815141037652804\n",
      "Gradient Descent(22/99): loss=0.48276969357153215, w0=0.0822490317633902, w1=0.06916002884700526\n",
      "Gradient Descent(23/99): loss=0.47771441836989903, w0=0.08040643764815558, w1=0.057422978476532105\n",
      "Gradient Descent(24/99): loss=0.473126777111464, w0=0.07857821873824454, w1=0.046257793451152225\n",
      "Gradient Descent(25/99): loss=0.4689449052701813, w0=0.07676664303512758, w1=0.035624212336379246\n",
      "Gradient Descent(26/99): loss=0.4651173068406621, w0=0.0749744419097233, w1=0.025486150308611975\n",
      "Gradient Descent(27/99): loss=0.4616008013840469, w0=0.07320456605505504, w1=0.015811108444430524\n",
      "Gradient Descent(28/99): loss=0.4583589340920288, w0=0.07146000665098115, w1=0.006569686713630814\n",
      "Gradient Descent(29/99): loss=0.4553607321696553, w0=0.06974366738961255, w1=-0.00226482019994558\n",
      "Gradient Descent(30/99): loss=0.45257972324737505, w0=0.06805827589882951, w1=-0.010716760588752981\n",
      "Gradient Descent(31/99): loss=0.4499931541711354, w0=0.0664063254384302, w1=-0.018808409749243107\n",
      "Gradient Descent(32/99): loss=0.44758136453363395, w0=0.06479003963018713, w1=-0.026560210449412466\n",
      "Gradient Descent(33/99): loss=0.4453272807808161, w0=0.06321135450282989, w1=-0.033990977823255025\n",
      "Gradient Descent(34/99): loss=0.4432160050449399, w0=0.06167191335420789, w1=-0.04111807503296672\n",
      "Gradient Descent(35/99): loss=0.4412344789569433, w0=0.06017307091177037, w1=-0.04795756475579092\n",
      "Gradient Descent(36/99): loss=0.4393712072166368, w0=0.058715904054991545, w1=-0.05452434054839107\n",
      "Gradient Descent(37/99): loss=0.43761602909138175, w0=0.057301226987014585, w1=-0.060832241354234776\n",
      "Gradient Descent(38/99): loss=0.43595992858102606, w0=0.05592960923821082, w1=-0.06689415179990663\n",
      "Gradient Descent(39/99): loss=0.43439487594706044, w0=0.054601395276482625, w1=-0.07272209043694995\n",
      "Gradient Descent(40/99): loss=0.43291369481301717, w0=0.05331672480824383, w1=-0.07832728769779877\n",
      "Gradient Descent(41/99): loss=0.43150995021366184, w0=0.052075553096533826, w1=-0.08372025502525585\n",
      "Gradient Descent(42/99): loss=0.430177853884734, w0=0.050877670811981954, w1=-0.08891084638753206\n",
      "Gradient Descent(43/99): loss=0.42891218380352014, w0=0.04972272307910477, w1=-0.09390831319174373\n",
      "Gradient Descent(44/99): loss=0.42770821555856264, w0=0.048610227493386136, w1=-0.09872135344763013\n",
      "Gradient Descent(45/99): loss=0.42656166357824155, w0=0.04753959097076078, w1=-0.10335815590209506\n",
      "Gradient Descent(46/99): loss=0.42546863060853735, w0=0.04651012535612613, w1=-0.10782643975776131\n",
      "Gradient Descent(47/99): loss=0.4244255641195971, w0=0.0455210617658642, w1=-0.11213349050021217\n",
      "Gradient Descent(48/99): loss=0.4234292185538834, w0=0.04457156367467775, w1=-0.1162861922851898\n",
      "Gradient Descent(49/99): loss=0.4224766225173446, w0=0.043660738782223846, w1=-0.12029105727576757\n",
      "Gradient Descent(50/99): loss=0.4215650501682981, w0=0.04278764971235957, w1=-0.12415425226808087\n",
      "Gradient Descent(51/99): loss=0.42069199618365793, w0=0.04195132360912557, w1=-0.1278816229007582\n",
      "Gradient Descent(52/99): loss=0.419855153784374, w0=0.04115076070032805, w1=-0.13147871570628317\n",
      "Gradient Descent(53/99): loss=0.41905239538587613, w0=0.04038494190287648, w1=-0.13495079823098893\n",
      "Gradient Descent(54/99): loss=0.4182817555084532, w0=0.03965283554478716, w1=-0.1383028774233144\n",
      "Gradient Descent(55/99): loss=0.4175414156396227, w0=0.038953403277669496, w1=-0.14153971646659486\n",
      "Gradient Descent(56/99): loss=0.4168296907879131, w0=0.03828560525111499, w1=-0.1446658502124197\n",
      "Gradient Descent(57/99): loss=0.41614501750685035, w0=0.03764840461712564, w1=-0.14768559935298237\n",
      "Gradient Descent(58/99): loss=0.4154859432007922, w0=0.03704077142886987, w1=-0.150603083455471\n",
      "Gradient Descent(59/99): loss=0.4148511165517211, w0=0.036461685993881096, w1=-0.15342223296807952\n",
      "Gradient Descent(60/99): loss=0.4142392789291601, w0=0.03591014173749879, w1=-0.1561468002953823\n",
      "Gradient Descent(61/99): loss=0.41364925666476615, w0=0.035385147628027205, w1=-0.15878037003039158\n",
      "Gradient Descent(62/99): loss=0.4130799540895182, w0=0.03488573021084884, w1=-0.1613263684214086\n",
      "Gradient Descent(63/99): loss=0.412530347245271, w0=0.034410935294645756, w1=-0.1637880721436357\n",
      "Gradient Descent(64/99): loss=0.4119994781941843, w0=0.033959829328996466, w1=-0.16616861643829361\n",
      "Gradient Descent(65/99): loss=0.41148644985954586, w0=0.033531500508958, w1=-0.16847100267557766\n",
      "Gradient Descent(66/99): loss=0.41099042134002944, w0=0.033125059638826206, w1=-0.1706981053920842\n",
      "Gradient Descent(67/99): loss=0.4105106036467337, w0=0.03273964078409938, w1=-0.1728526788482621\n",
      "Gradient Descent(68/99): loss=0.4100462558186063, w0=0.03237440173774861, w1=-0.17493736314691805\n",
      "Gradient Descent(69/99): loss=0.4095966813772466, w0=0.03202852432421694, w1=-0.17695468994976504\n",
      "Gradient Descent(70/99): loss=0.4091612250867221, w0=0.031701214562119585, w1=-0.17890708782539633\n",
      "Gradient Descent(71/99): loss=0.40873926998805943, w0=0.03139170270438602, w1=-0.18079688725883963\n",
      "Gradient Descent(72/99): loss=0.40833023468154495, w0=0.03109924317255951, w1=-0.18262632534995968\n",
      "Gradient Descent(73/99): loss=0.40793357083300336, w0=0.03082311440013591, w1=-0.18439755022539198\n",
      "Gradient Descent(74/99): loss=0.40754876088286235, w0=0.03056261859816794, w1=-0.18611262518637192\n",
      "Gradient Descent(75/99): loss=0.40717531593911244, w0=0.03031708145486857, w1=-0.1877735326127452\n",
      "Gradient Descent(76/99): loss=0.40681277383729364, w0=0.03008585177960537, w1=-0.18938217764157872\n",
      "Gradient Descent(77/99): loss=0.40646069735241164, w0=0.029868301100472488, w1=-0.19094039163711513\n",
      "Gradient Descent(78/99): loss=0.40611867254924494, w0=0.029663823223546965, w1=-0.192449935467307\n",
      "Gradient Descent(79/99): loss=0.40578630725888376, w0=0.029471833760969186, w1=-0.1939125026008108\n",
      "Gradient Descent(80/99): loss=0.40546322967055093, w0=0.029291769634123253, w1=-0.19532972203710056\n",
      "Gradient Descent(81/99): loss=0.4051490870288401, w0=0.02912308855742161, w1=-0.19670316108126118\n",
      "Gradient Descent(82/99): loss=0.40484354442745707, w0=0.028965268507510553, w1=-0.1980343279740306\n",
      "Gradient Descent(83/99): loss=0.4045462836914037, w0=0.028817807182100575, w1=-0.19932467438676432\n",
      "Gradient Descent(84/99): loss=0.40425700234030315, w0=0.028680221452081017, w1=-0.20057559779018946\n",
      "Gradient Descent(85/99): loss=0.4039754126262403, w0=0.028552046810093913, w1=-0.20178844370508356\n",
      "Gradient Descent(86/99): loss=0.40370124064010127, w0=0.028432836818312657, w1=-0.2029645078423545\n",
      "Gradient Descent(87/99): loss=0.4034342254809355, w0=0.02832216255778983, w1=-0.2041050381393992\n",
      "Gradient Descent(88/99): loss=0.403174118483357, w0=0.02821961208140136, w1=-0.20521123669907634\n",
      "Gradient Descent(89/99): loss=0.4029206824984352, w0=0.028124789872115627, w1=-0.20628426163713792\n",
      "Gradient Descent(90/99): loss=0.4026736912239288, w0=0.028037316308052482, w1=-0.20732522884351734\n",
      "Gradient Descent(91/99): loss=0.402432928580065, w0=0.027956827135564525, w1=-0.20833521366246643\n",
      "Gradient Descent(92/99): loss=0.40219818812739694, w0=0.027882972951367907, w1=-0.20931525249616462\n",
      "Gradient Descent(93/99): loss=0.4019692725235604, w0=0.02781541869456953, w1=-0.21026634433608735\n",
      "Gradient Descent(94/99): loss=0.40174599301601593, w0=0.027753843149279016, w1=-0.2111894522261147\n",
      "Gradient Descent(95/99): loss=0.40152816896810645, w0=0.027697938458354807, w1=-0.21208550466108114\n",
      "Gradient Descent(96/99): loss=0.4013156274159724, w0=0.027647409648711943, w1=-0.21295539692421134\n",
      "Gradient Descent(97/99): loss=0.40110820265407443, w0=0.027601974168512896, w1=-0.21379999236665437\n",
      "Gradient Descent(98/99): loss=0.4009057358472448, w0=0.027561361436469978, w1=-0.21462012363211277\n",
      "Gradient Descent(99/99): loss=0.4007080746673617, w0=0.027525312403407255, w1=-0.21541659382936773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.7119485714285714%\n"
     ]
    }
   ],
   "source": [
    "pred_GD(max_iters,gamma,tX,y,seed,ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb14d7a",
   "metadata": {},
   "source": [
    "# Cross validation(using Ridge regression), building data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6bf62d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter\n",
    "degrees = np.arange(2,4)\n",
    "k_fold = 3\n",
    "lambdas = np.logspace(-4, 0, 30)\n",
    "seed = 1\n",
    "\n",
    "#split data train/test\n",
    "x_tr,x_te,y_tr,y_te = split_data(tX, y, 0.3, seed)\n",
    "\n",
    "best_degree, best_lambda, best_rmse = best_degree_selection(x_tr,y_tr,degrees, k_fold, lambdas, seed = 1)\n",
    "print(best_degree, best_lambda, best_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2873ed55",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35636/3144969414.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpoly_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_degree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mw_star\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoly_tr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_lambda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msubset_test_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoly_te\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_star\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35636/3407594426.py\u001b[0m in \u001b[0;36msubset_test_prediction\u001b[1;34m(x_te, w_star, y_te)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my_te\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0ms\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy:\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "#TEST intern substest\n",
    "poly_tr = build_poly(x_tr, best_degree)\n",
    "poly_te = build_poly(x_te, best_degree)\n",
    "w_star,_ = ridge_regression(y_tr,poly_tr,best_lambda)\n",
    "subset_test_prediction(poly_te,w_star,y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6e1606e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((175000, 90), (90,), (175000,), (175000, 30))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_te.shape,w_star.shape,y_te.shape,x_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa67685a",
   "metadata": {},
   "source": [
    "###  1 )Idea : test GD with ridge regression method(lambda), then Cross valid with GD ridge regression\n",
    "###  2) test same method with SGD\n",
    "###  3) test same method with MAE loss instead of MSE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
